
@book{berlyneStudiesNewExperimental1974,
  location = {{Oxford, England}},
  title = {Studies in the New Experimental Aesthetics: {{Steps}} toward an Objective Psychology of Aesthetic Appreciation.},
  shorttitle = {Studies in the New Experimental Aesthetics},
  publisher = {{Hemisphere}},
  date = {1974},
  author = {Berlyne, Daniel E.},
  file = {D\:\\Sauve\\Zotero\\storage\\6RJ7WQBD\\1975-07344-000.html}
}

@book{berlyneAestheticsPsychobiology1971,
  location = {{New York}},
  title = {Aesthetics and {{Psychobiology}}},
  publisher = {{Appleton Century Crofts}},
  date = {1971},
  author = {Berlyne, Daniel E.}
}

@article{northSubjectiveComplexityFamiliarity1995,
  title = {Subjective Complexity, Familiarity, and Liking for Popular Music.},
  volume = {14},
  doi = {http://dx.doi.org/10.1037/h0094090},
  number = {1-2},
  journaltitle = {Psychomusicology: A Journal of Research in Music Cognition},
  date = {1995},
  pages = {77},
  author = {North, Adrian C. and Hargreaves, David J.},
  file = {D\:\\Sauve\\Zotero\\storage\\2AUAJV6G\\1997-07268-005.html}
}

@article{orrRelationshipComplexityLiking2005,
  title = {Relationship between Complexity and Liking as a Function of Expertise},
  volume = {22},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2005},
  pages = {583--611},
  author = {Orr, Mark G. and Ohlsson, Stellan},
  file = {D\:\\Sauve\\Zotero\\storage\\VGX2X36C\\583.html}
}

@article{burkeMusicalPreferencesFunction1990,
  title = {Musical Preferences as a Function of Stimulus Complexity and Listeners' Sophistication},
  volume = {71},
  doi = {https://doi.org/10.2466/pms.1990.71.2.687},
  number = {2},
  journaltitle = {Perceptual and Motor Skills},
  date = {1990},
  pages = {687--690},
  author = {Burke, Michael J. and Gridley, Mark C.},
  file = {D\:\\Sauve\\Zotero\\storage\\JZBN9GCN\\pms.1990.71.2.html}
}

@article{gordonMusicalPreferencesFunction2013,
  title = {Musical Preferences as a Function of Stimulus Complexity of Piano Jazz},
  volume = {25},
  number = {1},
  journaltitle = {Creativity Research Journal},
  date = {2013},
  pages = {143--146},
  author = {Gordon, Josh and Gridley, Mark C.},
  file = {D\:\\Sauve\\Zotero\\storage\\SEBW5YS9\\Gordon and Gridley - 2013 - Musical preferences as a function of stimulus comp.pdf;D\:\\Sauve\\Zotero\\storage\\N3623R8U\\10400419.2013.html}
}

@inproceedings{eerolaExpectancybasedModelMelodic2000,
  title = {Expectancy-Based Model of Melodic Complexity},
  booktitle = {Proceedings of the {{Sixth International Conference}} on {{Music Perception}} and {{Cognition}}. {{Keele}}, {{Staffordshire}}, {{UK}}: {{Department}} of {{Psychology}}. {{CD}}-{{ROM}}},
  date = {2000},
  author = {Eerola, Tuomas and North, Adrian C.},
  file = {D\:\\Sauve\\Zotero\\storage\\KEWAN5AY\\Eerola and North - 2000 - Expectancy-based model of melodic complexity.pdf;D\:\\Sauve\\Zotero\\storage\\QIEWVAJQ\\Eerola and North - 2000 - Expectancy-based model of melodic complexity.pdf}
}

@article{beauvoisQuantifyingAestheticPreference2007,
  langid = {english},
  title = {Quantifying {{Aesthetic Preference And Perceived Complexity For Fractal Melodies}}},
  volume = {24},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/24/3/247},
  doi = {10.1525/mp.2007.24.3.247},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2018-08-11},
  date = {2007-02-01},
  pages = {247-264},
  author = {Beauvois, Michael W.},
  file = {D\:\\Sauve\\Zotero\\storage\\V7ALZPRY\\247.html}
}

@article{chenMovingTimeBrain2008,
  title = {Moving on Time: Brain Network for Auditory-Motor Synchronization Is Modulated by Rhythm Complexity and Musical Training},
  volume = {20},
  doi = {https://doi.org/10.1162/jocn.2008.20018},
  shorttitle = {Moving on Time},
  number = {2},
  journaltitle = {Journal of cognitive neuroscience},
  date = {2008},
  pages = {226--239},
  author = {Chen, Joyce L. and Penhune, Virginia B. and Zatorre, Robert J.},
  file = {D\:\\Sauve\\Zotero\\storage\\DHTUCUGY\\jocn.2008.html}
}

@article{largeTrackingSimpleComplex2002,
  title = {Tracking Simple and Complex Sequences},
  volume = {66},
  doi = {https://doi.org/10.1007/s004260100069},
  number = {1},
  journaltitle = {Psychological research},
  date = {2002},
  pages = {3--17},
  author = {Large, Edward W. and Fink, Philip and Kelso, Scott J.},
  file = {D\:\\Sauve\\Zotero\\storage\\GRFIBUWL\\Large et al. - 2002 - Tracking simple and complex sequences.pdf;D\:\\Sauve\\Zotero\\storage\\IMB57YDY\\s004260100069.html}
}

@article{birbaumerPerceptionMusicDimensional1996,
  title = {Perception of Music and Dimensional Complexity of Brain Activity},
  volume = {6},
  doi = {https://doi.org/10.1142/S0218127496000047},
  number = {02},
  journaltitle = {International Journal of Bifurcation and Chaos},
  date = {1996},
  pages = {267--278},
  author = {Birbaumer, Neil and Lutzenberger, W. and Rau, H. and Braun, C. and Mayer-Kress, G.},
  file = {D\:\\Sauve\\Zotero\\storage\\R6SSKE9Y\\Birbaumer et al. - 1996 - Perception of music and dimensional complexity of .pdf;D\:\\Sauve\\Zotero\\storage\\7CQRM5XM\\S0218127496000047.html}
}

@article{zatorreWhenBrainPlays2007,
  title = {When the Brain Plays Music: Auditory–Motor Interactions in Music Perception and Production},
  volume = {8},
  doi = {https://doi.org/10.1038/nrn2152},
  shorttitle = {When the Brain Plays Music},
  number = {7},
  journaltitle = {Nature reviews neuroscience},
  date = {2007},
  pages = {547},
  author = {Zatorre, Robert J. and Chen, Joyce L. and Penhune, Virginia B.},
  file = {D\:\\Sauve\\Zotero\\storage\\AIPJZRM3\\Zatorre et al. - 2007 - When the brain plays music auditory–motor interac.pdf;D\:\\Sauve\\Zotero\\storage\\I5UHQI9F\\nrn2152.html}
}

@article{mcadamsPerspectivesContributionTimbre1999,
  title = {Perspectives on the Contribution of Timbre to Musical Structure},
  volume = {23},
  number = {3},
  journaltitle = {Computer Music Journal},
  date = {1999},
  pages = {85--102},
  author = {McAdams, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\PUHACG2A\\McAdams - 1999 - Perspectives on the contribution of timbre to musi.pdf;D\:\\Sauve\\Zotero\\storage\\B2TH9Y6A\\014892699559797.html}
}

@article{greyMultidimensionalPerceptualScaling1977,
  title = {Multidimensional Perceptual Scaling of Musical Timbres},
  volume = {61},
  number = {5},
  journaltitle = {the Journal of the Acoustical Society of America},
  date = {1977},
  pages = {1270--1277},
  author = {Grey, John M.},
  file = {D\:\\Sauve\\Zotero\\storage\\YT6ZY8JD\\Grey - 1977 - Multidimensional perceptual scaling of musical tim.pdf;D\:\\Sauve\\Zotero\\storage\\FVVISVEN\\1.html}
}

@article{wesselTimbreSpaceMusical1979,
  title = {Timbre Space as a Musical Control Structure},
  journaltitle = {Computer music journal},
  date = {1979},
  pages = {45--52},
  author = {Wessel, David L.},
  file = {D\:\\Sauve\\Zotero\\storage\\QHR3ZZHD\\Wessel - 1979 - Timbre space as a musical control structure.pdf;D\:\\Sauve\\Zotero\\storage\\F6VPXYM2\\3680283.html}
}

@article{aucouturierWayItSounds2005,
  title = {" {{The}} Way It Sounds": Timbre Models for Analysis and Retrieval of Music Signals},
  volume = {7},
  shorttitle = {" {{The}} Way It Sounds"},
  number = {6},
  journaltitle = {IEEE Transactions on Multimedia},
  date = {2005},
  pages = {1028--1035},
  author = {Aucouturier, J.-J. and Pachet, François and Sandler, Mark},
  file = {D\:\\Sauve\\Zotero\\storage\\QHT6IAU5\\Aucouturier et al. - 2005 -  The way it sounds timbre models for analysis a.pdf;D\:\\Sauve\\Zotero\\storage\\PQ6WVXGU\\1542080.html}
}

@article{deliegeGroupingConditionsListening1987,
  title = {Grouping Conditions in Listening to Music: {{An}} Approach to {{Lerdahl}} \& {{Jackendoff}}'s Grouping Preference Rules},
  volume = {4},
  shorttitle = {Grouping Conditions in Listening to Music},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {1987},
  pages = {325--359},
  author = {Deliege, Irene},
  file = {D\:\\Sauve\\Zotero\\storage\\I5WPQBZB\\325.html}
}

@article{agresHarmonicStructurePredicts2017,
  title = {Harmonic Structure Predicts the Enjoyment of Uplifting Trance Music},
  volume = {7},
  journaltitle = {Frontiers in psychology},
  date = {2017},
  pages = {1999},
  author = {Agres, Kat and Herremans, Dorien and Bigo, Louis and Conklin, Darrell},
  file = {D\:\\Sauve\\Zotero\\storage\\BGU353AV\\full.html;D\:\\Sauve\\Zotero\\storage\\W8CIR2QM\\full.html}
}

@inproceedings{agresEffectRepetitiveStructure2015,
  title = {The Effect of Repetitive Structure on Enjoyment and Altered States in Uplifting Trance Music},
  booktitle = {2nd {{International Conference}} on {{Music}} and {{Consciousness}} ({{MUSCON}} 2), {{Oxford}}},
  date = {2015},
  author = {Agres, Kat and Bigo, Louis and Herremans, Dorien and Conklin, Darrell},
  file = {D\:\\Sauve\\Zotero\\storage\\7AHTFGST\\Agres et al. - 2015 - The effect of repetitive structure on enjoyment an.pdf}
}

@article{levitinMemoryMusicalAttributes2002,
  title = {Memory for Musical Attributes},
  journaltitle = {Foundations of cognitive psychology: Core readings},
  date = {2002},
  pages = {295--310},
  author = {Levitin, Daniel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\MTKBK3Q5\\Levitin - 2002 - Memory for musical attributes.pdf;D\:\\Sauve\\Zotero\\storage\\P9TFFI4P\\books.html}
}

@article{levitinAbsoluteMemoryMusical1994,
  title = {Absolute Memory for Musical Pitch: {{Evidence}} from the Production of Learned Melodies},
  volume = {56},
  shorttitle = {Absolute Memory for Musical Pitch},
  number = {4},
  journaltitle = {Attention, Perception, \& Psychophysics},
  date = {1994},
  pages = {414--423},
  author = {Levitin, Daniel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\6HVSRCH3\\pitch.html}
}

@article{levitinMemoryMusicalTempo1996,
  title = {Memory for Musical Tempo: {{Additional}} Evidence That Auditory Memory Is Absolute},
  volume = {58},
  shorttitle = {Memory for Musical Tempo},
  number = {6},
  journaltitle = {Attention, Perception, \& Psychophysics},
  date = {1996},
  pages = {927--935},
  author = {Levitin, Daniel J. and Cook, Perry R.},
  file = {D\:\\Sauve\\Zotero\\storage\\V5U6XTJV\\tempo.html}
}

@article{makrisVISA3RefiningVoice2016,
  title = {{{VISA3}}: {{Refining}} the Voice Integration/Segregation Algorithm},
  url = {https://www.researchgate.net/profile/Ioannis_Karydis2/publication/307866735_VISA3_Refining_the_Voice_IntegrationSegregation_Algorithm/links/57cfeceb08ae582e0695cec7.pdf},
  shorttitle = {V {{ISA3}}},
  date = {2016},
  author = {Makris, Dimos and Karydis, Ioannis and Cambouropoulos, Emilios},
  file = {D\:\\Sauve\\Zotero\\storage\\44T9QJ2P\\Makris et al. - 2016 - V ISA3 REFINING THE VOICE INTEGRATIONSEGREGATION.pdf}
}

@inproceedings{alexandrosVISAVoiceIntegration2007,
  title = {{{VISA}}: {{The}} Voice Integration/Segregation Algorithm},
  url = {http://www.ionio.gr/~karydis/my_papers/KNPC2007%20-%20VISA%20The%20voice%20integrationsegregation%20algorithm.pdf},
  shorttitle = {{{VISA}}},
  booktitle = {{{ISMIR}} 2007: {{Proceedings}} of the 8th {{International Conference}} on {{Music Information Retrieval}}:[{{September}} 23-27, 2007, {{Vienna}}, {{Austria}}]},
  publisher = {{Austrian Computer Society}},
  date = {2007},
  pages = {445},
  author = {Alexandros, Ioannis Karydis},
  file = {D\:\\Sauve\\Zotero\\storage\\REAJ58WU\\Alexandros - 2007 - VISA THE VOICE INTEGRATIONSEGREGATION ALGORITHM.pdf}
}

@article{palmerMentalRepresentationsMusical1990,
  title = {Mental Representations for Musical Meter},
  volume = {16},
  url = {http://www.brainmusic.org/EducationalActivities/Palmer_meter1990.pdf},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {1990},
  pages = {728--741},
  author = {Palmer, Caroline and Krumhansl, Carol L.},
  file = {D\:\\Sauve\\Zotero\\storage\\6MKMJHA5\\Palmer and Krumhansl - 1990 - Mental representations for musical meter.pdf}
}

@inproceedings{krumhanslKeyfindingMusicAlgorithm1986,
  location = {{Cambridge, MA}},
  title = {Key-Finding in Music: {{An}} Algorithm Based on Pattern Matching to Tonal Hierarchies.},
  eventtitle = {19th {{Annual Meeting}} of the {{Society}} of {{Mathematical Psychology}}},
  date = {1986},
  author = {Krumhansl, Carol L. and Schmuckler, M.}
}

@book{krumhanslCognitiveFoundationsMusical2001,
  title = {Cognitive Foundations of Musical Pitch},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=J4dJCAAAQBAJ&oi=fnd&pg=PT11&dq=krumhansl+cognitive+foundations&ots=PF47VC1-Cj&sig=DjBgMy-284NnR8pLJ3cb6AHMuZI},
  publisher = {{Oxford University Press}},
  date = {2001},
  author = {Krumhansl, Carol L.},
  file = {D\:\\Sauve\\Zotero\\storage\\KAPB7KZN\\Krumhansl - 2001 - Cognitive foundations of musical pitch.pdf;D\:\\Sauve\\Zotero\\storage\\B5QNCXDE\\books.html}
}

@article{krumhanslTonalHierarchiesRare1990,
  title = {Tonal Hierarchies and Rare Intervals in Music Cognition},
  volume = {7},
  url = {http://mp.ucpress.edu/content/7/3/39.abstract},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {1990},
  pages = {309--324},
  author = {Krumhansl, Carol L.},
  file = {D\:\\Sauve\\Zotero\\storage\\I2HPKX4U\\39.html}
}

@article{khalfaEventrelatedSkinConductance2002,
  title = {Event-Related Skin Conductance Responses to Musical Emotions in Humans},
  volume = {328},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394002004627},
  number = {2},
  journaltitle = {Neuroscience letters},
  date = {2002},
  pages = {145--149},
  author = {Khalfa, Stéphanie and Isabelle, Peretz and Jean-Pierre, Blondin and Manon, Robert},
  file = {D\:\\Sauve\\Zotero\\storage\\WX8M52BH\\Khalfa et al. - 2002 - Event-related skin conductance responses to musica.pdf;D\:\\Sauve\\Zotero\\storage\\4XRUAAXG\\S0304394002004627.html}
}

@article{marieEarlyDevelopmentPolyphonic2014,
  title = {Early Development of Polyphonic Sound Encoding and the High Voice Superiority Effect},
  volume = {57},
  url = {http://www.sciencedirect.com/science/article/pii/S002839321400075X},
  journaltitle = {Neuropsychologia},
  date = {2014},
  pages = {50--58},
  author = {Marie, Céline and Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\FMN835G7\\Marie and Trainor - 2014 - Early development of polyphonic sound encoding and.pdf;D\:\\Sauve\\Zotero\\storage\\5DB3Q52S\\S002839321400075X.html}
}

@article{trainorExplainingHighVoice2014,
  title = {Explaining the High Voice Superiority Effect in Polyphonic Music: {{Evidence}} from Cortical Evoked Potentials and Peripheral Auditory Models},
  volume = {308},
  url = {http://www.sciencedirect.com/science/article/pii/S0378595513001858},
  shorttitle = {Explaining the High Voice Superiority Effect in Polyphonic Music},
  journaltitle = {Hearing Research},
  date = {2014},
  pages = {60--70},
  author = {Trainor, Laurel J. and Marie, Céline and Bruce, Ian C. and Bidelman, Gavin M.},
  file = {D\:\\Sauve\\Zotero\\storage\\68NF7F2A\\Trainor et al. - 2014 - Explaining the high voice superiority effect in po.pdf;D\:\\Sauve\\Zotero\\storage\\49QWA485\\S0378595513001858.html}
}

@article{marieDevelopmentSimultaneousPitch2012,
  title = {Development of Simultaneous Pitch Encoding: Infants Show a High Voice Superiority Effect},
  volume = {23},
  url = {https://academic.oup.com/cercor/article-abstract/23/3/660/316161},
  shorttitle = {Development of Simultaneous Pitch Encoding},
  number = {3},
  journaltitle = {Cerebral Cortex},
  date = {2012},
  pages = {660--669},
  author = {Marie, Celine and Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\7PBQJUFS\\Development-of-Simultaneous-Pitch-Encoding-Infants.html;D\:\\Sauve\\Zotero\\storage\\QC22HTTV\\316161.html}
}

@inproceedings{gotoRealtimeMusicScene1999,
  title = {A Real-Time Music Scene Description System: {{Detecting}} Melody and Bass Lines in Audio Signals},
  url = {https://www.researchgate.net/profile/Satoru_Hayamizu/publication/2455822_A_Real-time_Music_Scene_Description_System_Detecting_Melody_and_Bass_Lines_in_Audio_Signals/links/540146ac0cf23d9765a49199.pdf},
  shorttitle = {A Real-Time Music Scene Description System},
  booktitle = {Working {{Notes}} of the {{IJCAI}}-99 {{Workshop}} on {{Computational Auditory Scene Analysis}}},
  date = {1999},
  pages = {31--40},
  author = {Goto, Masataka and Hayamizu, Satoru},
  file = {D\:\\Sauve\\Zotero\\storage\\8BGDEKK4\\Goto and Hayamizu - 1999 - A real-time music scene description system Detect.pdf}
}

@book{shiImageVideoCompression1999,
  title = {Image and Video Compression for Multimedia Engineering: {{Fundamentals}}, Algorithms, and Standards},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=LEjJYki9U0wC&oi=fnd&pg=PA1&dq=shi+sun+predictive+coding&ots=KuPt0vWthJ&sig=VL1uxyoAxC-lBSOvzhuRxXH_TKs},
  shorttitle = {Image and Video Compression for Multimedia Engineering},
  publisher = {{CRC press}},
  date = {1999},
  author = {Shi, Yun Q. and Sun, Huifang},
  file = {D\:\\Sauve\\Zotero\\storage\\BK8KAHBX\\books.html}
}

@article{lakatosHapticFormPerception1999,
  title = {Haptic Form Perception: {{Relative}} Salience of Local and Global Features},
  volume = {61},
  url = {http://www.springerlink.com/index/V36JV880118651NK.pdf},
  shorttitle = {Haptic Form Perception},
  number = {5},
  journaltitle = {Attention, Perception, \& Psychophysics},
  date = {1999},
  pages = {895--908},
  author = {Lakatos, Stephen and Marks, Lawrence E.},
  file = {D\:\\Sauve\\Zotero\\storage\\AF69BJ8K\\Lakatos and Marks - 1999 - Haptic form perception Relative salience of local.pdf;D\:\\Sauve\\Zotero\\storage\\AEBBI95Q\\10.html}
}

@article{stroopStudiesInterferenceSerial1935,
  title = {Studies of Interference in Serial Verbal Reactions.},
  volume = {18},
  url = {http://psycnet.apa.org/journals/xge/18/6/643/},
  number = {6},
  journaltitle = {Journal of experimental psychology},
  date = {1935},
  pages = {643},
  author = {Stroop, J. Ridley},
  file = {D\:\\Sauve\\Zotero\\storage\\JDK2RQKF\\Stroop - 1935 - Studies of interference in serial verbal reactions.pdf;D\:\\Sauve\\Zotero\\storage\\BCMRC6VT\\1936-01863-001.html}
}

@article{zarconeSalienceAttentionSurprisalBased2016,
  langid = {english},
  title = {Salience and {{Attention}} in {{Surprisal}}-{{Based Accounts}} of {{Language Processing}}},
  volume = {7},
  issn = {1664-1078},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2016.00844/full},
  doi = {10.3389/fpsyg.2016.00844},
  abstract = {The notion of salience has been singled out as the explanatory factor for a diverse range of linguistic phenomena. In particular, perceptual salience (e.g. visual salience of objects in the world, acoustic prominence of linguistic sounds) and semantic-pragmatic salience (e.g. prominence of recently mentioned or topical referents) have been shown to influence language comprehension and production. A different line of research has sought to account for behavioral correlates of cognitive load during comprehension as well as for certain patterns in language usage using information-theoretic notions, such as surprisal. Surprisal and salience both affect language processing at different levels, but the relationship between the two has not been adequately elucidated, and the question of whether salience can be reduced to surprisal / predictability is still open. Our review identifies two main challenges in addressing this question: terminological inconsistency and lack of integration between high and low levels of representations in salience- based accounts and surprisal-based accounts. We capitalise upon work in visual cognition in order to orient ourselves in surveying the different facets of the notion of salience in linguistics and their relation with models of surprisal. We find that work on salience highlights aspects of linguistic communication that models of surprisal tend to overlook, namely the role of attention and relevance to current goals, and we argue that the Predictive Coding framework provides a unified view which can account for the role played by attention and predictability at different levels of processing and which can clarify the interplay between low and high levels of processes and between predictability-driven expectation and attention-driven focus.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  urldate = {2017-08-28},
  date = {2016},
  keywords = {accessibility,Attention,goals,Language,Predictability,predictive coding,relevance,Salience,surprisal},
  author = {Zarcone, Alessandra and van Schijndel, Marten and Vogels, Jorrig and Demberg, Vera},
  options = {useprefix=true}
}

@article{zendelImpactMusicianshipCortical2015,
  title = {The Impact of Musicianship on the Cortical Mechanisms Related to Separating Speech from Background Noise},
  url = {https://www.mitpressjournals.org/doi/full/10.1162/jocn_a_00758},
  journaltitle = {Journal of cognitive neuroscience},
  date = {2015},
  author = {Zendel, Benjamin Rich and Tremblay, Charles-David and Belleville, Sylvie and Peretz, Isabelle},
  file = {D\:\\Sauve\\Zotero\\storage\\I4DP3DQ5\\Zendel et al. - 2015 - The impact of musicianship on the cortical mechani.pdf;D\:\\Sauve\\Zotero\\storage\\82JRJIUJ\\jocn_a_00758.html}
}

@article{zendelImpactAttentionalTraining2016,
  title = {The Impact of Attentional Training on Event-Related Potentials in Older Adults},
  volume = {47},
  url = {http://www.sciencedirect.com/science/article/pii/S0197458016301233},
  journaltitle = {Neurobiology of aging},
  date = {2016},
  pages = {10--22},
  author = {Zendel, Benjamin Rich and de Boysson, Chloé and Mellah, Samira and Démonet, Jean-François and Belleville, Sylvie},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\JTXQKEGE\\Zendel et al. - 2016 - The impact of attentional training on event-relate.pdf;D\:\\Sauve\\Zotero\\storage\\IH6MCUH4\\S0197458016301233.html}
}

@article{toiviainenSimilarityPerceptionListening2007,
  title = {Similarity Perception in Listening to Music},
  volume = {11},
  issue = {1\_suppl},
  journaltitle = {Musicae Scientiae},
  date = {2007},
  editor = {Toiviainen, Petri}
}

@article{parbery-clarkMusiciansHaveFinetuned2012,
  title = {Musicians Have Fine-Tuned Neural Distinction of Speech Syllables},
  volume = {219},
  url = {http://www.sciencedirect.com/science/article/pii/S0306452212005106},
  journaltitle = {Neuroscience},
  date = {2012},
  pages = {111--119},
  author = {Parbery-Clark, Alexandra and Tierney, Adam and Strait, Dana L. and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\574IU9XU\\PMC3402586.html;D\:\\Sauve\\Zotero\\storage\\6DUSQQQN\\S0306452212005106.html}
}

@article{parbery-clarkContextdependentEncodingAuditory2011,
  title = {Context-Dependent Encoding in the Auditory Brainstem Subserves Enhanced Speech-in-Noise Perception in Musicians},
  volume = {49},
  url = {http://www.sciencedirect.com/science/article/pii/S0028393211003861},
  number = {12},
  journaltitle = {Neuropsychologia},
  date = {2011},
  pages = {3338--3345},
  author = {Parbery-Clark, Alexandra and Strait, Dana L. and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\2IUJDUW5\\S0028393211003861.html;D\:\\Sauve\\Zotero\\storage\\QGCSBQCG\\PMC3445334.html}
}

@article{parbery-clarkMusicianEnhancementSpeechinnoise2009,
  title = {Musician Enhancement for Speech-in-Noise},
  volume = {30},
  url = {http://journals.lww.com/ear-hearing/Abstract/2009/12000/Musician_Enhancement_for_Speech_In_Noise.2.aspx},
  number = {6},
  journaltitle = {Ear and hearing},
  date = {2009},
  pages = {653--661},
  author = {Parbery-Clark, Alexandra and Skoe, Erika and Lam, Carrie and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\SIJW6KBP\\Parbery-Clark et al. - 2009 - Musician enhancement for speech-in-noise.pdf;D\:\\Sauve\\Zotero\\storage\\WEX5T4I2\\Musician_Enhancement_for_Speech_In_Noise.2.html}
}

@article{andersonReversalAgerelatedNeural2013,
  title = {Reversal of Age-Related Neural Timing Delays with Training},
  volume = {110},
  url = {http://www.pnas.org/content/110/11/4357.short},
  number = {11},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2013},
  pages = {4357--4362},
  author = {Anderson, Samira and White-Schwoch, Travis and Parbery-Clark, Alexandra and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\3SAMTI5T\\4357.html;D\:\\Sauve\\Zotero\\storage\\5QUHI2DU\\4357.html}
}

@article{parbery-clarkMusicalExperienceAging2011,
  title = {Musical Experience and the Aging Auditory System: Implications for Cognitive Abilities and Hearing Speech in Noise},
  volume = {6},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0018082},
  shorttitle = {Musical Experience and the Aging Auditory System},
  number = {5},
  journaltitle = {PloS one},
  date = {2011},
  pages = {e18082},
  author = {Parbery-Clark, Alexandra and Strait, Dana L. and Anderson, Samira and Hittner, Emily and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\FV6WCQ2U\\article.html}
}

@article{bugosIndividualizedPianoInstruction2007,
  title = {Individualized Piano Instruction Enhances Executive Functioning and Working Memory in Older Adults},
  volume = {11},
  url = {http://www.tandfonline.com/doi/abs/10.1080/13607860601086504},
  number = {4},
  journaltitle = {Aging and Mental Health},
  date = {2007},
  pages = {464--471},
  author = {Bugos, Jennifer A. and Perlstein, William M. and McCrae, Christina S. and Brophy, Timothy S. and Bedenbaugh, P. H.},
  file = {D\:\\Sauve\\Zotero\\storage\\ZWTK8W72\\Bugos et al. - 2007 - Individualized piano instruction enhances executiv.pdf;D\:\\Sauve\\Zotero\\storage\\BKTEM74J\\13607860601086504.html}
}

@article{hirokawaEffectsMusicListening2004,
  title = {Effects of Music Listening and Relaxation Instructions on Arousal Changes and the Working Memory Task in Older Adults},
  volume = {41},
  url = {https://academic.oup.com/jmt/article-abstract/41/2/107/929502},
  number = {2},
  journaltitle = {Journal of Music Therapy},
  date = {2004},
  pages = {107--127},
  author = {Hirokawa, Eri},
  file = {D\:\\Sauve\\Zotero\\storage\\2URQVM7G\\929502.html;D\:\\Sauve\\Zotero\\storage\\NV69DLDX\\929502.html}
}

@article{demorest12QuantifyingCulture2016,
  title = {12 {{Quantifying Culture}}: {{The Cultural Distance Hypothesis}} of {{Melodic Expectancy}}},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=rtbiCgAAQBAJ&oi=fnd&pg=PA183&ots=V0wZ98RKFi&sig=Oyq-VCWk9tqo3JfaKyBtwGa7T5k},
  shorttitle = {12 {{Quantifying Culture}}},
  journaltitle = {The Oxford Handbook of Cultural Neuroscience},
  date = {2016},
  pages = {183},
  author = {Demorest, Steven M. and Morrison, Steven J.},
  file = {D\:\\Sauve\\Zotero\\storage\\4WEE63IW\\Demorest and Morrison - 2016 - 12 Quantifying Culture The Cultural Distance Hypo.pdf;D\:\\Sauve\\Zotero\\storage\\ZF8XA23D\\books.html}
}

@article{deanAlgorithmicallygeneratedCorporaThat2016,
  title = {Algorithmically-Generated Corpora That Use Serial Compositional Principles Can Contribute to the Modeling of Sequential Pitch Structure in Non-Tonal Music},
  volume = {11},
  url = {http://search.proquest.com/openview/a11b3886d178da880d54f66d0fafda62/1?pq-origsite=gscholar&cbl=2030105},
  number = {1},
  journaltitle = {Empirical Musicology Review},
  date = {2016},
  pages = {27--46},
  author = {Dean, Roger},
  file = {D\:\\Sauve\\Zotero\\storage\\HMWX9G7A\\Dean - 2016 - Algorithmically-generated corpora that use serial .pdf;D\:\\Sauve\\Zotero\\storage\\S9XXI4I3\\1.html}
}

@article{thomassenSubjectivePerceptualOrganization2017,
  title = {Subjective Perceptual Organization of a Complex Auditory Scene},
  volume = {141},
  issn = {0001-4966},
  url = {http://asa.scitation.org/doi/full/10.1121/1.4973806},
  doi = {10.1121/1.4973806},
  abstract = {Empirical research on the sequential decomposition of an auditory scene primarily relies on interleaved sound mixtures of only two tone sequences (e.g., ABAB…). This oversimplifies the sound decomposition problem by limiting the number of putative perceptual organizations. The current study used a sound mixture composed of three different tones (ABCABC…) that could be perceptually organized in many different ways. Participants listened to these sequences and reported their subjective perception by continuously choosing one out of 12 visually presented perceptual organization alternatives. Different levels of frequency and spatial separation were implemented to check whether participants' perceptual reports would be systematic and plausible. As hypothesized, while perception switched back and forth in each condition between various perceptual alternatives (multistability), spatial as well as frequency separation generally raised the proportion of segregated and reduced the proportion of integrated alternatives. During segregated percepts, in contrast to the hypothesis, many participants had a tendency to perceive two streams in the foreground, rather than reporting alternatives with a clear foreground-background differentiation. Finally, participants perceived the organization with intermediate feature values (e.g., middle tones of the pattern) segregated in the foreground slightly less often than similar alternatives with outer feature values (e.g., higher tones).},
  number = {1},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  date = {2017-01-01},
  pages = {265-276},
  author = {Thomassen, Sabine and Bendixen, Alexandra},
  file = {D\:\\Sauve\\Zotero\\storage\\WD7FAJQG\\Thomassen and Bendixen - 2017 - Subjective perceptual organization of a complex au.pdf;D\:\\Sauve\\Zotero\\storage\\6F7V7VA4\\1.html}
}

@book{meyerStyleMusicTheory1989,
  title = {Style and Music: {{Theory}}, History, and Ideology},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=X44_zSrh0nYC&oi=fnd&pg=PR9&dq=meyer+primary+secondary+musical+parameters&ots=OsESqVwavO&sig=X4sh2yejSQvrjNzgS08wT2rd8dk},
  shorttitle = {Style and Music},
  publisher = {{University of Chicago Press}},
  date = {1989},
  author = {Meyer, Leonard B.},
  file = {D\:\\Sauve\\Zotero\\storage\\Z9F87BXJ\\books.html}
}

@article{straitMusicalExperiencePromotes2009,
  langid = {english},
  title = {Musical {{Experience Promotes Subcortical Efficiency}} in {{Processing Emotional Vocal Sounds}}},
  volume = {1169},
  issn = {1749-6632},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.2009.04864.x/abstract},
  doi = {10.1111/j.1749-6632.2009.04864.x},
  abstract = {To understand how musical experience influences subcortical processing of emotionally salient sounds, we recorded brain stem potentials to affective vocal sounds. Our results suggest that auditory expertise engenders subcortical auditory processing efficiency that is intricately connected with acoustic features important for the communication of emotion. This establishes a subcortical role in the auditory processing of emotional cues, providing the first biological evidence for musicians’ enhanced perception of vocally expressed emotion.},
  number = {1},
  journaltitle = {Annals of the New York Academy of Sciences},
  date = {2009-07-01},
  pages = {209-213},
  keywords = {auditory brain stem response,brain,emotion,Musicians,plasticity},
  author = {Strait, Dana L. and Kraus, Nina and Skoe, Erika and Ashley, Richard},
  file = {D\:\\Sauve\\Zotero\\storage\\R6C4HB7F\\abstract.html}
}

@article{straitMusicalExperienceNeural2009,
  langid = {english},
  title = {Musical Experience and Neural Efficiency – Effects of Training on Subcortical Processing of Vocal Expressions of Emotion},
  volume = {29},
  issn = {1460-9568},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1460-9568.2009.06617.x/abstract},
  doi = {10.1111/j.1460-9568.2009.06617.x},
  abstract = {Musicians exhibit enhanced perception of emotion in speech, although the biological foundations for this advantage remain unconfirmed. In order to gain a better understanding for the influences of musical experience on neural processing of emotionally salient sounds, we recorded brainstem potentials to affective human vocal sounds. Musicians showed enhanced time-domain response magnitude to the most spectrally complex portion of the stimulus and decreased magnitude to the more periodic, less complex portion. Enhanced phase-locking to stimulus periodicity was likewise seen in musicians’ responses to the complex portion. These results suggest that auditory expertise engenders both enhancement and efficiency of subcortical neural responses that are intricately connected with acoustic features important for the communication of emotional states. Our findings provide the first biological evidence for behavioral observations indicating that musical training enhances the perception of vocally expressed emotion in addition to establishing a subcortical role in the auditory processing of emotional cues.},
  number = {3},
  journaltitle = {European Journal of Neuroscience},
  date = {2009-02-01},
  pages = {661-668},
  keywords = {emotion,auditory brainstem response,auditory plasticity,Evoked Potentials,Training},
  author = {Strait, Dana L. and Kraus, Nina and Skoe, Erika and Ashley, Richard},
  file = {D\:\\Sauve\\Zotero\\storage\\TZCD4QPX\\abstract.html}
}

@article{furlNeuralPredictionHigherorder2011,
  title = {Neural Prediction of Higher-Order Auditory Sequence Statistics},
  volume = {54},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811910013303},
  doi = {10.1016/j.neuroimage.2010.10.038},
  abstract = {During auditory perception, we are required to abstract information from complex temporal sequences such as those in music and speech. Here, we investigated how higher-order statistics modulate the neural responses to sound sequences, hypothesizing that these modulations are associated with higher levels of the peri-Sylvian auditory hierarchy. We devised second-order Markov sequences of pure tones with uniform first-order transition probabilities. Participants learned to discriminate these sequences from random ones. Magnetoencephalography was used to identify evoked fields in which second-order transition probabilities were encoded. We show that improbable tones evoked heightened neural responses after 200ms post-tone onset during exposure at the learning stage or around 150ms during the subsequent test stage, originating near the right temporoparietal junction. These signal changes reflected higher-order statistical learning, which can contribute to the perception of natural sounds with hierarchical structures. We propose that our results reflect hierarchical predictive representations, which can contribute to the experiences of speech and music.},
  number = {3},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  date = {2011-02-01},
  pages = {2267-2277},
  keywords = {predictive coding,Magnetoencephalography (MEG),statistical learning,Temporoparietal junction (TPJ)},
  author = {Furl, Nicholas and Kumar, Sukhbinder and Alter, Kai and Durrant, Simon and Shawe-Taylor, John and Griffiths, Timothy D.},
  file = {D\:\\Sauve\\Zotero\\storage\\XX5T8P24\\S1053811910013303.html}
}

@article{kumarPredictiveCodingPitch2011,
  title = {Predictive Coding and Pitch Processing in the Auditory Cortex},
  volume = {23},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_00021},
  number = {10},
  journaltitle = {Journal of Cognitive Neuroscience},
  date = {2011},
  pages = {3084--3094},
  author = {Kumar, Sukhbinder and Sedley, William and Nourski, Kirill V. and Kawasaki, Hiroto and Oya, Hiroyuki and Patterson, Roy D. and Howard III, Matthew A. and Friston, Karl J. and Griffiths, Timothy D.},
  file = {D\:\\Sauve\\Zotero\\storage\\JX5I2S65\\jocn_a_00021.html;D\:\\Sauve\\Zotero\\storage\\Z5NS9XV9\\PMC3821983.html}
}

@article{clarkStandardizedAssessmentCognitive2006,
  title = {Standardized Assessment of Cognitive Functioning during Development and Aging Using an Automated Touchscreen Battery},
  volume = {21},
  issn = {0887-6177},
  url = {http://www.sciencedirect.com/science/article/pii/S0887617706000795},
  doi = {10.1016/j.acn.2006.06.005},
  abstract = {This study examined the effects of age, gender and education on subjects spanning nine decades on a new cognitive battery of 12 tests. One thousand and seven participants between 6 and 82 completed the battery under standardized conditions using an automated, computerized touchscreen. Sensitive indicators of change were obtained on measures of attention and working memory, learning and memory retrieval, and language, visuospatial function, sensori-motor and executive function. Improvement tended to occur through to the third and fourth decade of life, followed by gradual decrement and/or stabilized performance thereafter. Gender differences were obtained on measures of sustained attention, verbal learning and memory, visuospatial processing and dexterity. Years of education in adults was reflected in performance on measures of verbal function. Overall, the test battery provided sensitive indicators on a range of cognitive functions suitable for the assessment of abnormal cognition, the evaluation of treatment effects and for longitudinal case management.},
  number = {5},
  journaltitle = {Archives of Clinical Neuropsychology},
  shortjournal = {Archives of Clinical Neuropsychology},
  date = {2006-08-01},
  pages = {449-467},
  keywords = {Ageing,cognition,Development,Education,Gender,Neuropsychology},
  author = {Clark, C. Richard and Paul, Robert H. and Williams, Leanne M. and Arns, Martijn and Fallahpour, Kamran and Handmer, Carolyn and Gordon, Evian},
  file = {D\:\\Sauve\\Zotero\\storage\\RQM6I59N\\S0887617706000795.html}
}

@article{phillipsFrequencyTemporalResolution2000,
  title = {Frequency and Temporal Resolution in Elderly Listeners with Good and Poor Word Recognition},
  volume = {43},
  url = {http://jslhr.pubs.asha.org/article.aspx?articleid=1781405},
  number = {1},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  date = {2000},
  pages = {217--228},
  author = {Phillips, Susan L. and Gordon-Salant, Sandra and Fitzgibbons, Peter J. and Yeni-Komshian, Grace},
  file = {D\:\\Sauve\\Zotero\\storage\\7ECC2QZV\\Phillips et al. - 2000 - Frequency and temporal resolution in elderly liste.pdf;D\:\\Sauve\\Zotero\\storage\\WIIUCBN9\\article.html}
}

@article{simmons-sternMusicMemoryEnhancer2010,
  title = {Music as a {{Memory Enhancer}} in {{Patients}} with {{Alzheimer}}’s {{Disease}}},
  volume = {48},
  issn = {0028-3932},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2914108/},
  doi = {10.1016/j.neuropsychologia.2010.04.033},
  abstract = {Musical mnemonics have a long and diverse history of popular use. In addition, music processing in general is often considered spared by the neurodegenerative effects of Alzheimer’s disease (AD). Research examining these two phenomena is limited, and no work to our knowledge has explored the effectiveness of musical mnemonics in AD. The present study sought to investigate the effect of music at encoding on the subsequent recognition of associated verbal information. Lyrics of unfamiliar children’s songs were presented bimodally at encoding, as visual stimuli accompanied by either a sung or a spoken recording. Patients with AD demonstrated better recognition accuracy for the sung lyrics than the spoken lyrics, while healthy older adults showed no significant difference between the two conditions. We propose two possible explanations for these findings: first, that the brain areas subserving music processing may be preferentially spared by AD, allowing a more holistic encoding that facilitates recognition, and second, that music heightens arousal in patients with AD, allowing better attention and improved memory.},
  number = {10},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  date = {2010-08},
  pages = {3164-3167},
  author = {Simmons-Stern, Nicholas R. and Budson, Andrew E. and Ally, Brandon A.},
  file = {D\:\\Sauve\\Zotero\\storage\\K82B57CD\\Simmons-Stern et al. - 2010 - Music as a Memory Enhancer in Patients with Alzhei.pdf},
  eprinttype = {pmid},
  eprint = {20452365},
  pmcid = {PMC2914108}
}

@article{verhaeghenAgingExecutiveControl2002,
  title = {Aging, Executive Control, and Attention: A Review of Meta-Analyses},
  volume = {26},
  url = {http://www.sciencedirect.com/science/article/pii/S0149763402000714},
  shorttitle = {Aging, Executive Control, and Attention},
  number = {7},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  date = {2002},
  pages = {849--857},
  author = {Verhaeghen, Paul and Cerella, John},
  file = {D\:\\Sauve\\Zotero\\storage\\P83EVQ92\\Verhaeghen and Cerella - 2002 - Aging, executive control, and attention a review .pdf;D\:\\Sauve\\Zotero\\storage\\Z5VRMVEX\\S0149763402000714.html}
}

@article{classonWorkingMemoryCompensates2013,
  title = {Working Memory Compensates for Hearing Related Phonological Processing Deficit},
  volume = {46},
  url = {http://www.sciencedirect.com/science/article/pii/S0021992412001207},
  number = {1},
  journaltitle = {Journal of Communication Disorders},
  date = {2013},
  pages = {17--29},
  author = {Classon, Elisabet and Rudner, Mary and Rönnberg, Jerker},
  file = {D\:\\Sauve\\Zotero\\storage\\6EVNAEVW\\Classon et al. - 2013 - Working memory compensates for hearing related pho.pdf;D\:\\Sauve\\Zotero\\storage\\6PA6PQ8D\\S0021992412001207.html}
}

@article{ronnbergHearingLossNegatively2011,
  title = {Hearing Loss Is Negatively Related to Episodic and Semantic Long-Term Memory but Not to Short-Term Memory},
  volume = {54},
  url = {http://aja.pubs.asha.org/article.aspx?articleid=1784200},
  number = {2},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  date = {2011},
  pages = {705--726},
  author = {Ronnberg, Jerker and Danielsson, Henrik and Rudner, Mary and Arlinger, Stig and Sternang, Ola and Wahlin, Ake and Nilsson, Lars-Goran},
  file = {D\:\\Sauve\\Zotero\\storage\\42PR7G7G\\article.html}
}

@article{zacksAgingLongTermMemory2006,
  title = {Aging and {{Long}}-{{Term Memory}}: {{Deficits Are Not Inevitable}}},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=nt6uqjnGW_8C&oi=fnd&pg=PA162&dq=aging+long+term+memory&ots=7CgMv4N4f7&sig=zfXrudU9pVwnHYWBVMR-IhI3uSo},
  shorttitle = {Aging and {{Long}}-{{Term Memory}}},
  journaltitle = {Lifespan cognition: Mechanisms of change},
  date = {2006},
  pages = {162},
  author = {Zacks, Rose T. and Hasher, Lynn},
  file = {D\:\\Sauve\\Zotero\\storage\\VXNPAZ9U\\Hasher - 2006 - Aging and Long-Term Memory Deficits Are Not Inevi.pdf;D\:\\Sauve\\Zotero\\storage\\SR3EGUB2\\books.html}
}

@article{arlingerEmergenceCognitiveHearing2009,
  title = {The Emergence of Cognitive Hearing Science},
  volume = {50},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9450.2009.00753.x/full},
  number = {5},
  journaltitle = {Scandinavian journal of psychology},
  date = {2009},
  pages = {371--384},
  author = {Arlinger, Stig and Lunner, Thomas and Lyxell, Björn and Kathleen Pichora-Fuller, M.},
  file = {D\:\\Sauve\\Zotero\\storage\\AX5VXVBM\\Arlinger et al. - 2009 - The emergence of cognitive hearing science.pdf;D\:\\Sauve\\Zotero\\storage\\W45HGN7K\\full.html}
}

@article{jergerHearingImpairmentOlder1995,
  title = {Hearing Impairment in Older Adults: New Concepts},
  volume = {43},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1532-5415.1995.tb05539.x/full},
  shorttitle = {Hearing Impairment in Older Adults},
  number = {8},
  journaltitle = {Journal of the American Geriatrics Society},
  date = {1995},
  pages = {928--935},
  author = {Jerger, James and Chmiel, Rose and Wilson, Nancy and Luchi, Robert},
  file = {D\:\\Sauve\\Zotero\\storage\\WUIFNS8E\\full.html}
}

@article{wingfieldHearingLossOlder2005,
  title = {Hearing Loss in Older Adulthood: {{What}} It Is and How It Interacts with Cognitive Performance},
  volume = {14},
  url = {http://journals.sagepub.com/doi/abs/10.1111/j.0963-7214.2005.00356.x},
  shorttitle = {Hearing Loss in Older Adulthood},
  number = {3},
  journaltitle = {Current directions in psychological science},
  date = {2005},
  pages = {144--148},
  author = {Wingfield, Arthur and Tun, Patricia A. and McCoy, Sandra L.},
  file = {D\:\\Sauve\\Zotero\\storage\\SCHCQ755\\Wingfield et al. - 2005 - Hearing loss in older adulthood What it is and ho.pdf;D\:\\Sauve\\Zotero\\storage\\7BCT6UR8\\j.0963-7214.2005.00356.html}
}

@article{pichora-fullerEffectsAgeAuditory2006,
  title = {Effects of Age on Auditory and Cognitive Processing: Implications for Hearing Aid Fitting and Audiologic Rehabilitation},
  volume = {10},
  url = {http://journals.sagepub.com/doi/abs/10.1177/108471380601000103},
  shorttitle = {Effects of Age on Auditory and Cognitive Processing},
  number = {1},
  journaltitle = {Trends in amplification},
  date = {2006},
  pages = {29--59},
  author = {Pichora-Fuller, M. Kathleen and Singh, Gurjit},
  file = {D\:\\Sauve\\Zotero\\storage\\B9H26RX5\\PMC4111543.html;D\:\\Sauve\\Zotero\\storage\\H4V3VDZN\\108471380601000103.html}
}

@article{pichora-fullerHearingImpairmentCognitive2016,
  title = {Hearing Impairment and Cognitive Energy: {{The}} Framework for Understanding Effortful Listening ({{FUEL}})},
  volume = {37},
  url = {http://journals.lww.com/ear-hearing/Abstract/2016/07001/Hearing_Impairment_and_Cognitive_Energy___The.2.aspx},
  shorttitle = {Hearing Impairment and Cognitive Energy},
  journaltitle = {Ear and hearing},
  date = {2016},
  pages = {5S--27S},
  author = {Pichora-Fuller, M. Kathleen and Kramer, Sophia E. and Eckert, Mark A. and Edwards, Brent and Hornsby, Benjamin WY and Humes, Larry E. and Lemke, Ulrike and Lunner, Thomas and Matthen, Mohan and Mackersie, Carol L. and others},
  file = {D\:\\Sauve\\Zotero\\storage\\5WM4TM83\\Hearing_Impairment_and_Cognitive_Energy___The.2.html;D\:\\Sauve\\Zotero\\storage\\G2TTNS7H\\Hearing_Impairment_and_Cognitive_Energy___The.2.html}
}

@article{alainAgerelatedChangesProcessing1999,
  title = {Age-Related Changes in Processing Auditory Stimuli during Visual Attention: Evidence for Deficits in Inhibitory Control and Sensory Memory.},
  volume = {14},
  url = {http://psycnet.apa.org/journals/pag/14/3/507/},
  shorttitle = {Age-Related Changes in Processing Auditory Stimuli during Visual Attention},
  number = {3},
  journaltitle = {Psychology and aging},
  date = {1999},
  pages = {507},
  author = {Alain, Claude and Woods, David L.},
  file = {D\:\\Sauve\\Zotero\\storage\\KM48RVNN\\Alain and Woods - 1999 - Age-related changes in processing auditory stimuli.pdf;D\:\\Sauve\\Zotero\\storage\\4HCHK94R\\507.html}
}

@article{humesSpeechIdentificationDifficulties1991,
  title = {Speech Identification Difficulties of Hearing-Impaired Elderly Personsthe Contributions of Auditory Processing Deficits},
  volume = {34},
  url = {http://lshss.pubs.asha.org/article.aspx?articleid=1778998},
  number = {3},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  date = {1991},
  pages = {686--693},
  author = {Humes, Larry E. and Christopherson, Laurel},
  file = {D\:\\Sauve\\Zotero\\storage\\5FPZ2DFJ\\Humes and Christopherson - 1991 - Speech identification difficulties of hearing-impa.pdf;D\:\\Sauve\\Zotero\\storage\\2Z47XQMA\\article.html}
}

@article{fitzgibbonsAgingTemporalDiscrimination2001,
  title = {Aging and Temporal Discrimination in Auditory Sequences},
  volume = {109},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.1371760},
  number = {6},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {2001},
  pages = {2955--2963},
  author = {Fitzgibbons, Peter J. and Gordon-Salant, Sandra},
  file = {D\:\\Sauve\\Zotero\\storage\\FS56P3AD\\Fitzgibbons and Gordon-Salant - 2001 - Aging and temporal discrimination in auditory sequ.pdf;D\:\\Sauve\\Zotero\\storage\\K3CDHD68\\1.html}
}

@article{fitzgibbonsAuditoryTemporalOrder1998,
  title = {Auditory Temporal Order Perception in Younger and Older Adults},
  volume = {41},
  url = {http://aja.pubs.asha.org/article.aspx?articleid=1780675},
  number = {5},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  date = {1998},
  pages = {1052--1060},
  author = {Fitzgibbons, Peter J. and Gordon-Salant, Sandra},
  file = {D\:\\Sauve\\Zotero\\storage\\Z7JW4DZM\\Fitzgibbons and Gordon-Salant - 1998 - Auditory temporal order perception in younger and .pdf;D\:\\Sauve\\Zotero\\storage\\U8GQEQIA\\article.html}
}

@article{gordon-salantProfileAuditoryTemporal1999,
  title = {Profile of Auditory Temporal Processing in Older Listeners},
  volume = {42},
  url = {http://cred.pubs.asha.org/article.aspx?articleid=1780884},
  number = {2},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  date = {1999},
  pages = {300--311},
  author = {Gordon-Salant, Sandra and Fitzgibbons, Peter J.},
  file = {D\:\\Sauve\\Zotero\\storage\\EQRRCVVQ\\Gordon-Salant and Fitzgibbons - 1999 - Profile of auditory temporal processing in older l.pdf;D\:\\Sauve\\Zotero\\storage\\K9IVGQ2I\\article.html}
}

@article{fitzgibbonsAuditoryTemporalProcessing1996,
  title = {Auditory Temporal Processing in Elderly Listeners},
  volume = {7},
  url = {https://pdfs.semanticscholar.org/7a35/b8ebed4b6c06bd0ffab563465c31d07be428.pdf},
  journaltitle = {JOURNAL-AMERICAN ACADEMY OF AUDIOLOGY},
  date = {1996},
  pages = {183--189},
  author = {Fitzgibbons, Peter J. and Gordon-Salant, Sandra and others},
  file = {D\:\\Sauve\\Zotero\\storage\\GZE2IPDG\\Fitzgibbons et al. - 1996 - Auditory temporal processing in elderly listeners.pdf}
}

@article{trainorAgingAuditoryTemporal1989,
  title = {Aging and Auditory Temporal Sequencing: {{Ordering}} the Elements of Repeating Tone Patterns},
  volume = {45},
  url = {http://www.springerlink.com/index/T4X8601138622720.pdf},
  shorttitle = {Aging and Auditory Temporal Sequencing},
  number = {5},
  journaltitle = {Attention, Perception, \& Psychophysics},
  date = {1989},
  pages = {417--426},
  author = {Trainor, Laurel J. and Trehub, Sandra E.},
  file = {D\:\\Sauve\\Zotero\\storage\\2JB7R993\\Trainor and Trehub - 1989 - Aging and auditory temporal sequencing Ordering t.pdf;D\:\\Sauve\\Zotero\\storage\\FUMSQ4KH\\10.html}
}

@article{alainAgingSegregationAuditory1996,
  title = {Aging and the Segregation of Auditory Stimulus Sequences},
  volume = {51},
  url = {https://academic.oup.com/psychsocgerontology/article-abstract/51B/2/P91/546629},
  number = {2},
  journaltitle = {The Journals of Gerontology Series B: Psychological Sciences and Social Sciences},
  date = {1996},
  pages = {P91--P93},
  author = {Alain, Claude and Ogawa, Keith H. and Woods, David L.},
  file = {D\:\\Sauve\\Zotero\\storage\\PD9KZ7US\\Alain et al. - 1996 - Aging and the segregation of auditory stimulus seq.pdf;D\:\\Sauve\\Zotero\\storage\\V2SQVP5A\\Alain et al. - 1996 - Aging and the segregation of auditory stimulus seq.pdf;D\:\\Sauve\\Zotero\\storage\\A3D2KUT9\\Aging-and-the-Segregation-of-Auditory-Stimulus.html;D\:\\Sauve\\Zotero\\storage\\FNVYY6PU\\546629.html}
}

@article{debruinLastDanceMe2005,
  title = {Save the Last Dance for Me: {{Unwanted}} Serial Position Effects in Jury Evaluations},
  volume = {118},
  url = {http://www.sciencedirect.com/science/article/pii/S0001691804000885},
  shorttitle = {Save the Last Dance for Me},
  number = {3},
  journaltitle = {Acta psychologica},
  date = {2005},
  pages = {245--260},
  author = {de Bruin, Wändi Bruine},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\MDSB8HQZ\\de Bruin - 2005 - Save the last dance for me Unwanted serial positi.pdf;D\:\\Sauve\\Zotero\\storage\\C58CCHPG\\S0001691804000885.html}
}

@article{ginsburghExpertOpinionCompensation2003,
  eprinttype = {jstor},
  eprint = {3132175},
  title = {Expert Opinion and Compensation: {{Evidence}} from a Musical Competition},
  volume = {93},
  shorttitle = {Expert Opinion and Compensation},
  number = {1},
  journaltitle = {The American Economic Review},
  date = {2003},
  pages = {289--296},
  author = {Ginsburgh, Victor A. and Van Ours, Jan C.},
  file = {D\:\\Sauve\\Zotero\\storage\\R9AS6E8R\\3132175.html}
}

@article{thorngateWhyBestPerson1987,
  title = {Why the Best Person Rarely Wins: {{Some}} Embarrassing Facts about Contests},
  volume = {18},
  url = {http://journals.sagepub.com/doi/pdf/10.1177/104687818701800301},
  shorttitle = {Why the Best Person Rarely Wins},
  number = {3},
  journaltitle = {Simulation \& Games},
  date = {1987},
  pages = {299--320},
  author = {Thorngate, Warren and Carroll, Barbara},
  file = {D\:\\Sauve\\Zotero\\storage\\P7NXWFGJ\\104687818701800301.html}
}

@article{tsaySightSoundJudgment2013,
  title = {Sight over Sound in the Judgment of Music Performance},
  volume = {110},
  url = {http://www.pnas.org/content/early/2013/08/16/1221454110.full.pdf+htmlIt&},
  number = {36},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2013},
  pages = {14580--14585},
  author = {Tsay, Chia-Jung},
  file = {D\:\\Sauve\\Zotero\\storage\\E6UI3WKB\\14580.html}
}

@article{conklinDiscoveryDistinctivePatterns2010,
  title = {Discovery of Distinctive Patterns in Music},
  volume = {14},
  url = {http://content.iospress.com/articles/intelligent-data-analysis/ida00438},
  number = {5},
  journaltitle = {Intelligent Data Analysis},
  date = {2010},
  pages = {547--554},
  author = {Conklin, Darrell},
  file = {D\:\\Sauve\\Zotero\\storage\\65CI5629\\Conklin - 2010 - Discovery of distinctive patterns in music.pdf;D\:\\Sauve\\Zotero\\storage\\G84P8AXD\\ida00438.html}
}

@inproceedings{conklinDiscoveryContrapuntalPatterns2010,
  title = {Discovery of {{Contrapuntal Patterns}}.},
  volume = {2010},
  url = {http://www.ehu.eus/cs-ikerbasque/conklin/papers/ismir2010-36.pdf},
  booktitle = {{{ISMIR}}},
  date = {2010},
  pages = {11th},
  author = {Conklin, Darrell and Bergeron, Mathieu},
  file = {D\:\\Sauve\\Zotero\\storage\\MP7ZDM3Q\\Conklin and Bergeron - 2010 - Discovery of Contrapuntal Patterns..pdf}
}

@article{robertsHowPersuasiveGood2000,
  title = {How Persuasive Is a Good Fit? {{A}} Comment on Theory Testing.},
  volume = {107},
  url = {http://psycnet.apa.org/psycinfo/2000-15248-005},
  shorttitle = {How Persuasive Is a Good Fit?},
  number = {2},
  journaltitle = {Psychological review},
  date = {2000},
  pages = {358},
  author = {Roberts, Seth and Pashler, Harold},
  file = {D\:\\Sauve\\Zotero\\storage\\BMKT8WQF\\2000-15248-005.html}
}

@book{friedmanElementsStatisticalLearning2001,
  title = {The Elements of Statistical Learning},
  volume = {1},
  url = {http://statweb.stanford.edu/~tibs/book/preface.ps},
  publisher = {{Springer series in statistics New York}},
  date = {2001},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert}
}

@article{gigerenzerReasoningFastFrugal1996,
  title = {Reasoning the Fast and Frugal Way: Models of Bounded Rationality.},
  volume = {103},
  url = {http://psycnet.apa.org/journals/rev/103/4/650/},
  shorttitle = {Reasoning the Fast and Frugal Way},
  number = {4},
  journaltitle = {Psychological review},
  date = {1996},
  pages = {650},
  author = {Gigerenzer, Gerd and Goldstein, Daniel G.},
  file = {D\:\\Sauve\\Zotero\\storage\\2WF9F52R\\Gigerenzer and Goldstein - 1996 - Reasoning the fast and frugal way models of bound.pdf;D\:\\Sauve\\Zotero\\storage\\HRDZXA7S\\650.html}
}

@incollection{gigerenzerBettingOneGood1999,
  title = {Betting on One Good Reason: {{The}} Take the Best Heuristic},
  url = {http://pubman.mpdl.mpg.de/pubman/item/escidoc:2102907/component/escidoc:2102906/GG_Betting_1999.pdf},
  shorttitle = {Betting on One Good Reason},
  booktitle = {Simple Heuristics That Make Us Smart},
  publisher = {{Oxford University Press}},
  date = {1999},
  pages = {75--95},
  author = {Gigerenzer, Gerd and Goldstein, Daniel G.},
  file = {D\:\\Sauve\\Zotero\\storage\\PCKCQ5IF\\Gigerenzer and Goldstein - 1999 - Betting on one good reason The take the best heur.pdf}
}

@article{czerlinskiHowGoodAre1999,
  title = {How Good Are Simple Heuristics?},
  url = {http://psycnet.apa.org/psycinfo/1999-04366-005},
  date = {1999},
  author = {Czerlinski, Jean and Gigerenzer, Gerd and Goldstein, Daniel G.},
  file = {D\:\\Sauve\\Zotero\\storage\\C3AH8MD3\\1999-04366-005.html}
}

@article{hutchinsonSimpleHeuristicsRules2005,
  title = {Simple Heuristics and Rules of Thumb: {{Where}} Psychologists and Behavioural Biologists Might Meet},
  volume = {69},
  url = {http://www.sciencedirect.com/science/article/pii/S0376635705000495},
  shorttitle = {Simple Heuristics and Rules of Thumb},
  number = {2},
  journaltitle = {Behavioural processes},
  date = {2005},
  pages = {97--124},
  author = {Hutchinson, John MC and Gigerenzer, Gerd},
  file = {D\:\\Sauve\\Zotero\\storage\\2ZSZVAE5\\Hutchinson and Gigerenzer - 2005 - Simple heuristics and rules of thumb Where psycho.pdf;D\:\\Sauve\\Zotero\\storage\\B7SH2JFH\\S0376635705000495.html}
}

@article{pearceMusicPerceptionHistorical2017,
  title = {Music Perception in Historical Audiences: Towards Predictive Models of Music Perception in Historical Audiences.},
  volume = {8},
  url = {http://dro.dur.ac.uk/21547/1/21547.pdf},
  shorttitle = {Music Perception in Historical Audiences},
  number = {1-2},
  journaltitle = {Journal of interdisciplinary music studies.},
  date = {2017},
  pages = {91--120},
  author = {Pearce, Marcus T. and Eerola, Tuomas},
  file = {D\:\\Sauve\\Zotero\\storage\\D5RJE34G\\Pearce and Eerola - 2017 - Music perception in historical audiences towards .pdf}
}

@article{globersonBrainResponsesRegular2017,
  title = {Brain Responses to Regular and Octave-Scrambled Melodies: {{A}} Case of Predictive-Coding?},
  volume = {43},
  url = {http://psycnet.apa.org/journals/xhp/43/3/487/},
  shorttitle = {Brain Responses to Regular and Octave-Scrambled Melodies},
  number = {3},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {2017},
  pages = {487},
  author = {Globerson, Eitan and Granot, Roni and Tal, Idan and Harpaz, Yuval and Zeev-Wolf, Maor and Golstein, Abraham},
  file = {D\:\\Sauve\\Zotero\\storage\\38GQF8JF\\487.html}
}

@article{agresInformationTheoreticPropertiesAuditory2017,
  title = {Information-{{Theoretic Properties}} of {{Auditory Sequences Dynamically Influence Expectation}} and {{Memory}}},
  volume = {42},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12477/full},
  doi = {https://doi.org/10.1111/cogs.12477},
  number = {1},
  journaltitle = {Cognitive science},
  date = {2017},
  pages = {43-76},
  author = {Agres, Kat and Abdallah, Samer and Pearce, Marcus},
  file = {D\:\\Sauve\\Zotero\\storage\\AQU3MFPJ\\Agres et al. - 2018 - Information-Theoretic Properties of Auditory Seque.pdf;D\:\\Sauve\\Zotero\\storage\\4UVZNXWP\\full.html;D\:\\Sauve\\Zotero\\storage\\W24MCBEK\\cogs.html}
}

@article{clarkManyFacesPrecision2013,
  langid = {english},
  title = {The Many Faces of Precision ({{Replies}} to Commentaries on “{{Whatever}} next? {{Neural}} Prediction, Situated Agents, and the Future of Cognitive Science”)},
  volume = {4},
  issn = {1664-1078},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00270/full},
  doi = {10.3389/fpsyg.2013.00270},
  shorttitle = {The Many Faces of Precision ({{Replies}} to Commentaries on “{{Whatever}} Next?},
  abstract = {An appreciation of the many roles of ‘precision-weighting’ (upping the gain on select populations of prediction error units) opens the door to better accounts of planning and ‘offline simulation’, makes suggestive contact with large bodies of work on embodied and situated cognition, and offers new perspectives on the ‘active brain’. Combined with the complex affordances of language and culture, and operating against the essential backdrop of a variety of more biologically basic ploys and stratagems, the result is a maximally context-sensitive, restless, constantly self-reconfiguring architecture.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  urldate = {2017-06-28},
  date = {2013},
  keywords = {predictive coding,embodiment,hierarchy,Precision,Prediction},
  author = {Clark, Andy}
}

@article{volkMusicSimilarityConcepts2016,
  title = {Music {{Similarity}}: {{Concepts}}, {{Cognition}} and {{Computation}}},
  volume = {45},
  issn = {0929-8215},
  url = {http://www.tandfonline.com/doi/full/10.1080/09298215.2016.1232412},
  doi = {10.1080/09298215.2016.1232412},
  shorttitle = {Music {{Similarity}}},
  number = {3},
  journaltitle = {Journal of New Music Research},
  shortjournal = {Journal of New Music Research},
  date = {2016-07-02},
  pages = {207-209},
  author = {Volk, Anja and Chew, Elaine and Hellmuth Margulis, Elizabeth and Anagnostopoulou, Christina},
  file = {D\:\\Sauve\\Zotero\\storage\\26IKV7GB\\09298215.2016.html}
}

@article{vareseLiberationSound1966,
  eprinttype = {jstor},
  eprint = {832385},
  title = {The Liberation of Sound},
  volume = {5},
  number = {1},
  journaltitle = {Perspectives of new music},
  date = {1966},
  pages = {11--19},
  author = {Varèse, Edgard and Wen-Chung, Chou},
  file = {D\:\\Sauve\\Zotero\\storage\\3ZT4T8PF\\Varèse and Wen-Chung - 1966 - The liberation of sound.pdf;D\:\\Sauve\\Zotero\\storage\\IEBEC3AE\\832385.html}
}

@article{cavnarNgrambasedTextCategorization1994,
  title = {N-Gram-Based Text Categorization},
  volume = {48113},
  url = {http://www.academia.edu/download/6397498/10.1.1.21.3248.pdf},
  number = {2},
  journaltitle = {Ann Arbor MI},
  date = {1994},
  pages = {161--175},
  author = {Cavnar, William B. and Trenkle, John M. and others}
}

@article{zivCompressionIndividualSequences1978,
  title = {Compression of Individual Sequences via Variable-Rate Coding},
  volume = {24},
  url = {http://ieeexplore.ieee.org/abstract/document/1055934/},
  number = {5},
  journaltitle = {IEEE transactions on Information Theory},
  date = {1978},
  pages = {530--536},
  author = {Ziv, Jacob and Lempel, Abraham},
  file = {D\:\\Sauve\\Zotero\\storage\\N66I6DQT\\Ziv and Lempel - 1978 - Compression of individual sequences via variable-r.pdf;D\:\\Sauve\\Zotero\\storage\\Q3KZX9JR\\1055934.html}
}

@article{glasbergDerivationAuditoryFilter1990,
  title = {Derivation of Auditory Filter Shapes from Notched-Noise Data},
  volume = {47},
  url = {http://www.sciencedirect.com/science/article/pii/037859559090170T},
  number = {1},
  journaltitle = {Hearing research},
  date = {1990},
  pages = {103--138},
  author = {Glasberg, Brian R. and Moore, Brian CJ},
  file = {D\:\\Sauve\\Zotero\\storage\\2V24P5WG\\037859559090170T.html}
}

@book{temperleyMelismaMusicAnalyzer2001,
  title = {The {{Melisma Music Analyzer}}},
  date = {2001},
  author = {Temperley, D. and Sleator, Daniel}
}

@article{leopoldMultistablePhenomenaChanging1999,
  title = {Multistable Phenomena: Changing Views in Perception},
  volume = {3},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661399013327},
  shorttitle = {Multistable Phenomena},
  number = {7},
  journaltitle = {Trends in cognitive sciences},
  date = {1999},
  pages = {254--264},
  author = {Leopold, David A. and Logothetis, Nikos K.},
  file = {D\:\\Sauve\\Zotero\\storage\\G49HSIN6\\Leopold and Logothetis - 1999 - Multistable phenomena changing views in perceptio.pdf;D\:\\Sauve\\Zotero\\storage\\SX969DDM\\S1364661399013327.html}
}

@article{wrightAuditoryStreamSegregation1987,
  title = {Auditory Stream Segregation and the Control of Dissonance in Polyphonic Music},
  volume = {2},
  doi = {10.1080/07494468708567054},
  abstract = {Explores ways that concepts from the psychoacoustic theory of auditory stream segregation (A. S. Bregman and J. Campbell; see record [rid]1971-30024-001[/rid]) can contribute to the understanding of the patterns of polyphonic music. These concepts include (1) the internal perceptual representation vs the external stimulus field and (2) the notions of tonal chimera and the emergence of perceptual qualities. Ways that the auditory stream-forming process can influence perception of the linear and harmonic dimensions of polyphonic music are discussed, focusing on how contrapuntal patterns can be manipulated to control perception of harmonic dissonance. This approach raises new questions about the psychoacoustic definition of dissonance, demystifies some established principles of music theory, and provides new ways to explain contrapuntal phenomena. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Contemporary Music Review},
  shortjournal = {Contemporary Music Review},
  date = {1987},
  pages = {63-92},
  keywords = {Auditory Perception,Music,Theories,theory of auditory stream segregation & comprehension of patterns of polyphonic music},
  author = {Wright, James K. and Bregman, Albert S.}
}

@article{bregmanEffectContinuityAuditory1973,
  title = {The Effect of Continuity on Auditory Stream Segregation},
  volume = {13},
  issn = {0031-5117},
  doi = {10.3758/BF03214144},
  abstract = {Presented a rapid, repeating cycle of alternating high and low tones to 51 graduate and undergraduate students under 3 conditions. In the "discrete" condition, transitions between tones were abrupt; in the "ramped" condition, successive tones were connected by frequency glides. In the "semiramped" condition, there were partial glides in frequency (as in speech). Discrete sequences were most likely to split perceptually into high and low streams, making order discriminations difficult. The ramped condition was least likely to split, and order perception was easiest. Results for the semiramped condition were intermediate. The discussion relates these findings to the acoustic properties of speech and to the process of auditory stream formation. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {2},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1973-04},
  pages = {308-312},
  keywords = {Auditory Perception,auditory stream segregation,college & graduate students,continuity in tonal transitions,implications for acoustic speech properties,Pitch (Frequency),Speech Characteristics},
  author = {Bregman, Albert S. and Dannenbring, Gary L.}
}

@article{bregmanAuditoryStreamingCompetition1978,
  title = {Auditory Streaming: {{Competition}} among Alternative Organizations},
  volume = {23},
  issn = {0031-5117},
  doi = {10.3758/BF03204141},
  shorttitle = {Auditory Streaming},
  abstract = {It has been proposed that auditory stream splitting in rapid tone sequences may occur whenever a tone falls outside some critical region surrounding its predecessor and some tracking mechanism cannot shift its frequency setting fast enough. If true, a certain pair of tones would split apart or not, depending on their separation in time and frequency. Adult Ss listened to a rapid repeating 4-tone cycle and made 3 types of judgments: (a) discriminating the order of 2 of the tones, (b) saying whether 2 of the tones could be heard as a separate pair, and (c) judging the rhythmic pattern. Results indicate that tone splitting depends on the context of other tones and that alternative groupings compete for tonal elements. It is proposed that stream formation is a pattern-factoring mechanism, sensitive to pattern properties. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {5},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1978-05},
  pages = {391-398},
  keywords = {Pitch (Frequency),Auditory Discrimination,Auditory Stimulation,auditory streaming,Contextual Associations,repeating 4-tone cycle & frequencies of 2 context tones},
  author = {Bregman, Albert S.}
}

@article{rogersCumulationTendencySegregate1998,
  title = {Cumulation of the Tendency to Segregate Auditory Streams: {{Resetting}} by Changes in Location and Loudness},
  volume = {60},
  issn = {0031-5117},
  doi = {10.3758/BF03206171},
  shorttitle = {Cumulation of the Tendency to Segregate Auditory Streams},
  abstract = {In four experiments, the accumulation, over time, of a tendency to hear separate high and low streams in a sequence of high (H) and low (L) tones, presented in a galloping rhythm (HLH–HLH–…), was studied. Each trial was composed of two parts, an induction sequence, then a test sequence, with no break between them. The test sequence was always heard at the far left. When the induction sequence and the test sequence were identical, the presence of the induction sequence increased the tendency for the test sequence to split into two streams. However, when the sequences differed in location (cued by differences in interaural timing or intensity over headphones and by loudspeaker placement in a free field) or when they differed in loudness, the accumulation of the segregative tendency was reset, and the test sequence sounded more integrated. When the induction sequence changed in location or loudness in gradual steps toward the value of the test sequence, resetting was much less. It appears that the accumulation of information about streams in different frequency regions is sensitive to sudden changes in parameters, even when they affect the frequency regions equally. This prevents the system from accumulating data across unrelated events. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {7},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1998-10},
  pages = {1216-1227},
  keywords = {Auditory Stimulation,accumulation of tendency to hear separate high & low streams in sequence of high vs low tones,adults,Auditory Localization,Stimulus Frequency,Stimulus Parameters},
  author = {Rogers, Wendy L. and Bregman, Albert S.}
}

@article{bregmanAuditoryGroupingBased1990,
  title = {Auditory Grouping Based on Fundamental Frequency and Formant Peak Frequency},
  volume = {44},
  issn = {0008-4255},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1991-06393-001&site=ehost-live},
  doi = {10.1037/h0084255},
  abstract = {Studied the perceptual grouping of a 4-tone cycle as a function of differences in fundamental frequencies and the freqencies of spectral peaks. Each tone had a single formant and at least 13 harmonics. In Exp 1, the formant was created by filtering a flat spectrum and in Exp 2 by adding harmonics. Fundamental frequency was found to be capable of controlling grouping even when the spectra spanned exactly the same frequency range. Formant peak separation became more effective as the sharpness increased. The effect of each type of acoustic difference depended on the task. Ss could group the tones by either sort of difference but were also capable of resisting the disruptive effect of the other one. This supports the presence of a schema-based process of perceptual grouping and the relative weakness of primitive segregation. (French abstract) (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  number = {3},
  journaltitle = {Canadian Journal of Psychology/Revue canadienne de psychologie},
  shortjournal = {Canadian Journal of Psychology/Revue canadienne de psychologie},
  urldate = {2015-06-16},
  date = {1990-09},
  pages = {400-413},
  keywords = {Auditory Perception,Pitch (Frequency),college students,fundamental frequencies & frequencies of spectral peaks & perceptual grouping of 4 tone cycle},
  author = {Bregman, Albert S. and Liao, Christine and Levitan, Robert},
  file = {D\:\\Sauve\\Zotero\\storage\\A2SC6AJD\\Bregman et al. - 1990 - Auditory grouping based on fundamental frequency a.pdf}
}

@article{bregmanEffectsTimeIntervals2000,
  title = {Effects of Time Intervals and Tone Durations on Auditory Stream Segregation},
  volume = {62},
  issn = {0031-5117},
  doi = {10.3758/BF03212114},
  abstract = {Explored the role of the interstimulus interval (ISI) involving overlapping high and low tones. A total of 48 adult, student listeners rated the difficulty of hearing a single coherent stream in a sequence of high (H) and low (L) tones that alternated in a repetitive galloping pattern (HLH-HLH-HLH…) in 2 experiments. These listeners could hear the gallop when the sequence was perceived as a single stream, but when it segregated into 2 substreams, they heard H-H-… in 1 stream and L-L-… in the other. The onset-to-onset time of the tones, their duration, the ISI between tones of the same frequency, and the frequency separation between H and L tones were varied. Ss' ratings on a 7-point scale showed that the well-known effect of speed's increasing stream segregation is primarily due to its effect on the ISI between tones in the same frequency region. This has implications for several theories of streaming. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {3},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {2000-04},
  pages = {626-636},
  keywords = {Auditory Perception,auditory stream segregation,Pitch (Frequency),Auditory Discrimination,Stimulus Frequency,college students,Interstimulus Interval,interstimulus intervals & tone durations},
  author = {Bregman, Albert S. and Ahad, Pierre A. and Crum, Poppy A. C. and O'Reilly, Julie}
}

@article{rogersExperimentalEvaluationThree1993,
  title = {An Experimental Evaluation of Three Theories of Auditory Stream Segregation},
  volume = {53},
  issn = {0031-5117},
  doi = {10.3758/BF03211728},
  abstract = {In 2-part trials, 21 adults and 16 university students heard an induction sequence, whose effects on an immediately subsequent test sequence were measured. The rhythm and total duration of induction sequence tones were varied in 2 experiments. The similarity between induction and test sequences aided segregation, but rhythmic predictability and longer tone durations did not. Frequency alternation during the induction sequence was not necessary to induce segregation in the test sequence. In a 3rd experiment, peripheral processes inadequately accounted for the segregation effects found. Data suggest that, once a distinct percept emerges from an auditory scene, properties derived from the percept are fed back to control the ongoing analysis of that auditory scene. A neural adaptation to stimuli with constant properties may form part of this analysis. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {2},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1993-02},
  pages = {179-189},
  keywords = {auditory stream segregation,Pitch (Frequency),Pitch Discrimination,tone frequency alternation},
  author = {Rogers, Wendy L. and Bregman, Albert S.}
}

@article{kellerMusicalMeterAttention2005,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2005.22.4.629},
  title = {Musical {{Meter}} in {{Attention}} to {{Multipart Rhythm}}},
  volume = {22},
  issn = {0730-7829},
  doi = {10.1525/mp.2005.22.4.629},
  abstract = {Performing in musical ensembles can be viewed as a dual task that requires simultaneous attention to a high priority �target� auditory pattern (e.g., a performer�s own part) and either (a) another part in the ensemble or (b) the aggregate texture that results when all parts are integrated. The current study tested the hypothesis that metric frameworks (rhythmic schemas) promote the efficient allocation of attentional resources in such multipart musical contexts. Experiment 1 employed a recognition memory paradigm to investigate the effects of attending to metrical versus nonmetrical target patterns upon the perception of aggregate patterns in which they were embedded. Experiment 2 required metrical and nonmetrical target patterns to be reproduced while memorizing different, concurrently presented metrical patterns that were also subsequently reproduced. Both experiments included conditions in which the different patterns within the multipart structure were matched or mismatched in terms of best-fitting meter. Results indicate that dual-task performance was best in matched-metrical conditions, intermediate in mismatched-metrical conditions, and worst in nonmetrical conditions. This suggests that metric frameworks may facilitate complex musical interactions by enabling efficient allocation of attentional resources.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2005-06-01},
  pages = {629-661},
  author = {Keller, Peter E. and Burnham, Denis K.},
  file = {D\:\\Sauve\\Zotero\\storage\\PZZK368A\\Keller and Burnham - 2005 - Musical Meter in Attention to Multipart Rhythm.pdf}
}

@article{vurmaProductionPerceptionMusical2006,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2006.23.4.331},
  title = {Production and {{Perception}} of {{Musical Intervals}}},
  volume = {23},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.23.4.331},
  abstract = {This Article Reports Two Experiments. In the first experiment, 13 professional singers performed a vocal exercise consisting of three ascending and descending melodic intervals: minor second, tritone, and perfect fifth. Seconds were sung more narrowly but fifths more widely in both directions, as compared to their equally tempered counterparts. In the second experiment, intonation accuracy in performances recorded from the first experiment was evaluated in a listening test. Tritones and fifths were more frequently classified as out of tune than seconds. Good correspondence was found between interval tuning and the listeners responses. The performers themselves evaluated their performance almost randomly in the immediate post-performance situation but acted comparably to the independent group after listening to their own recording. The data suggest that melodic intervals may be, on an average, 20 to 25 cents out of tune and still be estimated as correctly tuned by expert listeners.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2006-04-01},
  pages = {331-344},
  keywords = {melody},
  author = {Vurma, Allan and Ross, Jaan},
  file = {D\:\\Sauve\\Zotero\\storage\\IW4SWP4D\\Vurma and Ross - 2006 - Production and Perception of Musical Intervals.pdf}
}

@article{honingComputationalModelingMusic2006,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2006.23.5.365},
  title = {Computational {{Modeling}} of {{Music Cognition}}: {{A Case Study}} on {{Model Selection}}},
  volume = {23},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.23.5.365},
  shorttitle = {Computational {{Modeling}} of {{Music Cognition}}},
  abstract = {While the most common way of evaluating a computational model is to see whether it shows a good fit with the empirical data, recent literature on theory testing and model selection criticizes the assumption that this is actually strong evidence for the validity of a model. This article presents a case study from music cognition (modeling the ritardandi in music performance) and compares two families of computational models (kinematic and perceptual) using three different model selection criteria: goodness-of-fit, model simplicity, and the degree of surprise in the predictions. In the light of what counts as strong evidence for a model’s validity—namely that it makes limited range, nonsmooth, and relatively surprising predictions—the perception-based model is preferred over the kinematic model.},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2006-06-01},
  pages = {365-376},
  author = {Honing, Henkjan},
  file = {D\:\\Sauve\\Zotero\\storage\\PP46J7TD\\Honing - 2006 - Computational Modeling of Music Cognition A Case .pdf}
}

@article{rammsayerTemporalInformationProcessing2006,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2006.24.1.37},
  title = {Temporal {{Information Processing}} in {{Musicians}} and {{Nonmusicians}}},
  volume = {24},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.24.1.37},
  abstract = {The present study was designed to examine the general notion that temporal information processing is more accurate in musicians than in nonmusicians. For this purpose, 36 academically trained musicians and 36 nonmusicians performed seven different auditory temporal tasks. Superior temporal acuity for musicians compared to nonmusicians was shown for auditory fusion, rhythm perception, and three temporal discrimination tasks. The two groups did not differ, however, in terms of their performance on two tasks of temporal generalization. Musicians’superior performance appeared to be limited to aspects of timing which are considered to be automatically and immediately derived from online perceptual processing of temporal information. Unlike immediate online processing of temporal information, temporal generalizations, which involve a reference memory of sorts, seemed not to be influenced by extensive music training.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2006-09-01},
  pages = {37-48},
  author = {Rammsayer, Thomas and Altenmüller, Eckart},
  file = {D\:\\Sauve\\Zotero\\storage\\TTX2EHDZ\\Rammsayer and Altenmüller - 2006 - Temporal Information Processing in Musicians and N.pdf}
}

@article{neuhausProcessingRhythmicMelodic2006,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2006.24.2.209},
  title = {Processing of {{Rhythmic}} and {{Melodic Gestalts}}—{{An ERP Study}}},
  volume = {24},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.24.2.209},
  abstract = {In two experiments with event-related potentials (ERPs), we investigated the formation of auditory Gestalts. For this purpose, we used tone sequences of different structure. In the first experiment, we contrasted a rhythmic section to a section with random time values, each embedded in rhythmically irregular context. In the second experiment, melodies were contrasted to randomized sequences. Nonmusicians either had to detect the rhythmic pattern or to memorize short tone excerpts. Random versions in both experiments evoked a significant increase in the amplitude of P1 and P2. Randomized rhythm sections also evoked a late sustained negative potential. The enlarged P1 and P2 for random sequences might reflect stronger integration effort, as the predictability of tone progression was low. Thus, already at the early stage of encoding, sequence processing might be top-down-driven. The late negativity for rhythmically random sections is possibly task-related, reflecting expectancy violation in terms of regularity, since a metrical grid of beats could not be established. The memorizing of tone excerpts did not evoke a late neural correlate. (169)},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2006-12-01},
  pages = {209-222},
  keywords = {melody,ERP,rhythm},
  author = {Neuhaus, Christiane and Knösche, Thomas R.},
  file = {D\:\\Sauve\\Zotero\\storage\\9R592BKF\\Neuhaus and Knösche - 2006 - Processing of Rhythmic and Melodic Gestalts—An ERP.pdf}
}

@article{eckIdentifyingMetricalTemporal2006,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2006.24.2.167},
  title = {Identifying {{Metrical}} and {{Temporal Structure With}} an {{Autocorrelation Phase Matrix}}},
  volume = {24},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.24.2.167},
  abstract = {This article introduces a new method for detecting long-timescale structure in music. We describe a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting Autocorrelation Phase Matrix (APM) is useful for several tasks involving metrical structure. In this article we describe the details of calculating the APM. We then show how phase-related regularities from music are stored in the APM and present two ways to recover these regularities. The simpler approach uses variance or entropy calculated on the distribution of information in the APM. The more complex approach explicitly searches through the phase and lag space of the APM to predict meter and tempo in parallel. We compare these approaches against standard autocorrelation for the task of tempo prediction on a relatively large database of annotated digital audio files. We demonstrate that better tempo prediction is achieved by exploiting the phase-related information in the APM.We argue that the APM is an effective data structure for tempo prediction and related applications, such as real-time beat induction and music analysis.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2006-12-01},
  pages = {167-176},
  keywords = {rhythm,model},
  author = {Eck, Douglas},
  file = {D\:\\Sauve\\Zotero\\storage\\VB8PW8D6\\Eck - 2006 - Identifying Metrical and Temporal Structure With a.pdf}
}

@article{bisphamRhythmMusicWhat2006,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2006.24.2.125},
  title = {Rhythm in {{Music}}: {{What}} Is It? {{Who}} Has It? {{And Why}}?},
  volume = {24},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.24.2.125},
  shorttitle = {Rhythm in {{Music}}},
  abstract = {This article explores human rhythmic abilities and behaviors within a framework of evolutionary theory highlighting the need for research in this area to be grounded upon solid psychologically valid definitions of rhythm. A wide-ranging cross-species comparison of rhythmic or quasi-rhythmic behaviors is presented with a view to exploring possible homologies and homoplasies to rhythm in human music. Sustained musical pulse and period correction mechanisms are put forward as human-specific and music-specific traits. Finally hypotheses as to why these abilities may have been selected for—and uniquely selected for—in the course of human evolution are explored.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2006-12-01},
  pages = {125-134},
  keywords = {rhythm},
  author = {Bispham, John},
  file = {D\:\\Sauve\\Zotero\\storage\\RUTQUMVM\\Bispham - 2006 - Rhythm in Music What is it Who has it And Why.pdf}
}

@article{reppTappingVerySlow2007,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2007.24.4.367},
  title = {Tapping to a {{Very Slow Beat}}: {{A Comparison}} of {{Musicians}} and {{Nonmusicians}}},
  volume = {24},
  issn = {0730-7829},
  doi = {10.1525/mp.2007.24.4.367},
  shorttitle = {Tapping to a {{Very Slow Beat}}},
  abstract = {WHEN NONMUSICIANS TAP with isochronous auditory tone sequences, the taps typically precede the tone onsets. However, when the tone inter-onset interval (IOI) is increased beyond 2 s, an increasing proportion of taps follows the tone onsets by 150 ms or more. Such responses indicate reactions rather than anticipations, and they have been interpreted as reflecting a rate limit of synchronization related to a temporal limit of auditory working memory. In the present study, musicians and nonmusicians were asked to synchronize their taps with sequences whose IOIs ranged from 1000 to 3500 ms. Nonmusicians showed much larger anticipation errors and higher variability but actually fewer reactive responses than musicians. No clear landmarks of a rate limit for synchronization were observed.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2007-04-01},
  pages = {367-376},
  keywords = {rhythm,musical training},
  author = {Repp, Bruno H. and Doggett, Rebecca},
  file = {D\:\\Sauve\\Zotero\\storage\\5PEHMP9F\\Repp and Doggett - 2007 - Tapping to a Very Slow Beat A Comparison of Music.pdf}
}

@article{lerdahlModelingTonalTension2007,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2007.24.4.329},
  title = {Modeling {{Tonal Tension}}},
  volume = {24},
  issn = {0730-7829},
  doi = {10.1525/mp.2007.24.4.329},
  abstract = {THIS STUDY PRESENTS AND TESTS a theory of tonal tension (Lerdahl, 2001). The model has four components: prolongational structure, a pitch-space model, a surfacetension model, and an attraction model. These components combine to predict the rise and fall in tension in the course of listening to a tonal passage or piece. We first apply the theory to predict tension patterns in Classical diatonic music and then extend the theory to chromatic tonal music. In the experimental tasks, listeners record their experience of tension for the excerpts. Comparisons between predictions and data point to alternative analyses within the constraints of the theory. We conclude with a discussion of the underlying perceptual and cognitive principles engaged by the theory's components.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2007-04-01},
  pages = {329-366},
  keywords = {model,harmony},
  author = {Lerdahl, Fred and Krumhansl, Carol L.},
  file = {D\:\\Sauve\\Zotero\\storage\\CS8R6AFK\\Lerdahl and Krumhansl - 2007 - Modeling Tonal Tension.pdf}
}

@article{temperleyPitchClassDistributionIdentification2008,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2008.25.3.193},
  title = {Pitch-{{Class Distribution}} and the {{Identification}} of {{Key}}},
  volume = {25},
  issn = {0730-7829},
  doi = {10.1525/mp.2008.25.3.193},
  abstract = {THIS STUDY EXAMINES THE DISTRIBUTIONAL VIEW OF key-finding, which holds that listeners identify key by monitoring the distribution of pitch-classes in a piece and comparing this to an ideal distribution for each key. In our experiment, participants judged the key of melodies generated randomly from pitch-class distributions characteristic of tonal music. Slightly more than half of listeners' judgments matched the generating keys, on both the untimed and the timed conditions. While this performance is much better than chance, it also indicates that the distributional view is far from a complete explanation of human key identification. No difference was found between participants with regard to absolute pitch ability, either in the speed or accuracy of their key judgments. Several key-finding models were tested on the melodies to see which yielded the best match to participants' responses.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2008-02-01},
  pages = {193-212},
  author = {Temperley, David and Marvin, Elizabeth West},
  file = {D\:\\Sauve\\Zotero\\storage\\M8SRAQJF\\Temperley and Marvin - 2008 - Pitch-Class Distribution and the Identification of.pdf}
}

@article{bharuchaExpectationImplicitProcess2008,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2008.25.5.477a},
  title = {Expectation as an {{Implicit Process}}},
  volume = {25},
  issn = {0730-7829},
  doi = {10.1525/mp.2008.25.5.477a},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2008-06-01},
  pages = {477-478},
  keywords = {expectation},
  author = {Bharucha, Jamshed J.},
  file = {D\:\\Sauve\\Zotero\\storage\\G274Z9F2\\Bharucha - 2008 - Expectation as an Implicit Process.pdf}
}

@article{habibWhatMusicTraining2009,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2009.26.3.279},
  title = {What Do {{Music Training}} and {{Musical Experience Teach Us About Brain Plasticity}}?},
  volume = {26},
  issn = {0730-7829},
  doi = {10.1525/mp.2009.26.3.279},
  abstract = {THIS ARTICLE SUMMARIZES THE MAIN EVIDENCE TO date regarding links between the brain and music. Musical expertise, often linked to early and intensive learning, is associated with neuroanatomical distinctive features that have been demonstrated through modern neuroimaging techniques, especially magnetic resonance imaging (MRI). These distinctive features are present in several brain regions, all more or less involved either in gestural motor skill (therefore probably related to the use of an instrument) or auditory perception. There also is growing evidence that learning music has more general effects on brain plasticity. One important notion, related to this topic, is that of a probable "sensitive period," around 7 years of age, beyond which music-induced structural changes and learning effects are less pronounced. These data are discussed in the perspective of using music training for remediation in children with specific language and reading disorders.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2009-02-01},
  pages = {279-285},
  keywords = {musical training},
  author = {Habib, Michel and Besson, Mireille},
  file = {D\:\\Sauve\\Zotero\\storage\\2TCSQ9TM\\Habib and Besson - 2009 - What do Music Training and Musical Experience Teac.pdf}
}

@article{rauscherMusicInstructionIts2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.215},
  title = {Music {{Instruction}} and Its {{Diverse Extra}}-{{Musical Benefits}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.215},
  abstract = {this article provides an overview of our research, including studies yet unpublished, on the effects of music on cognition. Music instruction can enhance children's spatial-temporal reasoning, numerical reasoning, and phonemic awareness. Longitudinal studies of middle-income and economically disadvantaged preschoolers reveal that children who receive music instruction prior to age 7 show improved performance on spatial-temporal and numerical reasoning tasks compared to children in control groups—effects that persist for two years after the intervention ends. Three additional studies suggest that teacher gender may influence these transfer effects in children. Our studies also show improved perceptual discrimination as a function of music training: adult string players have lower than average pitch discrimination thresholds, whereas adult percussionists have lower than average temporal discrimination thresholds. These effects are strongest for musicians who begin their training before age 7. Related to these improvements in perceptual discrimination, children provided with violin instruction perform better than controls on tasks measuring phonemic awareness, a skill that correlates strongly with pitch discrimination and is related to reading acquisition.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {215-226},
  keywords = {musical training},
  author = {Rauscher, Frances H. and Hinton, Sean C.},
  file = {D\:\\Sauve\\Zotero\\storage\\S3MISAKC\\Rauscher and Hinton - 2011 - Music Instruction and its Diverse Extra-Musical Be.pdf}
}

@article{schellenbergMusicLessonsEmotional2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.185},
  title = {Music {{Lessons}}, {{Emotional Intelligence}}, and {{IQ}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.185},
  abstract = {musically trained and untrained participants were administered tests of emotional intelligence and IQ. As in previous research, trained participants scored higher than untrained participants on the IQ Composite score and on its Verbal and Nonverbal subtests. The advantage for the trained group on the Composite score and on the Nonverbal subtest was evident even when gender, parents' education, family income, and first language were held constant. The groups performed similarly, however, on the test of emotional intelligence, and scores on the IQ test were only weakly correlated with scores on the emotional intelligence test. The results imply that (1) associations between music lessons and nonmusical abilities are limited to intellectual abilities, and/or (2) associations between music lessons and emotional intelligence are not evident on visual- and/or text-based tests of emotional intelligence such as the one used here.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {185-194},
  keywords = {musical training},
  author = {Schellenberg, E. Glenn},
  file = {D\:\\Sauve\\Zotero\\storage\\5N65HIUQ\\Schellenberg - 2011 - Music Lessons, Emotional Intelligence, and IQ.pdf}
}

@article{patstonEffectBackgroundMusic2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.173},
  title = {The {{Effect}} of {{Background Music}} on {{Cognitive Performance}} in {{Musicians}} and {{Nonmusicians}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.173},
  abstract = {there is debate about the extent of overlap between music and language processing in the brain and whether these processes are functionally independent in expert musicians. A language comprehension task and a visuospatial search task were administered to 36 expert musicians and 36 matched nonmusicians in conditions of silence and piano music played correctly and incorrectly. Musicians performed more poorly on the language comprehension task in the presence of background music compared to silence, but there was no effect of background music on the musicians' performance on the visuospatial task. In contrast, the performance of nonmusicians was not affected by music on either task. The findings challenge the view that music and language are functionally independent in expert musicians, and instead suggest that when musicians process music they recruit a network that overlaps with the network used in language processing. Additionally, musicians outperformed nonmusicians on both tasks, reflecting either a general cognitive advantage in musicians or enhancement of more specific cognitive abilities such as processing speed or executive functioning.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {173-183},
  keywords = {musical training},
  author = {Patston, Lucy L. M. and Tippett, Lynette J.},
  file = {D\:\\Sauve\\Zotero\\storage\\SMMNF9C9\\Patston and Tippett - 2011 - The Effect of Background Music on Cognitive Perfor.pdf}
}

@article{corrigallAssociationsLengthMusic2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.147},
  title = {Associations {{Between Length}} of {{Music Training}} and {{Reading Skills}} in {{Children}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.147},
  abstract = {previous research has found that music training in childhood is associated with word decoding, a fundamental reading skill related to the ability to pronounce individual words. These findings have typically been explained by a near transfer mechanism because music lessons train auditory abilities associated with those needed for decoding words. Nevertheless, few studies have examined whether music training is associated with higher-level reading abilities such as reading comprehension, which would suggest far transfer. We tested whether the length of time children took music lessons was associated with word decoding and reading comprehension skills in 6- to 9-year-old normal-achieving readers. Our results revealed that length of music training was not associated with word decoding skills; however, length of music training predicted reading comprehension performance even after controlling for age, socioeconomic status, auditory perception, full-scale IQ, the number of hours that children spent reading per week, and word decoding skills. We suggest that if near transfer occurs, it is likely strongest in beginning readers or those experiencing reading difficulty. The strong association in our data—between length of music training and reading comprehension—is consistent with mechanisms involving far transfer.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {147-155},
  keywords = {musical training},
  author = {Corrigall, Kathleen A. and Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\9MMDX2WU\\Corrigall and Trainor - 2011 - Associations Between Length of Music Training and .pdf}
}

@article{morenoEffectMusicTraining2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.165},
  title = {Effect of {{Music Training}} on {{Promoting Preliteracy Skills}}: {{Preliminary Causal Evidence}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.165},
  shorttitle = {Effect of {{Music Training}} on {{Promoting Preliteracy Skills}}},
  abstract = {the present study investigated whether music training fosters children's preliteracy skills. Sixty children were randomly assigned to participate in a 20-day training program in either music or visual art. Before and after training, children's phonological awareness and their ability to map visual symbols onto words (i.e., visual-auditory learning) were assessed. Equivalent improvement after training was observed for both groups on the phonological awareness measure, but the children with music training improved significantly more than the art-trained children on the visual-auditory learning measure. Music training appears to benefit certain skills necessary for reading.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {165-172},
  keywords = {musical training},
  author = {Moreno, Sylvain and Friesen, Deanna and Bialystok, Ellen},
  file = {D\:\\Sauve\\Zotero\\storage\\4BQ6PFA6\\Moreno et al. - 2011 - Effect of Music Training on Promoting Preliteracy .pdf}
}

@article{tsangMusicTrainingReading2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.157},
  title = {Music {{Training}} and {{Reading Readiness}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.157},
  abstract = {several reports have noted significant associations among phonological awareness, early reading skills, and music perception skills in young children. We examined whether music processing skills differentially predicted reading performance in a broad age range of 69 children with and without formal music training. Pitch perception was correlated with phonological awareness, a finding consistent with the hypothesis that basic auditory processing skills underlie the association between music and reading abilities. Nevertheless, the correlation between music skills and reading skills was affected by the presence of formal music training: pitch discrimination predicted reading ability only in children without formal music training. Studies examining the association between music perception and reading (and perhaps other cognitive domains as well) should not ignore the factor of music training.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {157-163},
  keywords = {musical training},
  author = {Tsang, Christine D. and Conrad, Nicole J.},
  file = {D\:\\Sauve\\Zotero\\storage\\JJBFQFNW\\Tsang and Conrad - 2011 - Music Training and Reading Readiness.pdf}
}

@article{rammsayerMusiciansBetterNonmusicians2012,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2012.30.1.85},
  title = {Musicians {{Do Better}} than {{Nonmusicians}} in {{Both Auditory}} and {{Visual Timing Tasks}}},
  volume = {30},
  issn = {0730-7829},
  doi = {10.1525/mp.2012.30.1.85},
  abstract = {the present study was designed to investigate differences in auditory and visual temporal information processing between musicians and nonmusicians. For this purpose, timing performance on a set of six different psychophysical temporal tasks for both the auditory and visual sensory modalities was compared in 40 formally trained musicians and 40 controls without musical experience. Across modalities, superior temporal acuity for musicians compared to nonmusicians could be shown for all temporal tasks except for temporal generalization. When comparing the two sensory modalities, temporal acuity was superior to auditory stimuli as compared to visual stimuli, with the exception of the temporal generalization task in the 1-s range. The overall pattern of our findings is consistent with the notion that musicians' long-lasting intensive music training, starting in childhood, improves general timing ability irrespective of sensory modality.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2012-09-01},
  pages = {85-96},
  keywords = {musical training},
  author = {Rammsayer, Thomas H. and Buttkus, Franziska and Altenmüller, Eckart},
  file = {D\:\\Sauve\\Zotero\\storage\\ANKU2WS3\\Rammsayer et al. - 2012 - Musicians Do Better than Nonmusicians in Both Audi.pdf}
}

@article{tardieuPerceptionDyadsImpulsive2012,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2012.30.2.117},
  title = {Perception of {{Dyads}} of {{Impulsive}} and {{Sustained Instrument Sounds}}},
  volume = {30},
  issn = {0730-7829},
  doi = {10.1525/mp.2012.30.2.117},
  abstract = {perception of instrumental blend is important for understanding aspects of orchestration, but no work has studied blends of impulsive and sustained instruments. The first experiment identified the factors that influence the rating of blendedness of dyads formed of one sustained sound and one impulsive sound. Longer attack times and lower spectral centroids increased blend. The contribution of the impulsive sound’s properties to the degree of blend was greater than that of the sustained sound. The second experiment determined the factors that influence similarity ratings among dyads. The mean spectral envelope and the attack time of the dyad best explained the dissimilarity ratings. However, contrary to the first experiment, the spectral envelope of the sustained sound was more important than that of the impulsive sound. Multidimensional scaling of dissimilarity ratings on blended dyads yielded one dimension correlated with the attack time of the dyad and another dimension whose spectral correlate was different for two different clusters within the space, spectral spread for one and spectral flatness for the other, suggesting a combined categorical-analogical organization of the second dimension.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2012-12-01},
  pages = {117-128},
  keywords = {timbre},
  author = {Tardieu, Damien and McAdams, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\DT7DWEWW\\Tardieu and McAdams - 2012 - Perception of Dyads of Impulsive and Sustained Ins.pdf}
}

@article{marozeauEffectTimbreLoudness2013,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2012.30.3.259},
  title = {The {{Effect}} of {{Timbre}} and {{Loudness}} on {{Melody Segregation}}},
  volume = {30},
  issn = {0730-7829},
  doi = {10.1525/mp.2012.30.3.259},
  abstract = {The aim of this study was to examine the effects of three acoustic parameters on the difficulty of segregating a simple 4-note melody from a background of interleaved distractor notes. Melody segregation difficulty ratings were recorded while three acoustic parameters of the distractor notes were varied separately: intensity, temporal envelope, and spectral envelope. Statistical analyses revealed a significant effect of music training on difficulty rating judgments. For participants with music training, loudness was the most efficient perceptual cue, and no difference was found between the dimensions of timbre influenced by temporal and spectral envelope. For the group of listeners with less music training, both loudness and spectral envelope were the most efficient cues. We speculate that the difference between musicians and nonmusicians may be due to differences in processing the stimuli: musicians may process harmonic sound sequences using brain networks specialized for music, whereas nonmusicians may use speech networks.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2013-02-01},
  pages = {259-274},
  keywords = {timbre,streaming},
  author = {Marozeau, Jeremy and Innes-Brown, Hamish and Blamey, Peter J.},
  file = {D\:\\Sauve\\Zotero\\storage\\D33928BG\\Marozeau et al. - 2013 - The Effect of Timbre and Loudness on Melody Segreg.pdf}
}

@article{poudrierCanMusiciansTrack2013,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2013.30.4.369},
  title = {Can {{Musicians Track Two Different Beats Simultaneously}}?},
  volume = {30},
  issn = {0730-7829},
  doi = {10.1525/mp.2013.30.4.369},
  abstract = {The simultaneous presence of different meters is not uncommon in Western art music and the music of various non-Western cultures. However, it is unclear how listeners and performers deal with this situation, and whether it is possible to cognitively establish and maintain different beats simultaneously without integrating them into a single metric framework. The present study is an attempt to address this issue empirically. Two rhythms, distinguished by pitch register and representing different meters (2/4 and 6/8), were presented simultaneously in various phase relationships, and participants (who were classically trained musicians) had to judge whether a probe fell on the beat in one or both rhythms. In a selective attention condition, they had to attend to one rhythm and to ignore the other, whereas in a divided attention condition, they had to attend to both. In Experiment 1, participants performed significantly better in the divided attention condition than predicted if they had been able to attend to only one rhythm at a time. In Experiments 2 and 3, however, which used more complex combinations of rhythms, performance did not differ significantly from chance. These results suggest that in Experiment 1 participants relied on the composite beat pattern (i.e., a nonisochronous sequence corresponding to the serial ordering of the two underlying beats) rather than tracking the two beats independently, while in Experiments 2 and 3, the level of complexity of the composite beat pattern may have prevented participants from tracking both beats simultaneously.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2013-04-01},
  pages = {369-390},
  keywords = {rhythm,musical training},
  author = {Poudrier, Ève and Repp, Bruno H.},
  file = {D\:\\Sauve\\Zotero\\storage\\FW46AMFJ\\Poudrier and Repp - 2013 - Can Musicians Track Two Different Beats Simultaneo.pdf}
}

@article{habibiCorticalActivityPerception2013,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2013.30.5.463},
  title = {Cortical {{Activity During Perception}} of {{Musical Pitch}}: {{Comparing Musicians}} and {{Nonmusicians}}},
  volume = {30},
  issn = {0730-7829},
  doi = {10.1525/mp.2013.30.5.463},
  shorttitle = {Cortical {{Activity During Perception}} of {{Musical Pitch}}},
  abstract = {This study investigates the effects of music training on brain activity to violations of melodic expectancies. We recorded behavioral and event-related brain potential (ERP) responses of musicians and nonmusicians to discrepancies of pitch between pairs of unfamiliar melodies based on Western classical rules. Musicians detected pitch deviations significantly better than nonmusicians. In musicians compared to nonmusicians, auditory cortical potentials to notes but not unrelated warning tones exhibited enhanced P200 amplitude generally, and in response to pitch deviations enhanced amplitude for N150 and P300 (P3a) but not N100 was observed. P3a latency was shorter in musicians compared to nonmusicians. Both the behavioral and cortical activity differences observed between musicians and nonmusicians in response to deviant notes were significant with stimulation of the right but not the left ear, suggesting that left-sided brain activity differentiated musicians from nonmusicians. The enhanced amplitude of N150 among musicians with right ear stimulation was positively correlated with earlier age onset of music training. Our data support the notion that long-term music training in musicians leads to functional reorganization of auditory brain systems, and that these effects are potentiated by early age onset of training.},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2013-06-01},
  pages = {463-479},
  keywords = {musical training},
  author = {Habibi, Assal and Wirantana, Vinthia and Starr, Arnold},
  file = {D\:\\Sauve\\Zotero\\storage\\G37H28EN\\Habibi et al. - 2013 - Cortical Activity During Perception of Musical Pit.pdf}
}

@article{londonBuildingRepresentativeCorpus2013,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2013.31.1.68},
  title = {Building a {{Representative Corpus}} of {{Classical Music}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2013.31.1.68},
  abstract = {This paper presents an object lesson in the challenges and considerations involved in assembling a musical corpus for empirical research. It develops a model for the construction of a representative corpus of classical music of the “common practice period” (1700-1900), using both specific composers as well as broader historical styles and musical genres (e.g., symphony, chamber music, songs, operas) as its sampling parameters. Five sources were used in the construction of the model: (a) The Oxford History of Western Music by Richard Taruskin (2005), (b) amalgamated Orchestral Repertoire Reports for the years 2000-2007, from the League of American Orchestras, (c) a list of titles from the Naxos.com “Music in the Movies” web-based library, (d) Barlow and Morgenstern’s Dictionary of Musical Themes (1948), and (e) for the composers listed in sources (a)-(d), counts of the number of recordings each has available from Amazon.com. General considerations for these sources are discussed, and specific aspects of each source are then detailed. Intersource agreement is assessed, showing strong consensus among all sources, save for the Taruskin History. Using the Amazon.com data to determine weighting factors for each parameter, a preliminary sampling model is proposed. Including adequate genre representation leads to a corpus of ≈300 pieces, suggestive of the minimum size for an adequately representative corpus of classical music. The approaches detailed here may be applied to more specialized contexts, such as the music of a particular geographic region, historical era, or genre.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2013-09-01},
  pages = {68-90},
  keywords = {corpus},
  author = {London, Justin},
  file = {D\:\\Sauve\\Zotero\\storage\\76ES9MS7\\London - 2013 - Building a Representative Corpus of Classical Musi.pdf}
}

@article{duaneAuditoryStreamingCues2013,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2013.31.1.46},
  title = {Auditory {{Streaming Cues}} in {{Eighteenth}}- and {{Early Nineteenth}}-{{Century String Quartets}}: {{A Corpus}}-{{Based Study}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2013.31.1.46},
  shorttitle = {Auditory {{Streaming Cues}} in {{Eighteenth}}- and {{Early Nineteenth}}-{{Century String Quartets}}},
  abstract = {This study uses a corpus of excerpts from eighteenth- and early nineteenth-century string quartets to examine how four acoustic cues—onset and offset synchrony, pitch comodulation, and spectral overlap—help to afford the perception of auditory streams. Two types of streams are dealt with: textural streams, which house individual string parts or groups of them that function as single musical units; and music streams, which typically house the music as a whole and distinguish it from other simultaneous sounds of music. The corpus contained real excerpts from classical string quartets as well as synthesized excerpts in which lines from two different quartets were combined. Both the author and ten survey respondents analyzed the corpus, identifying likely textural streams. Each of the four acoustic cues was modeled computationally, in order to assess its prevalence in textural and music streams found in the corpus. The results suggested that some cues are more important than others in establishing textural streams, music streams, or both.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2013-09-01},
  pages = {46-58},
  keywords = {model,streaming},
  author = {Duane, Ben},
  file = {D\:\\Sauve\\Zotero\\storage\\6XVTIEUP\\Duane - 2013 - Auditory Streaming Cues in Eighteenth- and Early N.pdf}
}

@article{albrechtUseLargeCorpora2013,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2013.31.1.59},
  title = {The {{Use}} of {{Large Corpora}} to {{Train}} a {{New Type}} of {{Key}}-{{Finding Algorithm}}: {{An Improved Treatment}} of the {{Minor Mode}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2013.31.1.59},
  shorttitle = {The {{Use}} of {{Large Corpora}} to {{Train}} a {{New Type}} of {{Key}}-{{Finding Algorithm}}},
  abstract = {Computational models of key estimation have struggled to emulate the accuracy levels of human listeners, especially with pieces in the minor mode. The current study proposes a new key-finding algorithm, which utilizes Euclidean distance, rather than correlation, and is trained on the statistical properties of a large musical sample. A model was trained on a dataset of 490 pieces encoded into the Humdrum “kern” format, in which the key was known. This model was tested on a reserve dataset of 492 pieces, and was found to have a significantly higher overall accuracy than previous models. In addition, we determined separate accuracy ratings for major mode and minor mode works for the existing key-finding models and report that most existing models provide greater accuracy for major mode rather than minor mode works. The proposed key-finding algorithm performs more accurately on minor mode works than all of the other models tested, although it does not perform significantly better than the models created by Aarden (2003), Bellman (2005), or Sapp (2011). Finally, an algorithm that combines the Aarden-Essen model (2003) and the proposed algorithm is suggested, and results in significantly more accurate key assessments than all of the other extant models.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2013-09-01},
  pages = {59-67},
  keywords = {model,key-finding},
  author = {Albrecht, Joshua and Shanahan, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\U35VCPX5\\Albrecht and Shanahan - 2013 - The Use of Large Corpora to Train a New Type of Ke.pdf}
}

@article{aksentijevicRatespecificEntrainmentHarmonic2014,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2014.31.4.316},
  title = {Rate-Specific {{Entrainment}} of {{Harmonic Pitch}}: {{Effects}} of {{Music Training}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2014.31.4.316},
  shorttitle = {Rate-Specific {{Entrainment}} of {{Harmonic Pitch}}},
  abstract = {Recent evidence indicates that synchronized brain oscillations in the low gamma range (∼33 Hz) are involved in the perceptual integration of harmonic complex tones. At this rate, reaction times (RTs) are faster to targets that are not harmonically related to the prime. In the current study, we investigated the presence of this rate-specific inharmonic pop-out in groups of musicians and nonmusicians. We found that rather than increasing the salience of inharmonic targets, 33-pps priming reduced the salience of harmonic targets. This effect was observed in nonmusicians only, suggesting that music training reduces the role of oscillatory coding mechanisms in the perceptual integration of harmonic information.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2014-04-01},
  pages = {316-322},
  keywords = {musical training},
  author = {Aksentijevic, Aleksandar and Smith, Anthony and Elliott, Mark A.},
  file = {D\:\\Sauve\\Zotero\\storage\\ZS78UXF6\\Aksentijevic et al. - 2014 - Rate-specific Entrainment of Harmonic Pitch Effec.pdf}
}

@article{caoSimilarityFamiliesMusical2014,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2014.31.5.444},
  title = {Similarity and {{Families}} of {{Musical Rhythms}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2014.31.5.444},
  abstract = {What determines the similarity of musical rhythms? According to the “family” theory, which this paper presents, one factor is the temporal sequence of the onsets of notes: rhythms with the same pattern of interonset intervals tend to sound similar. Another factor is meter. It determines whether or not rhythms are members of the same family, where families depend only on three types of possibility for each metrical unit. If the beat is the relevant metrical unit, these three possibilities are: 1) a note starts on a beat and therefore reinforces the meter, 2) a syncopation anticipates the beat and lasts through its onset and therefore disturbs the meter, and 3) all other events such as rests or ties that start on the beat provided no syncopation anticipates them. Two experiments showed that similarity between rhythms depends on both their temporal patterns of onsets and their families, which combined give a better account than edit distance – a metric of the distance apart of two strings of symbols. Two further experiments examined the errors that participants made in reproducing rhythms by tapping them. Errors more often yielded rhythms in the same family as the originals than rhythms in a different family.},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2014-06-01},
  pages = {444-469},
  keywords = {rhythm},
  author = {Cao, Erica and Lotstein, Max and Johnson-Laird, Philip N.},
  file = {D\:\\Sauve\\Zotero\\storage\\WKXIAPET\\Cao et al. - 2014 - Similarity and Families of Musical Rhythms.pdf}
}

@article{brozePolyphonicVoiceMultiplicity2014,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2014.32.2.143},
  title = {Polyphonic {{Voice Multiplicity}}, {{Numerosity}}, and {{Musical Emotion Perception}}},
  volume = {32},
  issn = {0730-7829},
  abstract = {Three experimental studies suggest that music with more musical voices (higher voice multiplicity) tends to be perceived more positively. In the first experiment, participants heard brief extracts from polyphonic keyboard works representing conditions of one, two, three, or four concurrent musical voices. Two basic emotions (happiness and sadness) and two social emotions (pride and loneliness) were rated on a continuous scale. Listeners rated excerpts with higher voice multiplicity as sounding more happy, less sad, less lonely, and more proud. Results from a second experiment indicate that this effect might extend to positive and negative emotions more generally. In a third experiment, participants were asked to count (denumerate) the number of musical voices in the same stimuli. Denumeration responses corresponded closely with ratings for both positive and negative emotions, suggesting that a single musical feature or percept might play a role in both. Possible roles for both symbolic and psychoacoustic musical features are discussed.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2014-12-01},
  pages = {143-159},
  keywords = {emotion,polyphony},
  author = {Broze, Yuri and Paul, Brandon T. and Allen, Erin T. and Guarna, Kathleen M.},
  file = {D\:\\Sauve\\Zotero\\storage\\3DHNU7R5\\Broze et al. - 2014 - Polyphonic Voice Multiplicity, Numerosity, and Mus.pdf}
}

@article{zacharakisInterlanguageUnificationMusical2015,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2015.32.4.394},
  title = {An {{Interlanguage Unification}} of {{Musical Timbre}}: {{Bridging Semantic}}, {{Perceptual}}, and {{Acoustic Dimensions}}},
  volume = {32},
  issn = {0730-7829},
  doi = {10.1525/mp.2015.32.4.394},
  shorttitle = {An {{Interlanguage Unification}} of {{Musical Timbre}}},
  abstract = {The current study expands our previous work on interlanguage musical timbre semantics by examining the relationship between semantics and perception of timbre. Following Zacharakis, Pastiadis, and Reiss (2014), a pairwise dissimilarity listening test involving participants from two separate linguistic groups (Greek and English) was conducted. Subsequent multidimensional scaling analysis produced a 3D perceptual timbre space for each language. The comparison between perceptual spaces suggested that timbre perception is unaffected by native language. Additionally, comparisons between semantic and perceptual spaces revealed substantial similarities which suggest that verbal descriptions can convey a considerable amount of perceptual information. The previously determined semantic labels “auditory texture” and “luminance” featured the highest associations with perceptual dimensions for both languages. “Auditory mass” failed to show any strong correlations. Acoustic analysis identified energy distribution of harmonic partials, spectral detail, temporal/spectrotemporal characteristics and the fundamental frequency as the most salient acoustic correlates of perceptual dimensions.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2015-04-01},
  pages = {394-412},
  keywords = {timbre},
  author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D.},
  file = {D\:\\Sauve\\Zotero\\storage\\2CE4THRT\\Zacharakis et al. - 2015 - An Interlanguage Unification of Musical Timbre Br.pdf}
}

@article{schutzSurveyingTemporalStructure2014,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2014.31.3.288},
  title = {Surveying the {{Temporal Structure}} of {{Sounds Used}} in {{Music Perception}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2014.31.3.288},
  abstract = {Recent work from our lab illustrates amplitude envelope’s crucial role in both perceptual (Schutz, 2009) and cognitive (Schutz \& Stefanucci, 2010) processing. Consequently, we surveyed the amplitude envelopes of sounds used in Music Perception, categorizing them as either flat (i.e., trapezoidal shape), percussive (aka “damped” or “decaying”), other, or undefined. Curiously, the undefined category represented the largest percentage of sounds observed, with 35\% lacking definition of this important property (approximately 27\% were percussive, 27\% flat, and 11\% other). This omission of relevant information was not indicative of general inattention to methodological detail. Studies using tones with undefined amplitude envelopes generally defined other properties such as spectral structure (85\%), duration (80\%), and even model of headphones/speakers (65\%) at high rates. Consequently, this targeted omission is intriguing, and suggests amplitude envelope is an area ripe for future research.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2014-02-01},
  pages = {288-296},
  author = {Schutz, Michael and Vaisberg, Jonathan M.},
  file = {D\:\\Sauve\\Zotero\\storage\\H8TANJ9A\\Schutz and Vaisberg - 2014 - Surveying the Temporal Structure of Sounds Used in.pdf}
}

@online{centerforhistoryandnewmediaZoteroQuickStart,
  title = {Zotero {{Quick Start Guide}}},
  url = {http://zotero.org/support/quick_start_guide},
  author = {{Center for History and New Media}}
}

@article{looiEffectCochlearImplantation2008,
  title = {The Effect of Cochlear Implantation on Music Perception by Adults with Usable Pre-Operative Acoustic Hearing},
  volume = {47},
  issn = {1499-2027},
  doi = {10.1080/14992020801955237},
  abstract = {This study investigated the change in music perception of adults undergoing cochlear implantation. Nine adults scheduled for a cochlear implant (CI) were assessed on a music test battery both prior to implantation (whilst using hearing aids; HAs), and three months after activation of their CIs. The results were compared with data from a group of longer-term CI users and a group of HA-only users. The tests comprised assessments of rhythm, pitch, instrument, and melody perception. Pre-to-post surgery comparisons showed no significant difference in the rhythm, melody, and instrument identification scores. Subjects' scores were significantly lower post-implant for ranking pitch intervals of one octave and a quarter octave (p = 0.007, and p {$<$} 0.001, respectively), and were only at chance levels for the smaller interval. However, although pitch perception was generally poorer with a CI than with a HA, it is likely that the use of both devices simultaneously could have provided higher scores for these subjects. Analysis of the other tests' results provided insights into factors affecting music perception for adults with severe to profound hearing impairment. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {5},
  journaltitle = {International Journal of Audiology},
  shortjournal = {International Journal of Audiology},
  date = {2008-05},
  pages = {257-268},
  keywords = {acoustics,cochlear implantation,cochlear implants,Hearing Disorders,hearing impairment,music perception,pre-operative acoustic hearing},
  author = {Looi, Valerie and McDermott, Hugh and McKay, Colette and Hickson, Louise}
}

@article{swansonInvestigatingCochlearImplant2009,
  title = {Investigating Cochlear Implant Place-Pitch Perception with the {{Modified Melodies Test}}},
  volume = {10},
  issn = {1467-0100},
  doi = {10.1179/cim.2009.10.Supplement-1.100},
  abstract = {There has been speculation that cochlear implant place-of-excitation cues could be more akin to the brightness attribute of timbre (the spectral profile) than to melodic pitch. As brightness can be ordered on a low-to-high scale, it would allow high scores on pitch-ranking tests. In contrast, the Modified Melodies test measures pitch perception in a melodic context. In each trial, a familiar melody was presented twice. In one presentation, randomly selected, the pitch was deliberately modified. The subject's task was to select the un-modified melody. Six Nucleus implant recipients were tested with melodies presented as pure tones in the frequency range C5-C6 (523-1046 Hz) through the ACE strategy on the Freedom processor. All subjects were able to identify incorrect melodic contours and three subjects were able, to recognize errors in musical intervals. These results are consistent with the hypothesis that cochlear implant place cues alone are sufficient to convey a melody. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
  issue = {Suppl1},
  journaltitle = {Cochlear Implants International},
  shortjournal = {Cochlear Implants International},
  date = {2009},
  pages = {100-104},
  keywords = {Stimulus Parameters,cochlear implants,music perception,Cues,Experimentation,investigation,melodic context,Modified Melodies Test,pitch perception,place of excitation cues,Treatment Effectiveness Evaluation},
  author = {Swanson, Brett and Dawson, Pam and McDermott, Hugh}
}

@article{sucherBimodalStimulationBenefits2009,
  title = {Bimodal Stimulation: {{Benefits}} for Music Perception and Sound Quality},
  volume = {10},
  issn = {1467-0100},
  doi = {10.1002/cii.398},
  shorttitle = {Bimodal Stimulation},
  abstract = {With recent expansions in cochlear implantation candidacy criteria, increasing numbers of implantees can exploit their remaining hearing by using bimodal stimulation (combining electrical stimulation via the implant with acoustic stimulation via hearing aids). This study examined the effect of bimodal stimulation on music perception and perceived sound quality. The perception of music and sound quality by nine post-lingually deafened adult implantees was examined in three conditions: implant alone, hearing aid alone and bimodal stimulation. On average, bimodal stimulation provided the best results for music perception and perceived sound quality when compared with results obtained with electrical stimulation alone. Thus, for implantees with usable acoustic hearing, bimodal stimulation may be advantageous when listening to music and other non-speech sounds. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
  issue = {Suppl1},
  journaltitle = {Cochlear Implants International},
  shortjournal = {Cochlear Implants International},
  date = {2009},
  pages = {96-99},
  keywords = {Auditory Stimulation,acoustics,cochlear implants,music perception,Treatment Effectiveness Evaluation,bimodal stimulation,Electrical Stimulation,sound quality},
  author = {Sucher, Catherine M. and McDermott, Hugh J.}
}

@article{mcdermottMusicalPitchPerception1997,
  title = {Musical Pitch Perception with Electrical Stimulation of the Cochlea},
  volume = {101},
  issn = {0001-4966},
  doi = {10.1121/1.418177},
  abstract = {Investigated the ability of a 36-yr-old male musically trained cochlear implant user to judge pitch of electrical stimuli. It was found that the pitch perceived depended on the temporal properties of the stimulation and on the intracochlear location to which the stimulation was delivered. Stimulation rate, or modulation frequency of amplitude-modulated pulse trains, affected frequency in normal hearing. However, the pitch was relatively weak and could be varied only over a narrow range. The place-related pitch varied in accordance with the tonotopic organization of the cochlea. When place and rate changes occurred in combination, the pitch was more affected by the place of stimulation, although the rate also had a slight effect. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {3},
  journaltitle = {Journal of the Acoustical Society of America},
  shortjournal = {Journal of the Acoustical Society of America},
  date = {1997-03},
  pages = {1622-1631},
  keywords = {music perception,pitch perception,Electrical Stimulation,36 yr old male,Cochlea,musical pitch perception & cochlear electrical stimulation},
  author = {McDermott, Hugh J. and McKay, Colette M.}
}

@article{bendixenPredictabilityEffectsAuditory2014,
  title = {Predictability Effects in Auditory Scene Analysis: A Review},
  volume = {8},
  issn = {1662-4548},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3978260/},
  doi = {10.3389/fnins.2014.00060},
  shorttitle = {Predictability Effects in Auditory Scene Analysis},
  abstract = {Many sound sources emit signals in a predictable manner. The idea that predictability can be exploited to support the segregation of one source's signal emissions from the overlapping signals of other sources has been expressed for a long time. Yet experimental evidence for a strong role of predictability within auditory scene analysis (ASA) has been scarce. Recently, there has been an upsurge in experimental and theoretical work on this topic resulting from fundamental changes in our perspective on how the brain extracts predictability from series of sensory events. Based on effortless predictive processing in the auditory system, it becomes more plausible that predictability would be available as a cue for sound source decomposition. In the present contribution, empirical evidence for such a role of predictability in ASA will be reviewed. It will be shown that predictability affects ASA both when it is present in the sound source of interest (perceptual foreground) and when it is present in other sound sources that the listener wishes to ignore (perceptual background). First evidence pointing toward age-related impairments in the latter capacity will be addressed. Moreover, it will be illustrated how effects of predictability can be shown by means of objective listening tests as well as by subjective report procedures, with the latter approach typically exploiting the multi-stable nature of auditory perception. Critical aspects of study design will be delineated to ensure that predictability effects can be unambiguously interpreted. Possible mechanisms for a functional role of predictability within ASA will be discussed, and an analogy with the old-plus-new heuristic for grouping simultaneous acoustic signals will be suggested.},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  urldate = {2015-06-17},
  date = {2014-03-31},
  author = {Bendixen, Alexandra},
  file = {D\:\\Sauve\\Zotero\\storage\\DJ94Z596\\Bendixen - 2014 - Predictability effects in auditory scene analysis.pdf},
  eprinttype = {pmid},
  eprint = {24744695},
  pmcid = {PMC3978260}
}

@article{spielmannAttentionEffectsAuditory2014,
  title = {Attention Effects on Auditory Scene Analysis: {{Insights}} from Event-Related Brain Potentials},
  volume = {78},
  issn = {0340-0727},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2014-06861-001&site=ehost-live},
  doi = {10.1007/s00426-014-0547-7},
  shorttitle = {Attention Effects on Auditory Scene Analysis},
  abstract = {Sounds emitted by different sources arrive at our ears as a mixture that must be disentangled before meaningful information can be retrieved. It is still a matter of debate whether this decomposition happens automatically or requires the listener’s attention. These opposite positions partly stem from different methodological approaches to the problem. We propose an integrative approach that combines the logic of previous measurements targeting either auditory stream segregation (interpreting a mixture as coming from two separate sources) or integration (interpreting a mixture as originating from only one source). By means of combined behavioral and event-related potential (ERP) measures, our paradigm has the potential to measure stream segregation and integration at the same time, providing the opportunity to obtain positive evidence of either one. This reduces the reliance on zero findings (i.e., the occurrence of stream integration in a given condition can be demonstrated directly, rather than indirectly based on the absence of empirical evidence for stream segregation, and vice versa). With this two-way approach, we systematically manipulate attention devoted to the auditory stimuli (by varying their task relevance) and to their underlying structure (by delivering perceptual tasks that require segregated or integrated percepts). ERP results based on the mismatch negativity (MMN) show no evidence for a modulation of stream integration by attention, while stream segregation results were less clear due to overlapping attention-related components in the MMN latency range. We suggest future studies combining the proposed two-way approach with some improvements in the ERP measurement of sequential stream segregation. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  urldate = {2015-06-17},
  date = {2014-05},
  pages = {361-378},
  keywords = {Attention,brain,Evoked Potentials,Auditory Stimulation,attention effects,Auditory Evoked Potentials,auditory scene analysis,auditory stimuli,event-related brain potentials},
  author = {Spielmann, Mona Isabel and Schröger, Erich and Kotz, Sonja A. and Bendixen, Alexandra},
  file = {D\:\\Sauve\\Zotero\\storage\\EJ7WAZB4\\Spielmann et al. - 2014 - Attention effects on auditory scene analysis Insi.pdf}
}

@article{haywoodBuildupAuditoryStream2013,
  title = {Build-up of Auditory Stream Segregation Induced by Tone Sequences of Constant or Alternating Frequency and the Resetting Effects of Single Deviants},
  volume = {39},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2013-17510-001&site=ehost-live},
  doi = {10.1037/a0032562},
  abstract = {A sequence of constant-frequency tones can promote streaming in a subsequent sequence of alternating-frequency tones, but why this effect occurs is not fully understood and its time course has not been investigated. Experiment 1 used a 2.0-s-long constant-frequency inducer (10 repetitions of a low-frequency pure tone) to promote segregation in a subsequent, 1.2-s test sequence of alternating low- and high-frequency tones. Replacing the final inducer tone with silence substantially reduced reported test-sequence segregation. This reduction did not occur when either the 4th or 7th inducer was replaced with silence. This suggests that a change at the induction/test-sequence boundary actively resets build-up, rather than less segregation occurring simply because fewer inducer tones were presented. Furthermore, Experiment 2 found that a constant-frequency inducer produced its maximum segregation-promoting effect after only three tones—this contrasts with the more gradual build-up typically observed for alternating-frequency sequences. Experiment 3 required listeners to judge continuously the grouping of 20-s test sequences. Constant-frequency inducers were considerably more effective at promoting segregation than alternating ones; this difference persisted for ∼10 s. In addition, resetting arising from a single deviant (longer tone) was associated only with constant-frequency inducers. Overall, the results suggest that constant-frequency inducers promote segregation by capturing one subset of test-sequence tones into an ongoing, preestablished stream, and that a deviant tone may reduce segregation by disrupting this capture. These findings offer new insight into the dynamics of stream segregation, and have implications for the neural basis of streaming and the role of attention in stream formation. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {6},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-17},
  date = {2013-12},
  pages = {1652-1666},
  keywords = {Pitch (Frequency),Auditory Stimulation,auditory grouping,build-up,deviant tone,resetting,stream segregation},
  author = {Haywood, Nicholas R. and Roberts, Brian},
  file = {D\:\\Sauve\\Zotero\\storage\\R37E96KM\\Haywood and Roberts - 2013 - Build-up of auditory stream segregation induced by.pdf}
}

@incollection{munteSpecializationSpecializedElectrophysiological2003,
  location = {{New York, NY, US}},
  title = {Specialization of the Specialized: {{Electrophysiological}} Investigations in Professional Musicians},
  isbn = {1-57331-452-8},
  shorttitle = {Specialization of the Specialized},
  abstract = {Several event-related brain potential (ERP) studies examining the processing of auditory stimuli by professional musicians compared with nonmusicians are reviewed. In the first study, musicians (string players) and nonmusicians attended to one of two streams of auditory stimuli characterized by a specific pitch. Musicians showed a prolonged ERP attention effect, the late portion of which was more frontally distributed than was that of the nonmusicians. In the second study, we investigated auditory spatial processing in conductors, pianists, and nonmusicians. Only the conductors showed behavioral selectivity of sound sources located in the peripheral auditory space. In addition, this group showed a negative/positive mismatch response for deviant stimuli occurring outside the focus of spatial attention. Finally, a group of drummers was compared to woodwind players and nonmusicians in a passive listening task. A real continuous drum sequence was manipulated so that some beats were anticipated by 80 ms. The drummers showed a mismatch response not only for the anticipated beats but also for the subsequent beats, suggesting a more complex representation of the temporal aspects stimulus sequence in this subject group. Together, these studies suggest qualitative differences of the neural correlates of auditory processing between musicians and nonmusicians. Moreover, these differences appear to be shaped by the specific training of a musician. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (chapter)},
  booktitle = {The Neurosciences and Music.},
  series = {Annals of the {{New York Academy}} of {{Sciences}}, {{Vol}}. 999},
  publisher = {{New York Academy of Sciences}},
  date = {2003},
  pages = {131-139},
  keywords = {Attention,Musicians,Auditory Perception,Auditory Stimulation,Auditory Evoked Potentials,nonmusicians,professional musicians,spatial attention},
  author = {Münte, Thomas F. and Nager, Wido and Beiss, Tilla and Schroeder, Christine and Altenmüller, Eckart},
  editor = {Avanzini, Giuliano and Faienza, Carmine and Minciacchi, Diego and Lopez, Luisa and Majno, Maria and Avanzini, Giuliano (Ed) and Faienza, Carmine (Ed) and Minciacchi, Diego (Ed) and Lopez, Luisa (Ed) and Majno, Maria (Ed)}
}

@article{munteSuperiorAuditorySpatial2001,
  title = {Superior Auditory Spatial Tuning in Conductors},
  volume = {409},
  issn = {0028-0836},
  doi = {10.1038/35054668},
  abstract = {Provides evidence from brain-potential recordings that experienced professional conductors develop enhanced auditory localization mechanisms in peripheral space. Seven classical-music conductors, 7 pianists, and 7 non-musicians participated in the study. Ss listened to brief pink-noise bursts delivered by central and peripheral arrays of 3 loudspeakers each. Although a spatial gradient was evident in all 3 groups for central auditory space, only the conductors displayed a gradient for the periphery. The very similar scalp topography of the attention effect for the different groups indicates that conductors probably do not engage different neural populations to perform the task. The authors believe that although conductors probably employ other mechanisms such as perceptual grouping to identify single musicians, the findings of this study provide another example of how extensive training can shape cognitive processes and their neural underpinnings. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {6820},
  journaltitle = {Nature},
  shortjournal = {Nature},
  date = {2001-02},
  pages = {580-580},
  keywords = {brain,Musicians,Auditory Localization,music perception,conductors vs pianists vs non-musicians,enhanced auditory localization mechanisms in peripheral space,spatial gradient for central vs peripheral auditory space shown by brain-potential recordings,Spatial Perception},
  author = {Münte, Thomas F. and Kohlmetz, Christine and Nager, Wido and Altenmüller, Eckart}
}

@article{nagerFateSoundsConductors2003,
  title = {The Fate of Sounds in Conductors' Brains: An {{ERP}} Study},
  volume = {17},
  issn = {0926-6410},
  doi = {10.1016/S0926-6410(03)00083-1},
  shorttitle = {The Fate of Sounds in Conductors' Brains},
  abstract = {Professional music conductors are required to home in on a particular musician but at the same time have to monitor the entire orchestra. It was hypothesized that this unique experience should be reflected by superior auditory spatial processing. Event-related brain potentials were obtained, while conductors, professional pianists, and non-musicians listened to sequences of bandpass-filtered noise-bursts presented in random order from 6 speakers, 3 located in front and 3 to the right of the subjects. In different runs, Ss either attended the centermost or the most peripheral speaker in order to detect slightly deviant noise-bursts. For centrally located speakers, the ERPs showed a typical negative displacement (Nd) attention effect for the relevant location with a steep decline for the neighboring speakers in all S groups. For peripheral speakers, only the conductors showed attentional selectivity, while the Nd effect was of similar size for all 3 peripheral speakers in the other 2 groups. These ERP effects were paralleled by an enhanced behavioral selectivity in peripheral auditory space in conductors. Moreover, the pre-attentive monitoring of the entire auditory scene indexed by the mismatch negativity was superior in musicians compared to non-musicians. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Cognitive Brain Research},
  shortjournal = {Cognitive Brain Research},
  date = {2003-06},
  pages = {83-93},
  keywords = {Musicians,Auditory Discrimination,Auditory Stimulation,Auditory Localization,music perception,Auditory Evoked Potentials,event-related brain potentials,Spatial Perception,attentional selectivity,auditory spatial processing,conductors,negative displacement attention effect,peripheral auditory,pianists},
  author = {Nager, Wido and Kohlmetz, Christine and Altenmuller, Eckart and Rodriguez-Fornells, Antoni and Münte, Thomas F.}
}

@article{weintraubEffectsAttentionAwareness2014,
  title = {Effects of Attention to and Awareness of Preceding Context Tones on Auditory Streaming},
  volume = {40},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2013-38554-001&site=ehost-live},
  doi = {10.1037/a0034720},
  abstract = {This study determined whether facilitation of auditory stream segregation could occur when facilitating context tones are accompanied by other sounds. Facilitation was measured as the likelihood of a repeated context tone that could match the low (A) or high (B) frequency of a repeating ABA test to increase the likelihood of hearing the test as segregated. We observed this type of facilitation when matching tones were alone, or with simultaneous bandpass noises or continuous speech, neither of which masked the tones. However, participants showed no streaming facilitation when a harmonic complex masked the context tones. Mistuning or desynchronizing the context tone relative to the rest of the complex did not facilitate streaming, despite the fact that the context tone was accessible to awareness and attention. Even presenting the context tone in a separate ear from the rest of the harmonic complex did not facilitate streaming, ruling out peripheral interference. Presenting the test as mistuned or desynchronized tones relative to complex tones eliminated the possibility that timbre changes from context to test interfered with facilitation resulting from the context. These results demonstrate the fragility of streaming facilitation and show that awareness of and attention to the context tones are not sufficient to overcome interference. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-17},
  date = {2014-04},
  pages = {685-701},
  keywords = {Attention,Auditory Perception,Auditory Stimulation,Stimulus Frequency,auditory scene analysis,awareness,facilitation,frequency dependence},
  author = {Weintraub, David M. and Metzger, Brian A. and Snyder, Joel S.},
  file = {D\:\\Sauve\\Zotero\\storage\\QHDA47W4\\Weintraub et al. - 2014 - Effects of attention to and awareness of preceding.pdf}
}

@article{carlRolePatternRegularity2013,
  title = {Role of Pattern, Regularity, and Silent Intervals in Auditory Stream Segregation Based on Inter-Aural Time Differences},
  volume = {224},
  issn = {0014-4819},
  doi = {10.1007/s00221-012-3333-z},
  abstract = {Tone triplets separated by a pause (ABA\_) are a popular tone-repetition pattern to study auditory stream segregation. Such triplets produce a galloping rhythm when integrated, but isochronous rhythms when segregated. Other patterns lacking a pause may produce less-prominent rhythmic differences but stronger streaming. Here, we evaluated whether this difference is readily explained by the presence of the pause and potentially associated with the reduction of adaptation, or whether there is contribution of tone pattern per se. Sequences with repetitive ABA\_ and ABAA patterns were presented in magnetoencephalography. A and B tones were separated by differences in inter-aural time differences (ΔITD). Results showed that the stronger streaming of ABAA was associated with a more prominent release from the adaptation of the P₁m in auditory cortex. We further compared behavioral streaming responses for patterns with and without pauses, and varied the position of the pause and pattern regularity. Results showed a major effect of the pauses’ presence, but no prominent effects of tone pattern or pattern regularity. These results make a case for the existence of an early, primitive streaming mechanism that does not require an analysis of the tone pattern at later stages suggested by predictive-coding models of auditory streaming. The results are better explained by the simpler population-separation model and stress the previously observed role of neural adaptation for streaming perception. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Experimental Brain Research},
  date = {2013-02},
  pages = {557-570},
  keywords = {auditory stream segregation,Pitch (Frequency),Auditory Stimulation,inter-aural time differences,isochronous rhythms,Rhythm,silent intervals,time perception,tone repetition pattern,tone repetition regularity},
  author = {Carl, David and Gutschalk, Alexander}
}

@article{francoisFasterSoundStream2014,
  title = {Faster {{Sound Stream Segmentation}} in {{Musicians}} than in {{Nonmusicians}}},
  volume = {9},
  url = {http://dx.doi.org/10.1371/journal.pone.0101340},
  doi = {10.1371/journal.pone.0101340},
  abstract = {The musician's brain is considered as a good model of brain plasticity as musical training is known to modify auditory perception and related cortical organization. Here, we show that music-related modifications can also extend beyond motor and auditory processing and generalize (transfer) to speech processing. Previous studies have shown that adults and newborns can segment a continuous stream of linguistic and non-linguistic stimuli based only on probabilities of occurrence between adjacent syllables, tones or timbres. The paradigm classically used in these studies consists of a passive exposure phase followed by a testing phase. By using both behavioural and electrophysiological measures, we recently showed that adult musicians and musically trained children outperform nonmusicians in the test following brief exposure to an artificial sung language. However, the behavioural test does not allow for studying the learning process per se but rather the result of the learning. In the present study, we analyze the electrophysiological learning curves that are the ongoing brain dynamics recorded as the learning is taking place. While musicians show an inverted U shaped learning curve, nonmusicians show a linear learning curve. Analyses of Event-Related Potentials (ERPs) allow for a greater understanding of how and when musical training can improve speech segmentation. These results bring evidence of enhanced neural sensitivity to statistical regularities in musicians and support the hypothesis of positive transfer of training effect from music to sound stream segmentation in general.},
  number = {7},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2015-06-17},
  date = {2014-07-11},
  pages = {e101340},
  author = {François, Clément and Jaillet, Florent and Takerkart, Sylvain and Schön, Daniele},
  file = {D\:\\Sauve\\Zotero\\storage\\F8Q2N2CT\\François et al. - 2014 - Faster Sound Stream Segmentation in Musicians than.pdf}
}

@article{mullensiefenMusicalityNonMusiciansIndex2014,
  title = {The {{Musicality}} of {{Non}}-{{Musicians}}: {{An Index}} for {{Assessing Musical Sophistication}} in the {{General Population}}},
  volume = {9},
  url = {http://dx.doi.org/10.1371/journal.pone.0089642},
  doi = {10.1371/journal.pone.0089642},
  shorttitle = {The {{Musicality}} of {{Non}}-{{Musicians}}},
  abstract = {Musical skills and expertise vary greatly in Western societies. Individuals can differ in their repertoire of musical behaviours as well as in the level of skill they display for any single musical behaviour. The types of musical behaviours we refer to here are broad, ranging from performance on an instrument and listening expertise, to the ability to employ music in functional settings or to communicate about music. In this paper, we first describe the concept of ‘musical sophistication’ which can be used to describe the multi-faceted nature of musical expertise. Next, we develop a novel measurement instrument, the Goldsmiths Musical Sophistication Index (Gold-MSI) to assess self-reported musical skills and behaviours on multiple dimensions in the general population using a large Internet sample (n{$\mkern1mu$}={$\mkern1mu$}147,636). Thirdly, we report results from several lab studies, demonstrating that the Gold-MSI possesses good psychometric properties, and that self-reported musical sophistication is associated with performance on two listening tasks. Finally, we identify occupation, occupational status, age, gender, and wealth as the main socio-demographic factors associated with musical sophistication. Results are discussed in terms of theoretical accounts of implicit and statistical music learning and with regard to social conditions of sophisticated musical engagement.},
  number = {2},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2015-06-17},
  date = {2014-02-26},
  pages = {e89642},
  author = {Müllensiefen, Daniel and Gingras, Bruno and Musil, Jason and Stewart, Lauren},
  file = {D\:\\Sauve\\Zotero\\storage\\6KSNFZJB\\Müllensiefen et al. - 2014 - The Musicality of Non-Musicians An Index for Asse.pdf}
}

@article{marieHighvoiceSuperiorityEffect2012,
  title = {The High-Voice Superiority Effect in Polyphonic Music Is Influenced by Experience: {{A}} Comparison of Musicians Who Play Soprano-Range Compared with Bass-Range Instruments},
  volume = {22},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2013-01453-003&site=ehost-live},
  doi = {10.1037/a0030858},
  shorttitle = {The High-Voice Superiority Effect in Polyphonic Music Is Influenced by Experience},
  abstract = {Western polyphonic music is typically composed of multiple simultaneous melodic lines of equal importance, referred to as “voices.” Previous studies have shown that adult nonmusicians are able to encode each voice in separate parallel sensory memory traces during passive listening. Specifically, when presented with sequences composed of two simultaneous voices (melodies), listeners show mismatch negativity (MMN) responses to pitch changes in each voice, although only 50\% of trials are unchanged. Interestingly, MMN is larger for the change in the higher compared to lower voice in both musicians and nonmusicians. This high-voice superiority effect has also been found in nonmusician adults and 7-month-old infants presented with two simultaneous tones, suggesting that a more robust memory trace for the higher-pitched voice might be an innate or early-acquired characteristic of human auditory processing. The present study tested whether musicians with experience playing a bass-range instrument (e.g., cello, double bass) would show a similar high-voice superiority effect as musicians with experience playing a soprano-range instrument (e.g., violin, flute). We found that musicians playing soprano-range instruments showed a high-voice superiority effect in line with previous studies, but musicians playing bass-range instruments showed similar MMN responses for both voices. These results suggest that with years of experience playing a lower-voiced instrument, cortical encoding of the lower of two simultaneous voices can be enhanced to some extent despite the early developing bias for better encoding of the higher voice. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  shortjournal = {Psychomusicology: Music, Mind, and Brain},
  series = {Neurosciences and {{Music}}},
  urldate = {2015-06-17},
  date = {2012-12},
  pages = {97-104},
  keywords = {Musicians,Music,auditory scene analysis (ASA),bass-range instruments,brain plasticity,high-voice superiority effect,mismatch negativity,Mismatch negativity (MMN),musical expertise,Musical Instruments,polyphonic music,soprano-range,Voice},
  author = {Marie, Céline and Fujioka, Takako and Herrington, Leland and Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\F6TEB6KH\\Marie et al. - 2012 - The high-voice superiority effect in polyphonic mu.pdf}
}

@article{fujiokaAutomaticEncodingPolyphonic2005,
  title = {Automatic Encoding of Polyphonic Melodies in Musicians and Nonmusicians},
  volume = {17},
  issn = {0898-929X},
  doi = {10.1162/089892905774597263},
  abstract = {In music, multiple musical objects often overlap in time. Western polyphonic music contains multiple simultaneous melodic lines (referred to as "voices") of equal importance. Previous electrophysiological studies have shown that pitch changes in a single melody are automatically encoded in memory traces, as indexed by mismatch negativity (MMN) and its magnetic counterpart (MMNm), and that this encoding process is enhanced by musical experience. In the present study, we examined whether two simultaneous melodies in polyphonic music are represented as separate entities in the auditory memory trace. Musicians and untrained controls were tested in both magnetoencephalogram and behavioral sessions. Polyphonic stimuli were created by combining two melodies (A and B), each consisting of the same five notes but in a different order. Melody A was in the high voice and Melody B in the low voice in one condition, and this was reversed in the other condition. On 50\% of trials, a deviant final (5th) note was played either in the high or in the low voice, and it either went outside the key of the melody or remained within the key. These four deviations occurred with equal probability of 12.5\% each. Clear MMNm was obtained for most changes in both groups, despite the 50\% deviance level, with a larger amplitude in musicians than in controls. The response pattern was consistent across groups, with larger MMNm for deviants in the high voice than in the low voice, and larger MMNm for in-key than out-of-key changes, despite better behavioral performance for out-of-key changes. The results suggest that melodic information in each voice in polyphonic music is encoded in the sensory memory trace, that the higher voice is more salient than the lower, and that tonality may be processed primarily at cognitive stages subsequent to MMN generation. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {10},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2005-10},
  pages = {1578-1592},
  keywords = {Musicians,Music,mismatch negativity,auditory memory trace,automatic encoding,encoding process,Human Information Storage,Memory Trace,polyphonic melodies},
  author = {Fujioka, Takako and Trainor, Laurel J. and Ross, Bernhard and Kakigi, Ryusuke and Pantev, Christo}
}

@article{cirelliBeatinducedFluctuationsAuditory2014,
  title = {Beat-Induced Fluctuations in Auditory Cortical Beta-Band Activity: {{Using EEG}} to Measure Age-Related Changes},
  volume = {5},
  issn = {1664-1078},
  shorttitle = {Beat-Induced Fluctuations in Auditory Cortical Beta-Band Activity},
  abstract = {People readily extract regularity in rhythmic auditory patterns, enabling prediction of the onset of the next beat. Recent magnetoencephalography (MEG) research suggests that such prediction is reflected by the entrainment of oscillatory networks in the brain to the tempo of the sequence. In particular, induced beta-band oscillatory activity from auditory cortex decreases after each beat onset and rebounds prior to the onset of the next beat across tempi in a predictive manner. The objective of the present study was to examine the development of such oscillatory activity by comparing electroencephalography (EEG) measures of beta-band fluctuations in 7-year-old children to adults. EEG was recorded while participants listened passively to isochronous tone sequences at three tempi (390, 585, and 780 ms for onset-to-onset interval). In adults, induced power in the high beta-band (20– 25 Hz) decreased after each tone onset and rebounded prior to the onset of the next tone across tempo conditions, consistent with MEG findings. In children, a similar pattern was measured in the two slower tempo conditions, but was weaker in the fastest condition. The results indicate that the beta-band timing network works similarly in children, although there are age-related changes in consistency and the tempo range over which it operates. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Frontiers in Psychology},
  date = {2014-07-11},
  keywords = {time perception,Age Differences,Auditory Cortex,Beta Rhythm,child development,Childhood Development,electroencephalography,electroencephalography (EEG),musical rhythm,neural oscillation,Oscillatory Network},
  author = {Cirelli, Laura K. and Bosnyak, Dan and Manning, Fiona C. and Spinelli, Christina and Marie, Céline and Fujioka, Takako and Ghahremani, Ayda and Trainor, Laurel J.}
}

@article{fujiokaOneYearMusical2006,
  title = {One Year of Musical Training Affects Development of Auditory Cortical-Evoked Fields in Young Children},
  volume = {129},
  issn = {0006-8950},
  doi = {10.1093/brain/awl247},
  abstract = {Auditory evoked responses to a violin tone and a noise-burst stimulus were recorded from 4- to 6-year-old children in four repeated measurements over a 1-year period using magnetoencephalography (MEG). Half of the subjects participated in musical lessons throughout the year; the other half had no music lessons. Auditory evoked magnetic fields showed prominent bilateral P100m, N250m, P320m and N450m peaks. Significant change in the peak latencies of all components except P100m was observed over time. Larger P100m and N450m amplitude as well as more rapid change of N250m amplitude and latency was associated with the violin rather than the noise stimuli. Larger P100m and P320m peak amplitudes in the left hemisphere than in the right are consistent with left-lateralized cortical development in this age group. A clear musical training effect was expressed in a larger and earlier N250m peak in the left hemisphere in response to the violin sound in musically trained children compared with untrained children. This difference coincided with pronounced morphological change in a time window between 100 and 400 ms, which was observed in musically trained children in response to violin stimuli only, whereas in untrained children a similar change was present regardless of stimulus type. This transition could be related to establishing a neural network associated with sound categorization and/or involuntary attention, which can be altered by music learning experience. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {10},
  journaltitle = {Brain: A Journal of Neurology},
  shortjournal = {Brain: A Journal of Neurology},
  date = {2006-10},
  pages = {2593-2608},
  keywords = {musical training,Auditory Evoked Potentials,Auditory Cortex,auditory evoked responses,cortical plasticity,magnetoencephalography,maturation,Music Education,Neural Development,Neural Plasticity},
  author = {Fujioka, Takako and Ross, Bernhard and Kakigi, Ryusuke and Pantev, Christo and Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\8TM4HIQK\\293065.html;D\:\\Sauve\\Zotero\\storage\\VCJEMNAC\\293065.html}
}

@article{fujiokaSimultaneousPitchesAre2008,
  title = {Simultaneous Pitches Are Encoded Separately in Auditory Cortex: {{An MMNm}} Study},
  volume = {19},
  issn = {0959-4965},
  doi = {10.1097/WNR.0b013e3282f51d91},
  shorttitle = {Simultaneous Pitches Are Encoded Separately in Auditory Cortex},
  abstract = {This study examined whether two simultaneous pitches have separate memory representations or an integrated representation in preattentive auditory memory. Mismatch negativity fields were examined when a pitch change occurred in either the higher-pitched or the lower-pitched tone at 25\% probability each, thus making the total deviation rate of the two-tone dyad 50\%. Clear MMNm was obtained for deviants in both tones confirming separate memory traces for concurrent tones. At the same time, deviants to the lower-pitched, but not higher-pitched, tone within the two-tone dyad elicited a reduced MMNm compared to when each tone was presented alone, indicating that the representations of two pitches are not completely independent. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {NeuroReport: For Rapid Communication of Neuroscience Research},
  shortjournal = {NeuroReport: For Rapid Communication of Neuroscience Research},
  date = {2008-02},
  pages = {361-366},
  keywords = {Auditory Perception,Pitch (Frequency),Auditory Stimulation,Auditory Evoked Potentials,mismatch negativity,Auditory Cortex,auditory memory,Memory,memory representations,simultaneous pitches,tones},
  author = {Fujioka, Takako and Trainor, Laurel J. and Ross, Bernhard}
}

@article{vuustNewFastMismatch2011,
  title = {New Fast Mismatch Negativity Paradigm for Determining the Neural Prerequisites for Musical Ability},
  volume = {47},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2011.04.026},
  abstract = {Studies have consistently shown that the mismatch negativity (MMN) for different auditory features correlates with musical skills, and that this effect is more pronounced for stimuli integrated in complex musical contexts. Hence, the MMN can potentially be used for determining the development of auditory skills and musical expertise. MMN paradigms, however, are typically very long in duration, and far from sounding musical. Therefore, we developed a novel multi-feature MMN paradigm with 6 different deviant types integrated in a complex musical context of no more than 20 min in duration. We found significant MMNs for all 6 deviant types. Hence, this short objective measure can putatively be used as an index for auditory and musical development. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {9},
  journaltitle = {Cortex: A Journal Devoted to the Study of the Nervous System and Behavior},
  shortjournal = {Cortex: A Journal Devoted to the Study of the Nervous System and Behavior},
  date = {2011-10},
  pages = {1091-1098},
  keywords = {music perception,mismatch negativity,Neural Development,mismatch negativity paradigm,Musical Ability,musical development,neural prerequisites},
  author = {Vuust, Peter and Brattico, Elvira and Glerean, Enrico and Seppänen, Miia and Pakarinen, Satu and Tervaniemi, Mari and Näätänen, Risto}
}

@article{hansenWorkingMemoryMusical2013,
  title = {Working Memory and Musical Competence of Musicians and Non-Musicians},
  volume = {41},
  issn = {0305-7356},
  doi = {10.1177/0305735612452186},
  abstract = {Musical ability has been found to be associated with an enhancement of verbal working memory. In this study, we investigated whether this effect would generalize to visual-spatial working memory as would be expected if the effect were driven by general intelligence. We administered the WAIS-III Digit Span; the WMS-III Spatial Span; and the Musical Ear Test (MET), a forced-choice same/different listening task measuring musical ability, to non-musicians, amateur musicians, and expert musicians. Expert musicians significantly outperformed non-musicians on the Digit Span. Additionally, Digit Span Forward scores were found to be correlated with MET total scores and with scores on the rhythm subtest of the MET. No between-group differences were found on the Spatial Span. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {6},
  journaltitle = {Psychology of Music},
  shortjournal = {Psychology of Music},
  date = {2013-11},
  pages = {779-793},
  keywords = {Musicians,Auditory Perception,Musical Instruments,Musical Ability,Competence,general intelligence,Intelligence,listening,musical competence,Short Term Memory,visual-spatial working memory,Visuospatial Ability},
  author = {Hansen, Mads and Wallentin, Mikkel and Vuust, Peter}
}

@article{wallentinMusicalEarTest2010,
  title = {The {{Musical Ear Test}}, a New Reliable Test for Measuring Musical Competence},
  volume = {20},
  issn = {1041-6080},
  doi = {10.1016/j.lindif.2010.02.004},
  abstract = {[Correction Notice: An erratum for this article was reported in Vol 20(6) of Learning and Individual Differences (see record [rid]2010-26710-007[/rid]). In the original article, the reported measures of Cronbach alpha internal consistency were wrong. The correct values are given in the erratum.] This paper reports results from three experiments using the Musical Ear Test (MET), a new test designed for measuring musical abilities in both musicians and non-musicians in an objective way with a relatively short duration ({$<$}20 min.). In the first experiment we show how the MET is capable of clearly distinguishing between a group of professional musicians and a group of non-musicians. In the second experiment we demonstrate that results from the MET are strongly correlated with measures of musical expertise obtained using an imitation test. In the third experiment we show that the MET also clearly distinguishes groups of non-musicians, amateurs and professional musicians. The test is found to have a large internal consistency (Cronbach alpha: 0.87). We further show a correlation with amount of practice within the group of professionals as well as a correlation with a forward digit span test. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {Learning and Individual Differences},
  shortjournal = {Learning and Individual Differences},
  date = {2010-06},
  pages = {188-196},
  keywords = {Musicians,Musical Ability,Measurement,musical competence measurement,Musical Ear Test,psychometrics,test reliability,test validity},
  author = {Wallentin, Mikkel and Nielsen, Andreas Højlund and Friis-Olivarius, Morten and Vuust, Christian and Vuust, Peter}
}

@article{mackenDoesAuditoryStreaming2003,
  title = {Does Auditory Streaming Require Attention? {{Evidence}} from Attentional Selectivity in Short-Term Memory},
  volume = {29},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2003-04481-003&site=ehost-live},
  doi = {10.1037/0096-1523.29.1.43},
  shorttitle = {Does Auditory Streaming Require Attention?},
  abstract = {R. P. Carlyon, R. Cusack, J. M. Foxton, and I. H. Robertson (2001; see record [rid]2001-16068-008[/rid]) have argued that attention is crucial for auditory streaming. The authors review R. P. Carlyon et al.'s (2001) arguments and suggest that a pertinent literature, the irrelevant sound paradigm--demonstrating preattentive auditory streaming--has been overlooked. In illustration of this alternative approach, the authors include a novel single experiment demonstrating the impact of preattentive auditory streaming on short-term serial memory. It is concluded that R. P. Carlyon et al.'s (2001) results do not definitively demonstrate that auditory streaming processes are dependent on attention; indeed, they are compatible with alternative accounts of the relationship between perceptual organization and attention. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-17},
  date = {2003-02},
  pages = {43-51},
  keywords = {Attention,auditory stream segregation,Auditory Discrimination,auditory streaming,Stimulus Parameters,Short Term Memory,attention selectivity,disruption,irrelevant sound paradigm,rate of presentation,selective attention},
  author = {Macken, William J. and Tremblay, Sébastien and Houghton, Robert J. and Nicholls, Alastair P. and Jones, Dylan M.},
  file = {D\:\\Sauve\\Zotero\\storage\\4TI97WMC\\Macken et al. - 2003 - Does auditory streaming require attention Evidenc.pdf}
}

@article{mackenAuditoryDistractionPerceptual2014,
  title = {Auditory Distraction and Perceptual Organization: {{Streams}} of Unconscious Processing},
  volume = {3},
  issn = {2046-0252},
  doi = {10.1002/pchj.46},
  shorttitle = {Auditory Distraction and Perceptual Organization},
  abstract = {Abstract Perceptual organization is key to understanding auditory distraction. In order to achieve a fundamental understanding of distraction it is necessary to understand how auditory stimuli are perceived; specifically, how they are organized into entities that do not map directly onto simple single stimuli as defined by the experimenter. It is important not to mistake some arbitrary unit of analysis, such as the word, as the correct unit for understanding auditory processing; rather, the unit of the auditory object and its relative position to other auditory objects is the key to understanding distraction (as well as the whole of auditory cognition more generally). Here I provide two illustrative examples of auditory perceptual organization showing the superlative power of organizational principles: streaming by similarity and stimulus capture. I go on to show how these have been used to refine our understanding of distraction, and of the effects of distraction from sequences of sound, from single sounds, or single changes within a sequence. A common feature of work described here is that it compares the effects of different forms of organization: The nominal stimuli themselves are largely unchanged but the way they relate to each other can change distraction appreciably. That is, it is not the mere presence of sound that causes distraction but its organization and the way that relates to the currently prevailing activity. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {PsyCh Journal},
  shortjournal = {PsyCh Journal},
  date = {2014-03},
  pages = {4-16},
  keywords = {Attention,Auditory Perception,Auditory Stimulation,auditory distraction,Cognitive Processes,Distraction,perceptual grouping,Perceptual Style,unconscious processing},
  author = {Macken, Bill}
}

@article{sussmanInvestigationAuditoryStreaming1999,
  title = {An Investigation of the Auditory Streaming Effect Using Event-Related Brain Potentials},
  volume = {36},
  issn = {0048-5772},
  doi = {10.1017/S0048577299971056},
  abstract = {There is uncertainty concerning the extent to which the auditory streaming effect is a function of attentive or preattentive mechanisms. The mismatch negativity (MMN), which indexes preattentive acoustic processing, was used to probe whether the segregation associated with the streaming effect occurs preattentively. In Experiment 1, with 10 subjects (aged 23–42 yrs), alternating high and low tones were presented at fast and slow paces while subjects ignored the stimuli. At the slow pace, tones were heard as alternating high and low pitches, and no MMN was elicited. At the fast pace a streaming effect was induced and an MMN was observed for the low stream, indicating a preattentive locus for the streaming effect. The high deviant did not elicit an MMN. MMNs were obtained to both the high and low deviants when the interval between the across-stream deviance was lengthened to more than 250 ms in Experiment 2 (with 11 subjects aged 21–45 yrs), indicating that the MMN system is susceptible to processing constraints. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Psychophysiology},
  shortjournal = {Psychophysiology},
  date = {1999-01},
  pages = {22-34},
  keywords = {Evoked Potentials,Auditory Discrimination,Auditory Stimulation,Stimulus Parameters,acoustics,21–45 yr olds,mechanisms of auditory streaming effect & auditory stream segregation using event related potentials},
  author = {Sussman, Elyse and Ritter, Walter and Vaughan, Herbert G. Jr.}
}

@article{sussmanPredictabilityStimulusDeviance1998,
  title = {Predictability of Stimulus Deviance and the Mismatch Negativity},
  volume = {9},
  issn = {0959-4965},
  doi = {10.1097/00001756-199812210-00031},
  abstract = {The purpose of this study was to test the previous report that generation of the mismatch negativity (MMN) component of event-related brain potentials (ERPs) is indifferent to the predictable occurrence of stimulus deviance. A total of 14 Ss (aged 26–47 yrs) participated in the experiment. A pattern of standards (S) and deviants (D) were delivered in a predictable fashion (SSSSD) at two different speeds (1.3 sec and 100 msec). An MMN was obtained to the D position tone at the slow but not the fast pace. These results demonstrate that, unlike the P3 component, the MMN is sensitive to the predictable occurrence of stimulus deviance when the predictability can be detected by the brain within the estimated limits of sensory memory. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {18},
  journaltitle = {Neuroreport: An International Journal for the Rapid Communication of Research in Neuroscience},
  shortjournal = {Neuroreport: An International Journal for the Rapid Communication of Research in Neuroscience},
  date = {1998-12},
  pages = {4167-4170},
  keywords = {Auditory Stimulation,Auditory Evoked Potentials,mismatch negativity,26–47 yr olds,Predictability (Measurement),predictability of auditory stimulus deviance,Stimulus Discrimination},
  author = {Sussman, Elyse and Ritter, Walter and Vaughan, Herbert G. Jr.}
}

@article{jonesOrganizationalFactorsSelective1999,
  title = {Organizational Factors in Selective Attention: {{The}} Interplay of Acoustic Distinctiveness and Auditory Streaming in the Irrelevant Sound Effect},
  volume = {25},
  issn = {0278-7393},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1999-00904-010&site=ehost-live},
  doi = {10.1037/0278-7393.25.2.464},
  shorttitle = {Organizational Factors in Selective Attention},
  abstract = {A series of studies further explored the way in which irrelevant sound disrupts the serial recall of visually presented verbal sequences. The hypothesis that distinctiveness (stimulus mismatch) within auditory irrelevant sequences is a critical determinant of disruption of serial recall was tested. Experiment 1 showed that the degree of disruption was related to the degree of mismatch between successive stimuli. However, in Experiment 2, changes in 2 attributes of a stimulus produced less disruption than when only 1 was changed, suggesting mismatch alone was not the key factor. These results were reconciled with the changing-state hypothesis in Experiment 3 in which change and disruption were monotonically related up to the point at which mismatch created 2 streams. Object-based theories are able to explain this pattern of results. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  shortjournal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  urldate = {2015-06-17},
  date = {1999-03},
  pages = {464-473},
  keywords = {Auditory Stimulation,college students,selective attention,distinctiveness of auditory irrelevant sequences,Serial Recall,serial recall of visually presented verbal sequences,Stimulus Salience,Visual Stimulation},
  author = {Jones, Dylan and Alford, David and Bridges, Andrew and Tremblay, Sébastien and Macken, Bill},
  file = {D\:\\Sauve\\Zotero\\storage\\I9ZIJQGU\\Jones et al. - 1999 - Organizational factors in selective attention The.pdf}
}

@article{arnottFunctionalOrganizationAuditory2005,
  title = {The {{Functional Organization}} of {{Auditory Working Memory}} as {{Revealed}} by {{fMRI}}},
  volume = {17},
  issn = {0898-929X},
  doi = {10.1162/0898929053747612},
  abstract = {Spatial and nonspatial auditory tasks preferentially recruit dorsal and ventral brain areas, respectively. However, the extent to which these auditory differences reflect specific aspects of mental processing has not been directly studied. In the present functional magnetic resonance imaging experiment, participants encoded and maintained either the location or the identity of a sound for a delay period of several seconds and then subsequently compared that information with a second sound. Relative to sound localization, sound identification was associated with greater hemodynamic activity in the left rostral superior temporal gyrus. In contrast, localizing sounds recruited greater activity in the parietal cortex, posterior temporal lobe, and superior frontal sulcus. The identification differences were most prominent during the early stage of the trial, whereas the location differences were most evident during the late (i.e., comparison) stage. Accordingly, our results suggest that auditory spatial and identity dissociations as revealed by functional imaging may be dependent to some degree on the type of processing being carried out. In addition, dorsolateral prefrontal and lateral superior parietal areas showed greater activity during the comparison as opposed to the earlier stage of the trial, regardless of the type of auditory task, consistent with results from visual working memory studies. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {5},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2005-05},
  pages = {819-831},
  keywords = {Auditory Discrimination,Auditory Localization,Short Term Memory,auditory working memory,functional organization,Magnetic Resonance Imaging,sound identification,sound location},
  author = {Arnott, Stephen R. and Grady, Cheryl L. and Hevenor, Stephanie J. and Graham, Simon and Alain, Claude}
}

@article{chennuExpectationAttentionHierarchical2013,
  title = {Expectation and Attention in Hierarchical Auditory Prediction},
  volume = {33},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.0114-13.2013},
  abstract = {Hierarchical predictive coding suggests that attention in humans emerges from increased precision in probabilistic inference, whereas expectation biases attention in favor of contextually anticipated stimuli. We test these notions within auditory perception by independently manipulating top-down expectation and attentional precision alongside bottom-up stimulus predictability. Our findings support an integrative interpretation of commonly observed electrophysiological signatures of neurodynamics, namely mismatch negativity (MMN), P300, and contingent negative variation (CNV), as manifestations along successive levels of predictive complexity. Early first-level processing indexed by the MMN was sensitive to stimulus predictability: here, attentional precision enhanced early responses, but explicit top-down expectation diminished it. This pattern was in contrast to later, second-level processing indexed by the P300: although sensitive to the degree of predictability, responses at this level were contingent on attentional engagement and in fact sharpened by top-down expectation. At the highest level, the drift of the CNV was a fine-grained marker of top-down expectation itself. Source reconstruction of high-density EEG, supported by intracranial recordings, implicated temporal and frontal regions differentially active at early and late levels. The cortical generators of the CNV suggested that it might be involved in facilitating the consolidation of context-salient stimuli into conscious perception. These results provide convergent empirical support to promising recent accounts of attention and expectation in predictive coding. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {27},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {The Journal of Neuroscience},
  date = {2013-07-03},
  pages = {11194-11205},
  keywords = {Auditory Perception,mismatch negativity,electroencephalography,Contingent Negative Variation,EEG,hierarchical auditory prediction,P300},
  author = {Chennu, Srivas and Noreika, Valdas and Gueorguiev, David and Blenkmann, Alejandro and Kochen, Silvia and Ibáñez, Agustín and Owen, Adrian M. and Bekinschtein, Tristan A.}
}

@article{snyderNeurophysiologicalTheoryAuditory2007,
  title = {Toward a Neurophysiological Theory of Auditory Stream Segregation},
  volume = {133},
  issn = {0033-2909},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2007-12463-004&site=ehost-live},
  doi = {10.1037/0033-2909.133.5.780},
  abstract = {Auditory stream segregation (or streaming) is a phenomenon in which 2 or more repeating sounds differing in at least 1 acoustic attribute are perceived as 2 or more separate sound sources (i.e., streams). This article selectively reviews psychophysical and computational studies of streaming and comprehensively reviews more recent neurophysiological studies that have provided important insights into the mechanisms of streaming. On the basis of these studies, segregation of sounds is likely to occur beginning in the auditory periphery and continuing at least to primary auditory cortex for simple cues such as pure-tone frequency but at stages as high as secondary auditory cortex for more complex cues such as periodicity pitch. Attention-dependent and perception-dependent processes are likely to take place in primary or secondary auditory cortex and may also involve higher level areas outside of auditory cortex. Topographic maps of acoustic attributes, stimulus-specific suppression, and competition between representations are among the neurophysiological mechanisms that likely contribute to streaming. A framework for future research is proposed. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {5},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  urldate = {2015-06-17},
  date = {2007-09},
  pages = {780-799},
  keywords = {Attention,Auditory Perception,Pitch (Frequency),Auditory Discrimination,acoustics,auditory scene analysis,Auditory Cortex,neural suppression,Neurophysiology,tonotopic organization},
  author = {Snyder, Joel S. and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\BG73D3FR\\Snyder and Alain - 2007 - Toward a neurophysiological theory of auditory str.pdf}
}

@article{snyderAdaptationRevealsMultiple2009,
  title = {Adaptation Reveals Multiple Levels of Representation in Auditory Stream Segregation},
  volume = {35},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2009-11357-017&site=ehost-live},
  doi = {10.1037/a0012741},
  abstract = {When presented with alternating low and high tones, listeners are more likely to perceive 2 separate streams of tones (“streaming”) than a single coherent stream when the frequency separation (Δƒ) between tones is greater and the number of tone presentations is greater (“buildup”). However, the same large-Δƒ sequence reduces streaming for subsequent patterns presented after a gap of up to several seconds. Buildup occurs at a level of neural representation with sharp frequency tuning. The authors used adaptation to demonstrate that the contextual effect of prior Δƒ arose from a representation with broad frequency tuning, unlike buildup. Separate adaptation did not occur in a representation of Δƒ independent of frequency range, suggesting that any frequency-shift detectors undergoing adaptation are also frequency specific. A separate effect of prior perception was observed, dissociating stimulus-related (i.e., Δƒ) and perception-related (i.e., 1 stream vs. 2 streams) adaptation. Viewing a visual analogue to auditory streaming had no effect on subsequent perception of streaming, suggesting adaptation in auditory-specific brain circuits. These results, along with previous findings on buildup, suggest that processing in at least 3 levels of auditory neural representation underlies segregation and formation of auditory streams. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-17},
  date = {2009-08},
  pages = {1232-1244},
  keywords = {Auditory Perception,auditory scene analysis,Adaptation,buildup,cross-modal,frequency shift detector},
  author = {Snyder, Joel S. and Carter, Olivia L. and Hannon, Erin E. and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\76XPWVQK\\Snyder et al. - 2009 - Adaptation reveals multiple levels of representati.pdf}
}

@article{snyderEffectsContextAuditory2008,
  title = {Effects of Context on Auditory Stream Segregation},
  volume = {34},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2008-09670-017&site=ehost-live},
  doi = {10.1037/0096-1523.34.4.1007},
  abstract = {The authors examined the effect of preceding context on auditory stream segregation. Low tones (A), high tones (B), and silences (-) were presented in an ABA- pattern. Participants indicated whether they perceived 1 or 2 streams of tones. The A tone frequency was fixed, and the B tone was the same as the A tone or had 1 of 3 higher frequencies. Perception of 2 streams in the current trial increased with greater frequency separation between the A and B tones (Δf). Larger Δf in previous trials modified this pattern, causing less streaming in the current trial. This occurred even when listeners were asked to bias their perception toward hearing 1 stream or 2 streams. The effect of previous Δf was not due to response bias because simply perceiving 2 streams in the previous trial did not cause less streaming in the current trial. Finally, the effect of previous Δf was diminished, though still present, when the silent duration between trials was increased to 5.76 s. The time course of this context effect on streaming implicates the involvement of auditory sensory memory or neural adaptation. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-17},
  date = {2008-08},
  pages = {1007-1016},
  keywords = {Auditory Perception,Auditory Discrimination,auditory scene analysis,Neural Plasticity,Memory,Adaptation,auditory sensory memory,neural adaptation},
  author = {Snyder, Joel S. and Carter, Olivia L. and Lee, Suh-Kyung and Hannon, Erin E. and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\AGSS3U2Z\\Snyder et al. - 2008 - Effects of context on auditory stream segregation.pdf}
}

@article{snyderEffectsPriorStimulus2009,
  title = {Effects of Prior Stimulus and Prior Perception on Neural Correlates of Auditory Stream Segregation},
  volume = {46},
  issn = {0048-5772},
  doi = {10.1111/j.1469-8986.2009.00870.x},
  abstract = {We examined whether effects of prior experience are mediated by distinct brain processes from those processing current stimulus features. We recorded event-related potentials (ERPs) during an auditory stream segregation task that presented an adaptation sequence with a small, intermediate, or large frequency separation between low and high tones (Δf), followed by a test sequence with intermediate Δf. Perception of two streams during the test was facilitated by small prior Δf and by prior perception of two streams and was accompanied by more positive ERPs. The scalp topography of these perception-related changes in ERPs was different from that observed for ERP modulations due to increasing the current Δf. These results reveal complex interactions between stimulus-driven activity and temporal-context-based processes and suggest a complex set of brain areas involved in modulating perception based on current and previous experience. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {6},
  journaltitle = {Psychophysiology},
  shortjournal = {Psychophysiology},
  date = {2009-11},
  pages = {1208-1215},
  keywords = {brain,Evoked Potentials,Auditory Perception,auditory stream segregation,brain processes,Early Experience,event related potentials,neural correlates,Neural Pathways,prior experience,prior perception},
  author = {Snyder, Joel S. and Holder, W. Trent and Weintraub, David M. and Carter, Olivia L. and Alain, Claude}
}

@article{snyderAttentionAwarenessPerception2012,
  title = {Attention, Awareness, and the Perception of Auditory Scenes},
  volume = {3},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00015},
  abstract = {Auditory perception and cognition entails both low-level and high-level processes, which are likely to interact with each other to create our rich conscious experience of soundscapes. Recent research that we review has revealed numerous influences of high-level factors, such as attention, intention, and prior experience, on conscious auditory perception. And recently, studies have shown that auditory scene analysis tasks can exhibit multistability in a manner very similar to ambiguous visual stimuli, presenting a unique opportunity to study neural correlates of auditory awareness and the extent to which mechanisms of perception are shared across sensory modalities. Research has also led to a growing number of techniques through which auditory perception can be manipulated and even completely suppressed. Such findings have important consequences for our understanding of the mechanisms of perception and also should allow scientists to precisely distinguish the influences of different higher-level influences. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Frontiers in Psychology},
  date = {2012-02-07},
  keywords = {Attention,cognition,Auditory Perception,awareness,Visual Stimulation,auditory scenes perception,Consciousness States,intention,visual stimuli},
  author = {Snyder, Joel S. and Gregg, Melissa K. and Weintraub, David M. and Alain, Claude}
}

@article{summerfieldExpectationAttentionVisual2009,
  title = {Expectation (and Attention) in Visual Cognition},
  volume = {13},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2009.06.003},
  abstract = {Visual cognition is limited by computational capacity, because the brain can process only a fraction of the visual sensorium in detail, and by the inherent ambiguity of the information entering the visual system. Two mechanisms mitigate these burdens: attention prioritizes stimulus processing on the basis of motivational relevance, and expectations constrain visual interpretation on the basis of prior likelihood. Of the two, attention has been extensively investigated while expectation has been relatively neglected. Here, we review recent work that has begun to delineate a neurobiology of visual expectation, and contrast the findings with those of the attention literature, to explore how these two central influences on visual perception overlap, differ and interact. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {9},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  date = {2009-09},
  pages = {403-409},
  keywords = {Attention,cognition,Expectations,neurobiology,Visual Attention,visual cognition,VISUAL perception},
  author = {Summerfield, Christopher and Egner, Tobias}
}

@article{denhamRolePredictiveModels2006,
  langid = {english},
  title = {The Role of Predictive Models in the Formation of Auditory Streams},
  volume = {100},
  issn = {0928-4257},
  doi = {10.1016/j.jphysparis.2006.09.012},
  abstract = {Sounds provide us with useful information about our environment which complements that provided by other senses, but also poses specific processing problems. How does the auditory system distentangle sounds from different sound sources? And what is it that allows intermittent sound events from the same source to be associated with each other? Here we review findings from a wide range of studies using the auditory streaming paradigm in order to formulate a unified account of the processes underlying auditory perceptual organization. We present new computational modelling results which replicate responses in primary auditory cortex [Fishman, Y.I., Arezzo, J.C., Steinschneider, M., 2004. Auditory stream segregation in monkey auditory cortex: effects of frequency separation, presentation rate, and tone duration. J. Acoust. Soc. Am. 116, 1656-1670; Fishman, Y. I., Reser, D. H., Arezzo, J.C., Steinschneider, M., 2001. Neural correlates of auditory stream segregation in primary auditory cortex of the awake monkey. Hear. Res. 151, 167-187] to tone sequences. We also present the results of a perceptual experiment which confirm the bi-stable nature of auditory streaming, and the proposal that the gradual build-up of streaming may be an artefact of averaging across many subjects [Pressnitzer, D., Hupé, J. M., 2006. Temporal dynamics of auditory and visual bi-stability reveal common principles of perceptual organization. Curr. Biol. 16(13), 1351-1357.]. Finally we argue that in order to account for all of the experimental findings, computational models of auditory stream segregation require four basic processing elements; segregation, predictive modelling, competition and adaptation, and that it is the formation of effective predictive models which allows the system to keep track of different sound sources in a complex auditory environment.},
  number = {1-3},
  journaltitle = {Journal of Physiology, Paris},
  shortjournal = {J. Physiol. Paris},
  year = {2006 Jul-Sep},
  pages = {154-170},
  keywords = {Attention,Auditory Perception,Auditory Cortex,Acoustic Stimulation,animals,Auditory Pathways,humans,Models; Biological},
  author = {Denham, S. L. and Winkler, I.},
  eprinttype = {pmid},
  eprint = {17084600}
}

@inproceedings{denhamCompetitionCooperationModel2029,
  title = {Competition and Cooperation in a Model of Auditory Scene Analysis},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pxh&AN=636952013-318&site=ehost-live},
  doi = {10.1037/e636952013-318},
  abstract = {Auditory scene analysis addresses the difficult task of interpreting the sound world in terms of an unknown number of discrete sound sources with possibly overlapping signals. The problem is underconstrained as there are many different ways in which incoming events can be associated. This problem has been studied using the auditory streaming paradigm, and it has become apparent that instead of making one fixed perceptual decision, auditory perception switches back and forth between alternative groupings; a phenomenon known as perceptual multi-stability. We propose a new model of auditory scene analysis at the core of which is a process that seeks to discover predictable patterns in the ongoing sound sequence. Representations of predictable fragments are created on the fly, and are maintained, strengthened or weakened on the basis of their predictive success and clashes with other representations. Auditory perceptual organisation emerges spontaneously from process. The model accounts for many important findings, including the emergence of, and switching between, alternative organisations and the influence of stimulus parameters. Its principal contribution is to show that a two-stage process of pattern discovery, and competition between incompatible patterns, can account for both the contents (perceptual organisations) and the dynamics of human perception in auditory streaming. (PsycEXTRA Database Record (c) 2014 APA, all rights reserved)},
  eventtitle = {{{ESCOP}} 2013: 18th {{Meeting}} of the {{European Society}} for {{Cognitive Psychology}}, {{Budapest}}, {{Hungary}}, {{August}} 29-{{September}} 1, 2013 [{{Abstracts}}]},
  booktitle = {{{ESCOP}} 2013: 18th {{Meeting}} of the {{European Society}} for {{Cognitive Psychology}}, {{Budapest}}, {{Hungary}}, {{August}} 29-{{September}} 1, 2013 [{{Abstracts}}]},
  publisher = {{European Society for Cognitive Psychology}},
  urldate = {2015-06-17},
  date = {2029-08},
  pages = {140-140},
  keywords = {auditory streaming,Auditory Localization,decision making,human perception,Perceptual Closure,perceptual decision},
  author = {Denham, Susan L. and Mill, Robert W. and Bohm, Tamas M. and Winkler, Istvan},
  file = {D\:\\Sauve\\Zotero\\storage\\NRP97CZ2\\Denham et al. - 2029 - Competition and cooperation in a model of auditory.pdf}
}

@article{hadenTimbreindependentExtractionPitch2009,
  title = {Timbre-Independent Extraction of Pitch in Newborn Infants},
  volume = {46},
  issn = {0048-5772},
  doi = {10.1111/j.1469-8986.2008.00749.x},
  abstract = {The ability to separate pitch from other spectral sound features, such as timbre, is an important prerequisite of veridical auditory perception underlying speech acquisition and music cognition. The current study investigated whether or not newborn infants generalize pitch across different timbres. Perceived resonator size is an aspect of timbre that informs the listener about the size of the sound source, a cue that may be important already at birth. Therefore, detection of infrequent pitch changes was tested by recording event-related brain potentials in healthy newborn infants to frequent standard and infrequent pitch-deviant sounds while the perceived resonator size of all sounds was randomly varied. The elicitation of an early negative and a later positive discriminative response by deviant sounds demonstrated that the neonate auditory system represents pitch separately from timbre, thus showing advanced pitch processing capabilities. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Psychophysiology},
  shortjournal = {Psychophysiology},
  date = {2009-01},
  pages = {69-74},
  keywords = {brain,Auditory Perception,Pitch (Frequency),timbre,pitch perception,Auditory Evoked Potentials,event-related brain potentials,mismatch negativity,Infant Development,neonates,resonator size},
  author = {Háden, Gábor P. and Stefanics, Gábor and Vestergaard, Martin D. and Denham, Susan L. and Sziller, István and Winkler, István}
}

@article{vestergaardAuditorySizedeviantDetection2009,
  title = {Auditory Size-Deviant Detection in Adults and Newborn Infants},
  volume = {82},
  issn = {0301-0511},
  doi = {10.1016/j.biopsycho.2009.07.004},
  abstract = {Auditory size perception refers to the ability to make accurate judgements of the size of a sound source based solely upon the sound emitted from the source. Electro-physiological and behavioural data were collected to test whether sound-source size parameters are detected from task-irrelevant sequences in adults and newborn infants. The mismatch negativity (MMN) obtained from adults indexed automatic detection of changes in size for voices, musical instruments and animal calls, regardless of whether the acoustic change indicated larger or smaller sources. Neonates detected changes in the size of a musical instrument. The data are consistent with the notion that auditory size-deviant detection in humans is an innate automatic process. This conclusion is compatible with the theory that the ability to assess the size of sound sources evolved because it provided selective advantage of being able to detect larger (more competent) suitors and larger (more dangerous) predators. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Biological Psychology},
  shortjournal = {Biological Psychology},
  date = {2009-10},
  pages = {169-175},
  keywords = {Auditory Perception,Auditory Discrimination,mismatch negativity,electroencephalography,Cognitive Processes,neonates,auditory system,automatic source-size pre-processing,glottal pulse rate,Size Discrimination,size perception,vocal tract length},
  author = {Vestergaard, Martin D. and Háden, Gábor P. and Shtyrov, Yury and Patterson, Roy D. and Pulvermüller, Friedemann and Denham, Sue L. and Sziller, István and Winkler, István}
}

@article{winklerModelingAuditoryScene2009,
  title = {Modeling the Auditory Scene: {{Predictive}} Regularity Representations and Perceptual Objects},
  volume = {13},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2009.09.003},
  shorttitle = {Modeling the Auditory Scene},
  abstract = {Predictive processing of information is essential for goal-directed behavior. We offer an account of auditory perception suggesting that representations of predictable patterns, or ‘regularities’, extracted from the incoming sounds serve as auditory perceptual objects. The auditory system continuously searches for regularities within the acoustic signal. Primitive regularities may be encoded by neurons adapting their response to specific sounds. Such neurons have been observed in many parts of the auditory system. Representations of the detected regularities produce predictions of upcoming sounds as well as alternative solutions for parsing the composite input into coherent sequences potentially emitted by putative sound sources. Accuracy of the predictions can be utilized for selecting the most likely interpretation of the auditory input. Thus in our view, perception generates hypotheses about the causal structure of the world. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {12},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  date = {2009-12},
  pages = {532-540},
  keywords = {Prediction,Auditory Perception,auditory scene,Causality,goal-directed behavior,modeling,perceptual objects,predictive regularity representations},
  author = {Winkler, István and Denham, Susan L. and Nelken, Israel}
}

@article{schrogerPredictiveRegularityRepresentations2014,
  title = {Predictive Regularity Representations in Violation Detection and Auditory Stream Segregation: {{From}} Conceptual to Computational Models},
  volume = {27},
  issn = {0896-0267},
  doi = {10.1007/s10548-013-0334-6},
  shorttitle = {Predictive Regularity Representations in Violation Detection and Auditory Stream Segregation},
  abstract = {Predictive accounts of perception have received increasing attention in the past 20 years. Detecting violations of auditory regularities, as reflected by the Mismatch Negativity (MMN) auditory event-related potential, is amongst the phenomena seamlessly fitting this approach. Largely based on the MMN literature, we propose a psychological conceptual framework called the Auditory Event Representation System (AERS), which is based on the assumption that auditory regularity violation detection and the formation of auditory perceptual objects are based on the same predictive regularity representations. Based on this notion, a computational model of auditory stream segregation, called CHAINS, has been developed. In CHAINS, the auditory sensory event representation of each incoming sound is considered for being the continuation of likely combinations of the preceding sounds in the sequence, thus providing alternative interpretations of the auditory input. Detecting repeating patterns allows predicting upcoming sound events, thus providing a test and potential support for the corresponding interpretation. Alternative interpretations continuously compete for perceptual dominance. In this paper, we briefly describe AERS and deduce some general constraints from this conceptual model. We then go on to illustrate how these constraints are computationally specified in CHAINS. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Brain Topography},
  shortjournal = {Brain Topography},
  date = {2014-07},
  pages = {565-577},
  keywords = {Auditory Evoked Potentials,auditory scene analysis,mismatch negativity,Mismatch negativity (MMN),Auditory object,Computational Modeling,Deviance detection,Predictive modelling},
  author = {Schröger, Erich and Bendixen, Alexandra and Denham, Susan L. and Mill, Robert W. and Bőhm, Tamás M. and Winkler, István}
}

@article{alainSelectivelyAttendingAuditory2000,
  langid = {english},
  title = {Selectively Attending to Auditory Objects},
  volume = {5},
  issn = {1093-9946},
  abstract = {The ability to maintain a conversation with one person while at a noisy cocktail party has often been used to illustrate a general characteristic of auditory selective attention, namely that perceivers' attention is usually directed to a particular set of sounds and not to others. Part of the cocktail party problem involves parsing co-occurring speech sounds and simultaneously integrating these various speech tokens into meaningful units ("auditory scene analysis"). Here, we review auditory perception and selective attention studies in an attempt to determine the role of perceptual organization in selective attention. Results from several behavioral and electrophysiological studies indicate that the ability to focus attention selectively on a particular sound source depends on a preliminary analysis that partitions the auditory input into distinct perceptual objects. Most findings can be accounted for by an object-based hypothesis in which auditory attention is allocated to perceptual objects derived from the auditory scene according to perceptual grouping principles.},
  journaltitle = {Frontiers in Bioscience: A Journal and Virtual Library},
  shortjournal = {Front. Biosci.},
  date = {2000-01-01},
  pages = {D202-212},
  keywords = {Attention,Auditory Perception,humans,electrophysiology,HEARING},
  author = {Alain, C. and Arnott, S. R.},
  eprinttype = {pmid},
  eprint = {10702369}
}

@article{arnottEffectsPerceptualContext2002,
  title = {Effects of Perceptual Context on Event-Related Brain Potentials during Auditory Spatial Attention},
  volume = {39},
  issn = {0048-5772},
  doi = {10.1017/S0048577202394149},
  abstract = {The effects of auditory spatial attention on event-related brain potentials (ERPs) were examined in situations that promoted stream segregation. Short and long noise bursts were presented at three azimuth locations and listeners were asked to respond to the longer sounds occurring at either the right- or left-most location. Data from 15 young adults (7 men; aged 18-39 yrs) were analyzed. In the baseline condition, the three sound sources were evenly spaced apart. In the distractor clustering conditions, middle and far sounds were clustered. In the attended clustering conditions, middle and attended sounds were clustered. ERP indices of attention, isolated as negative difference (Nd) waves, were greater over the hemisphere contralateral to the attended location. Nd waves were also larger when the middle sounds were moved toward the far distractors, consistent with an object-based gradient of auditory attention in which higher order information provided by the perceptual context influences selective processing. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {5},
  journaltitle = {Psychophysiology},
  shortjournal = {Psychophysiology},
  date = {2002-09},
  pages = {625-632},
  keywords = {Attention,Evoked Potentials,Auditory Stimulation,event-related brain potentials,Spatial Perception,auditory spatial attention,distractor clustering,perceptual context,selective processing},
  author = {Arnott, Stephen R. and Alain, Claude}
}

@article{alainBottomTopInfluences2001,
  title = {Bottom–up and Top–down Influences on Auditory Scene Analysis: {{Evidence}} from Event-Related Brain Potentials},
  volume = {27},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2001-18525-004&site=ehost-live},
  doi = {10.1037/0096-1523.27.5.1072},
  shorttitle = {Bottom–up and Top–down Influences on Auditory Scene Analysis},
  abstract = {The physiological processes underlying the segregation of concurrent sounds were investigated through the use of event-related brain potentials. The stimuli were complex sounds containing multiple harmonics, one of which could be mistuned so that it was no longer an integer multiple of the fundamental. Perception of concurrent auditory objects increased with degree of mistuning and was accompanied by negative and positive waves that peaked at 180 and 400 ms poststimulus, respectively. The negative wave, referred to as object-related negativity, was present during passive listening, but the positive wave was not. These findings indicate bottom–up and top–down influences during auditory scene analysis. Brain electrical source analyses showed that distinguishing simultaneous auditory objects involved a widely distributed neural network that included auditory cortices, the medial temporal lobe, and posterior association cortices. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {5},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-17},
  date = {2001-10},
  pages = {1072-1089},
  keywords = {Auditory Discrimination,Auditory Evoked Potentials,event-related brain potentials,Contingent Negative Variation,bottom-up influences,brain electrical source analysis,object-related negativity,perception of concurrent auditory objects,top-down influences},
  author = {Alain, Claude and Arnott, Stephen R. and Picton, Terence W.},
  file = {D\:\\Sauve\\Zotero\\storage\\V98A6HVX\\Alain et al. - 2001 - Bottom–up and top–down influences on auditory scen.pdf}
}

@article{arnottSteppingOutSpotlight2002,
  title = {Stepping out of the Spotlight: {{MMN}} Attenuation as a Function of Distance from the Attended Location},
  volume = {13},
  issn = {0959-4965},
  doi = {10.1097/00001756-200212030-00009},
  shorttitle = {Stepping out of the Spotlight},
  abstract = {In this report we present neurophysiological evidence that spatial separation between attended and unattended sound sources influences a listener's ability to register changes in sounds presented outside the focus of attention. Standard and deviant stimuli were presented at three azimuth locations. Participants were asked to press a key whenever they heard a deviant at a designated location. Mismatch negativity waves were generated for deviants at the attended location and were attenuated for deviants occurring 30° away from the attended location. Mismatch negativities were not observed at distances of 60° or more. The results are consistent with a spotlight model of auditory attention in which the processing of stimuli outside the attentional focus is attenuated as a function of increasing distance from the focus. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {17},
  journaltitle = {NeuroReport: For Rapid Communication of Neuroscience Research},
  shortjournal = {NeuroReport: For Rapid Communication of Neuroscience Research},
  date = {2002-12},
  pages = {2209-2212},
  keywords = {Auditory Localization,Stimulus Parameters,Auditory Evoked Potentials,mismatch negativity,selective attention,Neurophysiology,attended location,attenuated sound sources,distance,listener's ability,MMN attenuation,neurophysiological evidence,Spatial Organization,spatial separation,unattenuated sound sources},
  author = {Arnott, Stephen R. and Alain, Claude}
}

@article{duRapidTuningAuditory2015,
  title = {Rapid Tuning of Auditory 'what' and 'Where' Pathways by Training},
  volume = {25},
  issn = {1047-3211},
  doi = {10.1093/cercor/bht251},
  abstract = {Behavioral improvement within the first hour of training is commonly explained as procedural learning (i.e., strategy changes resulting from task familiarization). However, it may additionally reflect a rapid adjustment of the perceptual and/or attentional system in a goal-directed task. In support of this latter hypothesis, we show feature-specific gains in performance for groups of participants briefly trained to use either a spectral or spatial difference between 2 vowels presented simultaneously during a vowel identification task. In both groups, the neuromagnetic activity measured during the vowel identification task following training revealed source activity in auditory cortices, prefrontal, inferior parietal, and motor areas. More importantly, the contrast between the 2 groups revealed a striking double dissociation in which listeners trained on spectral or spatial cues showed higher source activity in ventral (“what”) and dorsal (“where”) brain areas, respectively. These feature-specific effects indicate that brief training can implicitly bias top-down processing to a trained acoustic cue and induce a rapid recalibration of the ventral and dorsal auditory streams during speech segregation and identification. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cerebral Cortex},
  date = {2015-02},
  pages = {496-506},
  keywords = {Attention,plasticity,Training,Auditory Cortex,HEARING,Learning,MEG,speech},
  author = {Du, Yi and He, Yu and Arnott, Stephen R. and Ross, Bernhard and Wu, Xihong and Li, Liang and Alain, Claude}
}

@article{yabeOrganizingSoundSequences2001,
  title = {Organizing Sound Sequences in the Human Brain: {{The}} Interplay of Auditory Streaming and Temporal Integration},
  volume = {897},
  issn = {0006-8993},
  doi = {10.1016/S0006-8993(01)02224-7},
  shorttitle = {Organizing Sound Sequences in the Human Brain},
  abstract = {Examined the relationship between 2 of the early brain processes of sound organization: auditory streaming and the temporal window of integration. The precedence between these 2 sound organization processes was determined by using the stimulus-omission mismatch negativity paradigm. With 10 healthy 27–40 yr olds, magnetic brain responses elicited by infrequent stimulus omissions appearing in a sequence of 2 alternating tones were recorded. The magnetic mismatch negativity event related potential (ERP) was elicited by tone omission when the alternating tones formed a single stream (with no or only small frequency separation between the 2 tones) but not when separate high and low streams emerged in perception (large frequency separation between the 2 alternating tones). It is concluded that this result shows auditory streaming takes precedence over the processes of temporal integration. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1-2},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  date = {2001-04},
  pages = {222-227},
  keywords = {Pitch (Frequency),Auditory Stimulation,Auditory Evoked Potentials,27–40 yr olds,infrequent tone omission in sequence of 2 alternating tones,magnetic mismatch negativity ERP indicating precedence of auditory streaming vs temporal integration,Sensory Integration,Stimulus Onset,Stimulus Variability},
  author = {Yabe, Hirooki and Winkler, István and Czigler, István and Koyama, Sachiko and Kakigi, Ryusuke and Sutoh, Takeyuki and Hiruma, Tomiharu and Kaneko, Sunao}
}

@incollection{cziglerTemporalCharacteristicsAuditory2003,
  location = {{Ashland, OH, US}},
  title = {Temporal Characteristics of Auditory Event-Synthesis: {{Electrophysiological}} Studies},
  isbn = {0-88937-281-0},
  shorttitle = {Temporal Characteristics of Auditory Event-Synthesis},
  abstract = {Mismatch-negativity component of event-related potentials (MMN) is a sensitive indicator of automatic change detection in the auditory modality. This wave is elicited whenever auditory events do not fit the rules stored in sensory memory. No attention to the irregular event is needed to elicit MMN (see Naatanen, 1992, for a review). MMN studies will be reviewed showing that the stream of auditory input is integrated into chunks of approximately 200 ms. However, shorter stimuli within such chunks may preserve their identities, i.e., they are available for further processing. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (chapter)},
  booktitle = {Time and Mind {{II}}: {{Information}} Processing Perspectives.},
  publisher = {{Hogrefe \& Huber Publishers}},
  date = {2003},
  pages = {117-124},
  keywords = {Auditory Evoked Potentials,electrophysiology,auditory event-related potentials,mismatch-negativity,Time},
  author = {Czigler, István and Winkler, István and Sussmann, Elyse and Yabe, Hirooki and Horváth, János},
  editor = {Helfrich, Hede and Helfrich, Hede (Ed)}
}

@article{zendelMusiciansExperienceLess2012,
  title = {Musicians Experience Less Age-Related Decline in Central Auditory Processing},
  volume = {27},
  issn = {0882-7974},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2011-20240-001&site=ehost-live},
  doi = {10.1037/a0024816},
  abstract = {Age-related decline in auditory perception reflects changes in the peripheral and central auditory systems. These age-related changes include a reduced ability to detect minute spectral and temporal details in an auditory signal, which contributes to a decreased ability to understand speech in noisy environments. Given that musical training in young adults has been shown to improve these auditory abilities, we investigated the possibility that musicians experience less age-related decline in auditory perception. To test this hypothesis we measured auditory processing abilities in lifelong musicians (N = 74) and nonmusicians (N = 89), aged between 18 and 91. Musicians demonstrated less age-related decline in some auditory tasks (i.e., gap detection and speech in noise), and had a lifelong advantage in others (i.e., mistuned harmonic detection). Importantly, the rate of age-related decline in hearing sensitivity, as measured by pure-tone thresholds, was similar between both groups, demonstrating that musicians experience less age-related decline in central auditory processing. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Psychology and Aging},
  shortjournal = {Psychology and Aging},
  urldate = {2015-06-17},
  date = {2012-06},
  pages = {410-417},
  keywords = {Musicians,Auditory Perception,Age Differences,aging,auditory processing,musician},
  author = {Zendel, Benjamin Rich and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\B7PMM6Z8\\Zendel and Alain - 2012 - Musicians experience less age-related decline in c.pdf}
}

@article{zendelInfluenceLifelongMusicianship2013,
  title = {The Influence of Lifelong Musicianship on Neurophysiological Measures of Concurrent Sound Segregation},
  volume = {25},
  issn = {0898-929X},
  doi = {10.1162/jocn_a_00329},
  abstract = {The ability to separate concurrent sounds based on periodicity cues is critical for parsing complex auditory scenes. This ability is enhanced in young adult musicians and reduced in older adults. Here, we investigated the impact of lifelong musicianship on concurrent sound segregation and perception using scalp-recorded ERPs. Older and younger musicians and nonmusicians were presented with periodic harmonic complexes where the second harmonic could be tuned or mistuned by 1–16\% of its original value. The likelihood of perceiving two simultaneous sounds increased with mistuning, and musicians, both older and younger, were more likely to detect and report hearing two sounds when the second harmonic was mistuned at or above 2\%. The perception of a mistuned harmonic as a separate sound was paralleled by an object-related negativity that was larger and earlier in younger musicians compared with the other three groups. When listeners made a judgment about the harmonic stimuli, the perception of the mistuned harmonic as a separate sound was paralleled by a positive wave at about 400 msec poststimulus (P400), which was enhanced in both older and younger musicians. These findings suggest attention-dependent processing of a mistuned harmonic is enhanced in older musicians and provides further evidence that age-related decline in hearing abilities are mitigated by musical training. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2013-04},
  pages = {503-516},
  keywords = {Musicians,Evoked Potentials,Auditory Perception,musical training,acoustics,Age Differences,Neurophysiology,auditory scenes,concurrent sound segregation,hearing abilities,lifelong musicianship,neurophysiological measures},
  author = {Zendel, Benjamin Rich and Alain, Claude}
}

@article{zendelConcurrentSoundSegregation2009,
  title = {Concurrent Sound Segregation Is Enhanced in Musicians},
  volume = {21},
  issn = {0898-929X},
  doi = {10.1162/jocn.2009.21140},
  abstract = {The ability to segregate simultaneously occurring sounds is fundamental to auditory perception. Many studies have shown that musicians have enhanced auditory perceptual abilities; however, the impact of musical expertise on segregating concurrently occurring sounds is unknown. Therefore, we examined whether long-term musical training can improve listeners' ability to segregate sounds that occur simultaneously. Participants were presented with complex sounds that had either all harmonics in tune or the second harmonic mistuned by 1\%, 2\%, 4\%, 8\%, or 16\% of its original value. The likelihood of hearing two sounds simultaneously increased with mistuning, and this effect was greater in musicians than nonmusicians. The segregation of the mistuned harmonic from the harmonic series was paralleled by an object-related negativity that was larger and peaked earlier in musicians. It also coincided with a late positive wave referred to as the P400 whose amplitude was larger in musicians than in nonmusicians. The behavioral and electrophysiological effects of musical expertise were specific to processing the mistuned harmonic as the N1, the N1c, and the P2 waves elicited by the tuned stimuli were comparable in both musicians and nonmusicians. These results demonstrate that listeners' ability to segregate concurrent sounds based on harmonicity is modulated by experience and provides a basis for further studies assessing the potential rehabilitative effects of musical training on solving complex scene analysis problems illustrated by the cocktail party example. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {8},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2009-08},
  pages = {1488-1498},
  keywords = {Musicians,Auditory Perception,Auditory Stimulation,concurrent sound segregation,complex sounds},
  author = {Zendel, Benjamin Rich and Alain, Claude}
}

@article{zendelEnhancedAttentiondependentActivity2014,
  title = {Enhanced Attention-Dependent Activity in the Auditory Cortex of Older Musicians},
  volume = {35},
  issn = {0197-4580},
  doi = {10.1016/j.neurobiolaging.2013.06.022},
  abstract = {Musical training improves auditory processing abilities, which correlates with neuro-plastic changes in exogenous (input-driven) and endogenous (attention-dependent) components of auditory event-related potentials (ERPs). Evidence suggests that musicians, compared to non-musicians, experience less age-related decline in auditory processing abilities. Here, we investigated whether lifelong musicianship mitigates exogenous or endogenous processing by measuring auditory ERPs in younger and older musicians and non-musicians while they either attended to auditory stimuli or watched a muted subtitled movie of their choice. Both age and musical training-related differences were observed in the exogenous components; however, the differences between musicians and non-musicians were similar across the lifespan. These results suggest that exogenous auditory ERPs are enhanced in musicians, but decline with age at the same rate. On the other hand, attention-related activity, modeled in the right auditory cortex using a discrete spatiotemporal source analysis, was selectively enhanced in older musicians. This suggests that older musicians use a compensatory strategy to overcome age-related decline in peripheral and exogenous processing of acoustic information. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Neurobiology of Aging},
  shortjournal = {Neurobiology of Aging},
  date = {2014-01},
  pages = {55-63},
  keywords = {Attention,Musicians,Training,acoustics,Auditory Evoked Potentials,Age Differences,Auditory Cortex,event related potentials,older musicians},
  author = {Zendel, Benjamin Rich and Alain, Claude}
}

@article{alainTurningNoiseBenefit2014,
  title = {Turning down the Noise: {{The}} Benefit of Musical Training on the Aging Auditory Brain},
  volume = {308},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2013.06.008},
  shorttitle = {Turning down the Noise},
  abstract = {Age-related decline in hearing abilities is a ubiquitous part of aging, and commonly impacts speech understanding, especially when there are competing sound sources. While such age effects are partially due to changes within the cochlea, difficulties typically exist beyond measurable hearing loss, suggesting that central brain processes, as opposed to simple peripheral mechanisms (e.g., hearing sensitivity), play a critical role in governing hearing abilities late into life. Current training regimens aimed to improve central auditory processing abilities have experienced limited success in promoting listening benefits. Interestingly, recent studies suggest that in young adults, musical training positively modifies neural mechanisms, providing robust, long-lasting improvements to hearing abilities as well as to non-auditory tasks that engage cognitive control. These results offer the encouraging possibility that musical training might be used to counteract age-related changes in auditory cognition commonly observed in older adults. Here, we reviewed studies that have examined the effects of age and musical experience on auditory cognition with an emphasis on auditory scene analysis. We infer that musical training may offer potential benefits to complex listening and might be utilized as a means to delay or even attenuate declines in auditory perception and cognition that often emerge later in life. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  journaltitle = {Hearing Research},
  shortjournal = {Hearing Research},
  date = {2014-02},
  pages = {162-173},
  keywords = {Training,Music,Auditory Stimulation,musical training,Hearing Disorders,Music Education,aging,auditory cognition & processing,cognitive control,hearing loss},
  author = {Alain, Claude and Zendel, Benjamin Rich and Hutka, Stefanie and Bidelman, Gavin M.}
}

@article{singhInfluenceDifferentTimbre1997,
  title = {The Influence of Different Timbre Attributes on the Perceptual Segregation of Complex-Tone Sequences},
  volume = {102},
  issn = {0001-4966},
  doi = {10.1121/1.419688},
  abstract = {Investigated the influence of different timbre attributes on the perceptual organization of tone sequences. In particular this article examined the relative efficacy of differences in amplitude-envelope features, and harmonic content on stream segregation. A 2nd goal was to devise a paradigm that would provide a common scale against which to measure or "titrate" the potency of different physical features as initiators of stream segregation. Sequences of sounds with different spectral and temporal-amplitude envelope features were constructed following a 2-factor design to generate 4 presentation conditions with 10 listeners (aged 21–36 yrs). Results corroborated the importance of spectral differences in facilitating stream segregation. In the 2 conditions under which the sounds in a sequence differed from each other in terms of number of harmonics, Ss registered significantly lower ΔF0 values for segregation than the reference condition in which there was a difference between sounds in terms of harmonic structure. Ss attained significantly lower crossover points for the condition under which changes were made only in the envelope feature of sounds. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {4},
  journaltitle = {Journal of the Acoustical Society of America},
  shortjournal = {Journal of the Acoustical Society of America},
  date = {1997-10},
  pages = {1943-1952},
  keywords = {Pitch (Frequency),Auditory Discrimination,Auditory Stimulation,Pitch Discrimination,21–36 yr olds,harmonic content & amplitude envelope features,perceptual segregation of complex tone sequences,Stimulus Complexity},
  author = {Singh, Punita G. and Bregman, Albert S.}
}

@article{mcadamsPerceptualScalingSynthesized1995,
  title = {Perceptual Scaling of Synthesized Musical Timbres: {{Common}} Dimensions, Specificities, and Latent Subject Classes},
  volume = {58},
  issn = {0340-0727},
  doi = {10.1007/BF00419633},
  shorttitle = {Perceptual Scaling of Synthesized Musical Timbres},
  abstract = {Examined the perceptual structure of musical timbre and effects of musical training, using the ratings of 98 Ss (aged 18–57 yrs), with varying musical training, on timbral dissimilarities of synthesized instrument sounds. Ss in professional, amateur, and nonmusician groups, also completed a questionnaire on the amount and kind of musical training received, number of years of music making, and amounts and types of music listening. Results reveal 5 latent classes of a 3-dimensional spatial model. Further, musical timbres possessed specific attributes not accounted for by these shared perceptual dimensions. Weight patterns indicated that perceptual salience of dimensions and specificities varied across classes. A comparison of class structure with biographical factors associated with degree of musical training and activity was not clearly related to class structure. (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  number = {3},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  date = {1995-12},
  pages = {177-192},
  keywords = {Musicians,Auditory Perception,Music,Pitch (Frequency),18–57 yr old professional vs amateur musicians vs nonmusicians,Experience Level,France,perceptual structure of musical timbre & perceived timbral dissimilarities of synthesized instrument sounds},
  author = {McAdams, Stephen and Winsberg, Suzanne and Donnadieu, Sophie and De Soete, Geert and Krimphoff, Jochen}
}

@article{hartmannStreamSegregationPeripheral1991,
  title = {Stream Segregation and Peripheral Channeling},
  volume = {9},
  issn = {0730-7829},
  abstract = {Examined whether stream segregation is mediated entirely by channeling that is established in the auditory periphery or whether more complicated principles of source grouping are at work. Seven listeners with normal hearing and some musical experience performed 2 interleaved melody identification tasks. Data show that peripheral channeling was of paramount importance, suggesting that a set of rather simple rules can predict whether 2 interleaved melodies will be perceived as segregated or not. The data reveal a secondary effect of tone duration. Otherwise, in the absence of peripheral channeling, the experiments find little or no stream segregation, even in those cases where individual tones should clearly evoke images of different sources. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {2},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Win  1991},
  pages = {155-183},
  keywords = {Auditory Perception,Music,auditory peripheral channeling,listeners,stream segregation in interleaved melody identification},
  author = {Hartmann, William M. and Johnson, Douglas}
}

@article{beauvoisEffectToneDuration1998,
  title = {The Effect of Tone Duration on Auditory Stream Formation},
  volume = {60},
  issn = {0031-5117},
  doi = {10.3758/BF03206068},
  abstract = {Studied the effect of tone duration on the formation of auditory streams. 17 20–32-yr-olds were presented with 15-sec alternating pure-tone sequences (e.g., ABAB) and were asked to orient their attention over the duration of the sequence toward hearing either a temporally coherent or a segregated percept. At stimulus offset, Ss indicated whether their percept had been that of a temporally coherent ABAB trill or that of segregated A and B streams. Results indicate that the occurrence of stream segregation increases as (1) the duration of the A and B tones increases in unison, and (2) the difference in duration between the A and B tones increases, with the duration differences between the tones producing the strongest segregation effects. A simulation of the results using the M. W. Beauvois and R. Meddis (1996) stream segregation model suggests that both the tone duration effects reported here, and Gestalt auditory grouping on the basis of temporal proximity can be understood in terms of low-level neurophysiological processes and peripheral-channeling factors. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {5},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1998-07},
  pages = {852-861},
  keywords = {Auditory Perception,Pitch (Frequency),20–32 yr olds,auditory stream formation,Stimulus Duration,tone duration},
  author = {Beauvois, Michael W.}
}

@article{beauvoisTimeDelayAuditory1997,
  title = {Time Delay of Auditory Stream Biasing},
  volume = {59},
  issn = {0031-5117},
  doi = {10.3758/BF03206850},
  abstract = {Investigated the time decay of auditory stream biasing (ASB). 18 Ss (aged 19–30 yrs) were required to listen to a 1-sec induction sequence of repeated tones (AAAA…) designed to bias the listener's percept toward hearing an A stream. The induction sequence was followed immediately by a silent interval (0–8 sec), and then a short ABAB… test sequence. To measure the amount of ASB remaining at the end of the silent interval, Ss were asked to indicate whether the test sequence was temporally coherent or had segregated into separate A and B streams. A plot of the mean number of segregation responses against silent-interval duration indicated that the overall time decay of ASB can be described by an exponential decay function with a time constant of τ\hspace{0.6em}=\hspace{0.6em}3.84 sec, with musicians having a longer time constant (τ\hspace{0.6em}=\hspace{0.6em}7.84 sec) than nonmusicians (τ\hspace{0.6em}=\hspace{0.6em}1.42 sec). The length of the time constants for musicians and nonmusicians suggests that the mechanism responsible for ASB is associated with long auditory storage and that future experiments investigating auditory streaming phenomena should use interstimulus intervals of at least 8 sec. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1997-01},
  pages = {81-86},
  keywords = {Auditory Perception,Pitch (Frequency),Auditory Discrimination,Auditory Stimulation,19–30 yr olds,auditory stream biasing through sequence of repeated tones & silent interval & short segregated tones,Constant Time Delay,time delay & perception of coherent vs segregated tone streams},
  author = {Beauvois, Michael W. and Meddis, Ray}
}

@article{beauvoisComputerSimulationAuditory1996,
  title = {Computer Simulation of Auditory Stream Segregation in Alternating-Tone Sequences},
  volume = {99},
  issn = {0001-4966},
  doi = {10.1121/1.415414},
  abstract = {Presents a computer model to show that a number of auditory figure/ground phenomena can be accounted for by a small number of low-level processing principles compatible with the known physiology of the peripheral auditory system. A summary of model features is outlined. The operation of the model suggests that some Gestalt auditory grouping may be the product of low-level processes. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  issue = {4, Pt 1},
  journaltitle = {Journal of the Acoustical Society of America},
  shortjournal = {Journal of the Acoustical Society of America},
  date = {1996-04},
  pages = {2270-2280},
  keywords = {Auditory Discrimination,computer model of low-level processing principles related to physiology of peripheral auditory system,Figure Ground Discrimination,Gestalt group of auditory figure/ground phenomena,Gestalt Psychology,Mathematical Modeling,Peripheral Nervous System},
  author = {Beauvois, Michael W. and Meddis, Ray}
}

@article{beauvoisComputerModelAuditory1991,
  title = {A Computer Model of Auditory Stream Segregation},
  volume = {43A},
  issn = {0272-4987},
  abstract = {Describes a computer model that simulates some aspects of auditory stream segregation. The model consists of a multichannel bandpass-filter bank with a "noisy" output and an attentional mechanism that responds selectively to the channel with the greatest activity. The model produced similar results to 2 experimental demonstrations of streaming phenomena. In one experiment, 6 Ss listened to an alternating tone sequence and determined whether the percept was streaming or temporally coherent. The other experiment used data from a study by S. Anstis and S. Saida (see record [rid]1986-10938-001[/rid]). Results are discussed in terms of the emergent properties of a system governed by simple physiological principles. As such, the model is contrasted with higher-level Gestalt explanations of the same phenomena while accepting that they may constitute complementary kinds of explanation. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {3},
  journaltitle = {The Quarterly Journal of Experimental Psychology A: Human Experimental Psychology},
  shortjournal = {The Quarterly Journal of Experimental Psychology A: Human Experimental Psychology},
  series = {Hearing and Speech},
  date = {1991-08},
  pages = {517-541},
  keywords = {Auditory Discrimination,26–45 yr olds,computer simulated auditory stream segregation for successive pure tone stimuli & empirical verification,Computer Simulation},
  author = {Beauvois, Michael W. and Meddis, Ray}
}

@article{roseEffectsFrequencyLevel2000,
  langid = {english},
  title = {Effects of Frequency and Level on Auditory Stream Segregation},
  volume = {108},
  issn = {0001-4966},
  abstract = {This study examined the effect of center frequency and level on the perceptual grouping of rapid tone sequences. The sequence ABA-ABA-...was used, where A and B represent sinusoidal tone bursts (10-ms rise/fall, 80-ms steady state, 20-ms interval between tones) and - represents a silent interval of 120 ms. In experiment 1, tone A was fixed in frequency at 62, 125, 250, 500, 1000, 2000, 4000, 6000, or 8000 Hz. Both tones had a level of approximately 40 dB SL. Tone B started with a frequency well above that of tone A, and its frequency was swept toward that of tone A so that the frequency separation between them decreased in an exponential manner. Subjects were required to indicate when they could no longer hear the tones A and B as two separate streams, but heard only a single stream with a "gallop" rhythm. This changeover point between percepts is called the fission boundary. The separation between tones A and B at the fission boundary was roughly independent of the frequency of tone A when expressed as the difference in number of equivalent rectangular bandwidths (ERBs) between A and B, but varied more with frequency when the difference was expressed in barks or cents. In experiment 2, the center frequency was fixed at 250, 1000, or 4000 Hz, and the level of the A and B tones was 40, 55, 70, or 85 dB SPL. The frequency separation of the A and B tones at the fission boundary tended to increase slightly with increasing level, in a manner consistent with the broadening of the auditory filter with increasing level. The results support the "peripheral channeling" explanation of stream segregation advanced by Hartmann and Johnson [Music Percept. 9, 155-184 (1991)], and indicate that the perception of fusion or fission in alternating-tone sequences depends partly upon the degree of overlap of the excitation patterns evoked by the successive sounds in the cochlea, as assumed in the theory of Beauvois and Meddis [J. Acoust. Soc. Am. 99, 2270-2280 (1996)].},
  issue = {3 Pt 1},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  date = {2000-09},
  pages = {1209-1214},
  keywords = {Auditory Perception,humans,Models; Biological,Adult,Auditory Threshold,Middle Aged},
  author = {Rose, M. M. and Moore, B. C.},
  eprinttype = {pmid},
  eprint = {11008821}
}

@article{marozeauDependencyTimbreFundamental2003,
  langid = {english},
  title = {The Dependency of Timbre on Fundamental Frequency},
  volume = {114},
  issn = {0001-4966},
  abstract = {The dependency of the timbre of musical sounds on their fundamental frequency (F0) was examined in three experiments. In experiment I subjects compared the timbres of stimuli produced by a set of 12 musical instruments with equal F0, duration, and loudness. There were three sessions, each at a different F0. In experiment II the same stimuli were rearranged in pairs, each with the same difference in F0, and subjects had to ignore the constant difference in pitch. In experiment III, instruments were paired both with and without an F0 difference within the same session, and subjects had to ignore the variable differences in pitch. Experiment I yielded dissimilarity matrices that were similar at different F0's, suggesting that instruments kept their relative positions within timbre space. Experiment II found that subjects were able to ignore the salient pitch difference while rating timbre dissimilarity. Dissimilarity matrices were symmetrical, suggesting further that the absolute displacement of the set of instruments within timbre space was small. Experiment III extended this result to the case where the pitch difference varied from trial to trial. Multidimensional scaling (MDS) of dissimilarity scores produced solutions (timbre spaces) that varied little across conditions and experiments. MDS solutions were used to test the validity of signal-based predictors of timbre, and in particular their stability as a function of F0. Taken together, the results suggest that timbre differences are perceived independently from differences of pitch, at least for F0 differences smaller than an octave. Timbre differences can be measured between stimuli with different F0's.},
  number = {5},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  date = {2003-11},
  pages = {2946-2957},
  keywords = {pitch perception,humans,Adult,Female,Male,Models; Theoretical},
  author = {Marozeau, Jeremy and de Cheveigné, Alain and McAdams, Stephen and Winsberg, Suzanne},
  options = {useprefix=true},
  eprinttype = {pmid},
  eprint = {14650028}
}

@article{gutschalkNeuralCorrelatesAuditory2008,
  title = {Neural {{Correlates}} of {{Auditory Perceptual Awareness}} under {{Informational Masking}}},
  volume = {6},
  url = {http://dx.doi.org/10.1371/journal.pbio.0060138},
  doi = {10.1371/journal.pbio.0060138},
  abstract = {How does the brain process sound when we can't always be aware of all auditory information at once? A new study shows that auditory processing depends on whether the sound is consciously perceived.},
  number = {6},
  journaltitle = {PLoS Biol},
  shortjournal = {PLoS Biol},
  urldate = {2015-06-17},
  date = {2008-06-10},
  pages = {e138},
  author = {Gutschalk, Alexander and Micheyl, Christophe and Oxenham, Andrew J},
  file = {D\:\\Sauve\\Zotero\\storage\\8TR7ISZS\\Gutschalk et al. - 2008 - Neural Correlates of Auditory Perceptual Awareness.pdf}
}

@article{wilsonCorticalFMRIActivation2007,
  title = {Cortical {{fMRI}} Activation to Sequences of Tones Alternating in Frequency: {{Relationship}} to Perceived Rate and Streaming},
  volume = {97},
  issn = {0022-3077},
  doi = {10.1152/jn.00788.2006},
  shorttitle = {Cortical {{fMRI}} Activation to Sequences of Tones Alternating in Frequency},
  abstract = {Human listeners were functionally imaged while reporting their perception of sequences of alternating-frequency tone bursts separated by 0, 1/8, 1, or 20 semitones. Our goal was to determine whether functional magnetic resonance imaging (fMRI) activation of auditory cortex changes with frequency separation in a manner predictable from the perceived rate of the stimulus. At the null and small separations, the tones were generally heard as a single stream with a perceived rate equal to the physical tone presentation rate. fMRI activation in auditory cortex was appreciably phasic, showing prominent peaks at the sequence onset and offset. At larger-frequency separations, the higher- and lower-frequency tones perceptually separated into two streams, each with a rate equal to half the overall tone presentation rate. Under those conditions, fMRI activation in auditory cortex was more sustained throughout the sequence duration and was larger in magnitude and extent. Phasic to sustained changes in fMRI activation with changes in frequency separation and perceived rate are comparable to, and consistent with, those produced by changes in the physical rate of a sequence and are far greater than the effects produced by changing other physical stimulus variables, such as sound level or bandwidth. We suggest that the neural activity underlying the changes in fMRI activation with frequency separation contribute to the coding of the co-occurring changes in perceived rate and perceptual organization of the sound sequences into auditory streams. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  date = {2007-03},
  pages = {2230-2238},
  keywords = {Auditory Perception,Pitch (Frequency),Pitch Discrimination,Auditory Cortex,Magnetic Resonance Imaging,alternating-frequency tone bursts,fMRI activation,Functional Magnetic Resonance Imaging},
  author = {Wilson, E. Courtenay and Melcher, Jennifer R. and Micheyl, Christophe and Gutschalk, Alexander and Oxenham, Andrew J.}
}

@article{gutschalkHumanCorticalActivity2007,
  title = {Human Cortical Activity during Streaming without Spectral Cues Suggests a General Neural Substrate for Auditory Stream Segregation},
  volume = {27},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.2299-07.2007},
  abstract = {The brain continuously disentangles competing sounds, such as two people speaking, and assigns them to distinct streams. Neural mechanisms have been proposed for streaming based on gross spectral differences between sounds, but not for streaming based on other nonspectral features. Here, human listeners were presented with sequences of harmonic complex tones that had identical spectral envelopes, and unresolved spectral fine structure, but one of two fundamental frequencies (f₀) and pitches. As the f₀ difference between tones increased, listeners perceived the tones as being segregated into two streams (one stream for each f₀ ) and cortical activity measured with functional magnetic resonance imaging and magnetoencephalography increased. This trend was seen in primary cortex of Heschl's gyrus and in surrounding nonprimary areas. The results strongly resemble those for pure tones. Both the present and pure tone results may reflect neuronal forward suppression that diminishes as one or more features of successive sounds become increasingly different. We hypothesize that feature-specific forward suppression subserves streaming based on diverse perceptual cues and results in explicit neural representations for auditory streams within auditory cortex. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {48},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {The Journal of Neuroscience},
  date = {2007-11},
  pages = {13074-13081},
  keywords = {brain,Auditory Stimulation,Cues,Auditory Cortex,tones,Afferent Pathways,neural mechanisms,neural streaming,Neurology,spectral cues},
  author = {Gutschalk, Alexander and Oxenham, Andrew J. and Micheyl, Christophe and Wilson, E. Courtenay and Melcher, Jennifer R.}
}

@article{womelsdorfRoleNeuronalSynchronization2007,
  title = {The Role of Neuronal Synchronization in Selective Attention},
  volume = {17},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2007.02.002},
  abstract = {Attention selectively enhances the influence of neuronal responses conveying information about relevant sensory attributes. Accumulating evidence suggests that this selective neuronal modulation relies on rhythmic synchronization at local and long-range spatial scales: attention selectively synchronizes the rhythmic responses of those neurons that are tuned to the spatial and featural attributes of the attended sensory input. The strength of synchronization is thereby functionally related to perceptual accuracy and behavioural efficiency. Complementing this synchronization at a local level, attention has recently been demonstrated to regulate which locally synchronized neuronal groups phase-synchronize their rhythmic activity across long-range connections. These results point to a general computational role for selective synchronization in dynamically controlling which neurons communicate information about sensory inputs effectively. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  date = {2007-04},
  pages = {154-160},
  keywords = {selective attention,Neurology,neuronal modulation,neuronal synchronization role,Neurons,sensory attributes},
  author = {Womelsdorf, Thilo and Fries, Pascal}
}

@article{nieburSynchronyNeuronalMechanism2002,
  title = {Synchrony: {{A}} Neuronal Mechanism for Attentional Selection?},
  volume = {12},
  issn = {0959-4388},
  doi = {10.1016/S0959-4388(02)00310-0},
  shorttitle = {Synchrony},
  abstract = {Reviews studies on the neural mechanisms of descending attentional selection. Attentional selection involves brain processes that select and control the flow of information into the mechanisms that underlie perception and consciousness. One theory proposes that the neural activity that represents the stimuli or events to be attended to is selected through modification of its synchrony. Recent experimental evidence supports this theory, by showing that changes in attentional focus increase the synchrony of neural firing in some neuron pairs and decrease it in others. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {2},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  date = {2002-04},
  pages = {190-194},
  keywords = {selective attention,Neurophysiology,neural mechanisms,attentional selection,synchrony},
  author = {Niebur, Ernst and Hsiao, Steven S. and Johnson, Kenneth O.}
}

@article{rahneMultilevelCrossmodalApproach2008,
  title = {A Multilevel and Cross-Modal Approach towards Neuronal Mechanisms of Auditory Streaming},
  volume = {1220},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2007.08.011},
  abstract = {We report first results of a multilevel, cross-modal study on the neuronal mechanisms underlying auditory sequential streaming, with the focus on the impact of visual sequences on perceptually ambiguous tone sequences which can either be perceived as two separate streams or one alternating stream. We combined two psychophysical experiments performed on humans and monkeys with two human brain imaging experiments which allow to obtain complementary information on brain activation with high spatial (fMRI) and high temporal (MEG) resolution. The same acoustic paradigm based on the pairing of tone sequences with visual stimuli was used in all human studies and, in an adapted version, in the psychophysical study on monkeys. Our multilevel approach provides experimental evidence that the pairing of auditory and visual stimuli can reliably introduce a bias towards either an integrated or a segregated perception of ambiguous sequences. Thus, comparable to an explicit instruction, this approach can be used to control the subject's perceptual organization of an ambiguous sound sequence without the need for the subject to directly report it. This finding is of particular importance for animal studies because it allows to compare electrophysiological responses of auditory cortex neurons to the same acoustic stimulus sequence eliciting either a segregated or integrated percept. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Active Listening},
  date = {2008-07},
  pages = {118-131},
  keywords = {Auditory Stimulation,Auditory Evoked Potentials,Auditory Cortex,auditory sequential streaming,electrophysiological responses,neuronal mechanisms},
  author = {Rahne, Torsten and Deike, Susann and Selezneva, Elena and Brosch, Michael and König, Reinhard and Scheich, Henning and Böckmann, Martin and Brechmann, André}
}

@book{bregmanAuditorySceneAnalysis1990,
  langid = {english},
  title = {Auditory {{Scene Analysis}}: {{The Perceptual Organization}} of {{Sound}}},
  isbn = {978-0-262-52195-6},
  shorttitle = {Auditory {{Scene Analysis}}},
  abstract = {"Bregman has written a major book, a unique and important contribution to the rapidly expanding field of complex auditory perception. This is a big, rich, and fulfilling piece of work that deserves the wide audience it is sure to attract." -- Stewart H. Hulse, "Science" Auditory Scene Analysis addresses the problem of hearing complex auditory environments, using a series of creative analogies to describe the process required of the human auditory system as it analyzes mixtures of sounds to recover descriptions of individual sounds. In a unified and comprehensive way, Bregman establishes a theoretical framework that integrates his findings with an unusually wide range of previous research in psychoacoustics, speech perception, music theory and composition, and computer modeling.},
  pagetotal = {800},
  publisher = {{MIT Press}},
  date = {1990},
  keywords = {Psychology / Cognitive Psychology,Psychology / Cognitive Psychology & Cognition,Science / General},
  author = {Bregman, Albert S.}
}

@article{elhilaliTemporalCoherencePerceptual2009,
  title = {Temporal {{Coherence}} in the {{Perceptual Organization}} and {{Cortical Representation}} of {{Auditory Scenes}}},
  volume = {61},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627308010532},
  doi = {10.1016/j.neuron.2008.12.005},
  abstract = {Summary
Just as the visual system parses complex scenes into identifiable objects, the auditory system must organize sound elements scattered in frequency and time into coherent “streams.” Current neurocomputational theories of auditory streaming rely on tonotopic organization of the auditory system to explain the observation that sequential spectrally distant sound elements tend to form separate perceptual streams. Here, we show that spectral components that are well separated in frequency are no longer heard as separate streams if presented synchronously rather than consecutively. In contrast, responses from neurons in primary auditory cortex of ferrets show that both synchronous and asynchronous tone sequences produce comparably segregated responses along the tonotopic axis. The results argue against tonotopic separation per se as a neural correlate of stream segregation. Instead we propose a computational model of stream segregation that can account for the data by using temporal coherence as the primary criterion for predicting stream formation.},
  number = {2},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2015-06-17},
  date = {2009-01-29},
  pages = {317-329},
  author = {Elhilali, Mounya and Ma, Ling and Micheyl, Christophe and Oxenham, Andrew J. and Shamma, Shihab A.},
  file = {D\:\\Sauve\\Zotero\\storage\\VM95QRU5\\Elhilali et al. - 2009 - Temporal Coherence in the Perceptual Organization .pdf;D\:\\Sauve\\Zotero\\storage\\9CQBSQIK\\S0896627308010532.html}
}

@article{shinn-cunninghamSoundElementGets2007,
  title = {A Sound Element Gets Lost in Perceptual Competition},
  volume = {104},
  issn = {0027-8424},
  doi = {10.1073/pnas.0704641104},
  abstract = {Our ability to understand auditory signals depends on properly separating the mixture of sound arriving from multiple sources. Sound elements tend to belong to only one object at a time, consistent with the principle of disjoint allocation, although there are instances of duplex perception or coallocation, in which two sound objects share one sound element. Here we report an effect of "nonallocation," in which a sound element "disappears" when two ongoing objects compete for its ownership. When a target tone is presented either as one of a sequence of tones or simultaneously with a harmonic vowel complex, it is heard as part of the corresponding object. However, depending on the spatial configuration of the scene, if the target, the tones, and the vowel are all presented together, the target may not be perceived in either the tones or the vowel, even though it is not perceived as a separate entity. This finding suggests an asymmetry in the strength of the perceptual evidence required to reject vs. to include an element within the auditory foreground, a result with important implications for how we process complex auditory scenes containing ambiguous information. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {29},
  journaltitle = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  date = {2007-07},
  pages = {12223-12227},
  keywords = {Attention,Auditory Perception,Auditory Discrimination,Auditory Stimulation,streaming,auditory scene analysis,Spatial Organization,auditory objects,harmonic vowel,perceptual competition,sound element,spatial configuration complex,spatial hearing,Vowels},
  author = {Shinn-Cunningham, Barbara G. and Lee, Adrian K. C. and Oxenham, Andrew J.}
}

@article{krumhanslPerceivingTonalStructure1985,
  title = {Perceiving Tonal Structure in Music},
  volume = {73},
  issn = {0003-0996},
  abstract = {Summarizes studies on the knowledge that listeners have about certain aspects of pitch structure in traditional Western music. The quantification of the tonal hierarchy, the derivation of a map of musical keys, the perceived relations between tones and between chords, and the perception of and memory for tonal music are described. The convergence between the distribution of tones in tonal compositions and probe-tone ratings suggests that listeners internalize the statistical properties of music. Findings also indicate that the psychological tonal hierarchy depends more on the ways tones are used in tonal music than on the single factor of consonance. Further studies suggest that listeners interpret chords in terms of their harmonic functions in the system of musical keys. (43 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {4},
  journaltitle = {American Scientist},
  shortjournal = {American Scientist},
  date = {1985-07},
  pages = {371-378},
  keywords = {Auditory Perception,Music,Pitch (Frequency),perception & memory of tone in music},
  author = {Krumhansl, Carol L.}
}

@article{careyGeneralitySpecificityEffects2015,
  title = {Generality and Specificity in the Effects of Musical Expertise on Perception and Cognition},
  volume = {137},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2014.12.005},
  abstract = {Performing musicians invest thousands of hours becoming experts in a range of perceptual, attentional, and cognitive skills. The duration and intensity of musicians’ training—Far greater than that of most educational or rehabilitation programs—Provides a useful model to test the extent to which skills acquired in one particular context (music) generalize to different domains. Here, we asked whether the instrument-specific and more instrument-general skills acquired during professional violinists’ and pianists’ training would generalize to superior performance on a wide range of analogous (largely non-musical) skills, when compared to closely matched non-musicians. Violinists and pianists outperformed non-musicians on fine-grained auditory psychophysical measures, but surprisingly did not differ from each other, despite the different demands of their instruments. Musician groups did differ on a tuning system perception task: violinists showed clearest biases towards the tuning system specific to their instrument, suggesting that long-term experience leads to selective perceptual benefits given a training-relevant context. However, we found only weak evidence of group differences in non-musical skills, with musicians differing marginally in one measure of sustained auditory attention, but not significantly on auditory scene analysis or multi-modal sequencing measures. Further, regression analyses showed that this sustained auditory attention metric predicted more variance in one auditory psychophysical measure than did musical expertise. Our findings suggest that specific musical expertise may yield distinct perceptual outcomes within contexts close to the area of training. Generalization of expertise to relevant cognitive domains may be less clear, particularly where the task context is non-musical. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  date = {2015-04},
  pages = {81-105},
  keywords = {Musicians,cognition,Experience Level,Expertise,Generalization,Perception},
  author = {Carey, Daniel and Rosen, Stuart and Krishnan, Saloni and Pearce, Marcus T. and Shepherd, Alex and Aydelott, Jennifer and Dick, Frederic}
}

@article{pearceExpectationMelodyInfluence2006,
  title = {Expectation in Melody: {{The}} Influence of Context and Learning},
  volume = {23},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.23.5.377},
  shorttitle = {Expectation in Melody},
  abstract = {The Implication-Realization (IR) theory (Narmour, 1990) posits two cognitive systems involved in the generation of melodic expectations: The first consists of a limited number of symbolic rules that are held to be innate and universal; the second reflects the top-down influences of acquired stylistic knowledge. Aspects of both systems have been implemented as quantitative models in research which has yielded empirical support for both components of the theory (Cuddy \& Lunny, 1995; Krumhansl, 1995a, 1995b; Schellenberg, 1996, 1997). However, there is also evidence that the implemented bottom-up rules constitute too inflexible a model to account for the influence of the musical experience of the listener and the melodic context in which expectations are elicited. A theory is presented, according to which both bottom-up and top-down descriptions of observed patterns of melodic expectation may be accounted for in terms of the induction of statistical regularities in existing musical repertoires. A computational model that embodies this theory is developed and used to reanalyze existing experimental data on melodic expectancy. The results of three experiments with increasingly complex melodic stimuli demonstrate that this model is capable of accounting for listeners' expectations as well as or better than the two-factor model of Schellenberg (1997). (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {5},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  date = {2006-06},
  pages = {377-405},
  keywords = {music perception,Expectations,computational model,Implication Realization theory,melodic expectation},
  author = {Pearce, Marcus T. and Wiggins, Geraint A.}
}

@article{pearceUnsupervisedStatisticalLearning2010,
  title = {Unsupervised Statistical Learning Underpins Computational, Behavioural, and Neural Manifestations of Musical Expectation},
  volume = {50},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2009.12.019},
  abstract = {The ability to anticipate forthcoming events has clear evolutionary advantages, and predictive successes or failures often entail significant psychological and physiological consequences. In music perception, the confirmation and violation of expectations are critical to the communication of emotion and aesthetic effects of a composition. Neuroscientific research on musical expectations has focused on harmony. Although harmony is important in Western tonal styles, other musical traditions, emphasizing pitch and melody, have been rather neglected. In this study, we investigated melodic pitch expectations elicited by ecologically valid musical stimuli by drawing together computational, behavioural, and electrophysiological evidence. Unlike rule-based models, our computational model acquires knowledge through unsupervised statistical learning of sequential structure in music and uses this knowledge to estimate the conditional probability (and information content) of musical notes. Unlike previous behavioural paradigms that interrupt a stimulus, we devised a new paradigm for studying auditory expectation without compromising ecological validity. A strong negative correlation was found between the probability of notes predicted by our model and the subjectively perceived degree of expectedness. Our electrophysiological results showed that low-probability notes, as compared to high-probability notes, elicited a larger (i) negative ERP component at a late time period (400–450 ms), (ii) beta band (14–30 Hz) oscillation over the parietal lobe, and (iii) long-range phase synchronization between multiple brain regions. Altogether, the study demonstrated that statistical learning produces information-theoretic descriptions of musical notes that are proportional to their perceived expectedness and are associated with characteristic patterns of neural activity. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  date = {2010-03},
  pages = {302-313},
  keywords = {statistical learning,Auditory Stimulation,music perception,Auditory Evoked Potentials,Expectations,Learning,behavioral manifestations,computational manifestations,Computational Neuroscience,musical expectation,musical stimulation,neural manifestations,Statistics},
  author = {Pearce, Marcus T. and Ruiz, María Herrojo and Kapasi, Selina and Wiggins, Geraint A. and Bhattacharya, Joydeep}
}

@article{pearceAuditoryExpectationInformation2012,
  title = {Auditory Expectation: {{The}} Information Dynamics of Music Perception and Cognition},
  volume = {4},
  issn = {1756-8757},
  doi = {10.1111/j.1756-8765.2012.01214.x},
  shorttitle = {Auditory Expectation},
  abstract = {Following in a psychological and musicological tradition beginning with Leonard Meyer, and continuing through David Huron, we present a functional, cognitive account of the phenomenon of expectation in music, grounded in computational, probabilistic modeling. We summarize a range of evidence for this approach, from psychology, neuroscience, musicology, linguistics, and creativity studies, and argue that simulating expectation is an important part of understanding a broad range of human faculties, in music and beyond. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Topics in Cognitive Science},
  shortjournal = {Topics in Cognitive Science},
  date = {2012-10},
  pages = {625-652},
  keywords = {cognition,Auditory Perception,Music,music perception,auditory expectation,information dynamics},
  author = {Pearce, Marcus T. and Wiggins, Geraint A.}
}

@incollection{aslinStatisticalLearningLinguistic1999,
  location = {{Mahwah, NJ, US}},
  title = {Statistical Learning in Linguistic and Nonlinguistic Domains},
  isbn = {0-8058-3010-3},
  abstract = {Discusses a series of studies of adults, children, and infants that show that sequential auditory patterns, composed of both speech and tonal elements presented in rapid succession, can be grouped solely on the basis of their distributional properties. The statistic that characterizes this grouping is the transitional probability between adjacent elements in the auditory stream. Such a statistic, or 1 essentially equivalent, is logically sufficient for at least certain parts of word segmentation and has been empirically verified as available even to 8-mo-olds. However, the segmentation of words from the speech stream is a rather rudimentary aspect of language structure. One challenge for the future is to explore other, more complex and less immediately sequential statistics that may characterize higher levels of linguistic structure. The authors also examine distributional, or statistical, learning at the level of phrase structure. Although it remains unclear how much of language acquisition can be accounted for by such sophisticated learning mechanisms, the authors believe that their work has drawn attention to the plausibility off considering statistical learning mechanisms as part of the battery of analytic abilities that infants bring to the problems of early development. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (chapter)},
  booktitle = {The Emergence of Language.},
  publisher = {{Lawrence Erlbaum Associates Publishers}},
  date = {1999},
  pages = {359-380},
  keywords = {Language,Learning,Statistics,Cognitive Development,Psycholinguistics,statistical learning of sequential auditory patterns & phrase structures},
  author = {Aslin, Richard N. and Saffran, Jenny R. and Newport, Elissa L.},
  editor = {MacWhinney, Brian and MacWhinney, Brian (Ed)}
}

@article{saffranStatisticalLearningTone1999,
  title = {Statistical Learning of Tone Sequences by Human Infants and Adults},
  volume = {70},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(98)00075-4},
  abstract = {Previous research suggests that language learners can detect and use the statistical properties of syllable sequences to discover words in continuous speech . In the present research, the authors asked whether this statistical learning ability is uniquely tied to linguistic materials. In three experiments, subjects were exposed to continuous non-linguistic auditory sequences whose elements were organized into "tone words." As in the authors' previous studies, statistical information was the only word boundary cue available to learners. Both adults (n\hspace{0.6em}=\hspace{0.6em}48) and infants (aged 7.1–8.2 yrs; n\hspace{0.6em}=\hspace{0.6em}24) succeeded at segmenting the tone stream, with performance indistinguishable from that obtained with syllable streams. These results suggest that a learning mechanism previously shown to be involved in word segmentation can also be used to segment sequences of non-linguistic stimuli. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  date = {1999-02},
  pages = {27-52},
  keywords = {Pitch (Frequency),Stimulus Parameters,adults & 7.1–8.2 yr olds,Language Development,Learning Ability,learning of tone sequences,Learning Strategies,statistical learning ability involved in word segmentation,Words (Phonetic Units)},
  author = {Saffran, Jenny R. and Johnson, Elizabeth K. and Aslin, Richard N. and Newport, Elissa L.}
}

@article{saffranStatisticalLearning8monthold1996,
  title = {Statistical Learning by 8-Month-Old Infants},
  volume = {274},
  issn = {0036-8075},
  doi = {10.1126/science.274.5294.1926},
  abstract = {Learners rely on a combination of experience-independent and experience-dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-mo-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 min of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {5294},
  journaltitle = {Science},
  shortjournal = {Science},
  date = {1996-12},
  pages = {1926-1928},
  keywords = {Auditory Stimulation,Infant Development,Language Development,Words (Phonetic Units),8 mo olds,segmentation of words from fluent speech based on statistical relationships between neighboring speech sounds,Speech Perception},
  author = {Saffran, Jenny R. and Aslin, Richard N. and Newport, Elissa L.}
}

@article{dowlingAimingAttentionPitch1987,
  title = {Aiming Attention in Pitch and Time in the Perception of Interleaved Melodies},
  volume = {41},
  issn = {0031-5117},
  doi = {10.3758/BF03210496},
  abstract = {Conducted 6 experiments in which hidden-melodies tasks were presented to groups of musically experienced or nonexperienced undergraduates. Results indicate that the Ss succeeded in following a melody interleaved at 6 or 8 notes/sec with distractor notes in the same pitch range and of the same timbre. Their ability to perform this auditory hidden-figures task depended on the rhythmic control of attention on the basis of culturally derived expectancies developed through perceptual learning with melodies. Ss appeared to have aimed expectancies in pitch and time at regions where events critical to the identification of melodies were likely to occur—regions defining expectancy windows through which target notes were perceived. Findings indicate that these windows were related to task demands and illustrate the active nature of auditory attention. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {6},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  series = {The Understanding of Melody and Rhythm},
  date = {1987-06},
  pages = {642-656},
  keywords = {Attention,Auditory Perception,Music,Pitch (Frequency),Experience Level,attention to & expectations of pitch & time,musically experienced vs nonexperienced college students,perception of interleaved melodies presented with distractor notes},
  author = {Dowling, W. Jay and Lung, Kitty M. and Herrbold, Susan}
}

@article{dowlingPerceptionInterleavedMelodies1973,
  title = {The Perception of Interleaved Melodies},
  volume = {5},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(73)90040-6},
  abstract = {Investigated perceptual grouping of successively-presented tones from 2 melodies and the effect of overlapping pitch ranges in 4 experiments. 6 undergraduates served as Ss in both Exp I and II, 12 in Exp III, and 21 in Exp IV. Exp I showed that identification of interleaved pairs of familiar melodies was possible if pitch ranges did not overlap, but difficult otherwise. A short-term recognition memory paradigm (Exp II) showed that interleaving a background melody with an unfamiliar melody interfered with same-different judgments regardless of the separation of pitch ranges, but that range separation attenuated the interference effect. When pitch ranges overlapped, Ss overcame the interference effect and recognized a familiar melody if it was prespecified (Exp III). Familiarity or prespecification of the interleaved background melody did not reduce interfering effects on same-different judgments of unfamiliar target melodies (Exp IV). (19 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {3},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  date = {1973-11},
  pages = {322-337},
  keywords = {Music,Pitch (Frequency),college students,pitch perception,overlapping pitch ranges,perceptual grouping of successively-presented tones from 2 melodies},
  author = {Dowling, W. J.}
}

@article{dowlingExpectancyAttentionMelody1990,
  title = {Expectancy and Attention in Melody Perception},
  volume = {9},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1991-32777-001&site=ehost-live},
  doi = {10.1037/h0094150},
  abstract = {Offers operational definitions to distinguish between attentional vs expectancy processes in melody identification. Attention is characterized as a process that selects stimulus elements for further processing, leading to the interpretation of whatever stimuli were picked up. That is, attended stimuli are heard as interpretable patterns or gestalts. In contrast, expectancy in the context of discerning melodies hidden among distractors leads to the isolation of a gestalt only when target elements match the expected pattern. Otherwise, the listener is not able to interpret the auditory pattern. These suggestions are applied to recent experiments (e.g., M. W. Andrews and W. J. Dowling; see record [rid]1991-30019-001[/rid]) that explored developmentally the consequences of violating expectancies at 2 levels: pitches within melodies, and pitches in the overall tonal scheme defined by a music key. (PsycINFO Database Record (c) 2014 APA, all rights reserved)},
  number = {2},
  journaltitle = {Psychomusicology: A Journal of Research in Music Cognition},
  shortjournal = {Psychomusicology: A Journal of Research in Music Cognition},
  series = {Music {{Expectancy}}},
  urldate = {2015-06-16},
  year = {Fal  1990},
  pages = {148-160},
  keywords = {Attention,Music,Auditory Discrimination,Cognitive Processes,Expectations,5–10 yr olds vs adults,attentional vs expectancy processes in melody identification},
  author = {Dowling, W. Jay},
  file = {D\:\\Sauve\\Zotero\\storage\\J9QWNE43\\Dowling - 1990 - Expectancy and attention in melody perception.pdf}
}

@article{andrewsDevelopmentPerceptionInterleaved1991,
  title = {The Development of Perception of Interleaved Melodies and Control of Auditory Attention},
  volume = {8},
  issn = {0730-7829},
  abstract = {158 children (aged 5–10 yrs) and 53 undergraduates were asked to discern familiar target melodies whose notes were temporally interleaved with distractor notes. Targets varied in perceptual salience: The most hidden targets were interleaved with distractors of the same pitch range, loudness, and timbre, whereas the most salient targets differed in those dimensions from their distractors. Targets either retained their familiar "straight" form or wandered in pitch. Performance improved with age and experience, and was better with salient and straight targets. All but the 5- and 6-yr-olds found salient targets easier with tonal distractors and hidden targets easier with atonal distractors. Only the youngest children found same-timbre distractors outside the pitch range of the target as disruptive as same-timbre distractors within that range. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {4},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Sum  1991},
  pages = {349-368},
  keywords = {Attention,Auditory Perception,Music,Pitch (Frequency),Auditory Stimulation,Age Differences,Experience Level,5–10 yr olds vs adults,age & music experience & type & perceptual salience of target melodies with tonal vs atonal distractors,identification of melody & auditory attention & expectancy,Perceptual Development},
  author = {Andrews, Melinda W. and Dowling, W. Jay}
}

@article{carlyonEffectsAttentionUnilateral2001,
  title = {Effects of Attention and Unilateral Neglect on Auditory Stream Segregation},
  volume = {27},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2001-16068-008&site=ehost-live},
  doi = {10.1037/0096-1523.27.1.115},
  abstract = {Two pairs of experiments studied the effects of attention and of unilateral neglect on auditory streaming. The first pair showed that the build up of auditory streaming in normal participants is greatly reduced or absent when they attend to a competing task in the contralateral ear. It was concluded that the effective build up of streaming depends on attention. The second pair showed that patients with an attentional deficit toward the left side of space (unilateral neglect) show less stream segregation of tone sequences presented to their left than to their right ears. Streaming in their right ears was similar to that for stimuli presented to either ear of healthy and of brain-damaged controls, who showed no across-ear asymmetry. This result is consistent with an effect of attention on streaming, constrains the neural sites involved, and reveals a qualitative difference between the perception of left- and right-sided sounds by neglect patients. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-16},
  date = {2001-02},
  pages = {115-127},
  keywords = {Attention,auditory stream segregation,Auditory Discrimination,humans,attention & unilateral neglect,Sensory Neglect},
  author = {Carlyon, Robert P. and Cusack, Rhodri and Foxton, Jessica M. and Robertson, Ian H.},
  file = {D\:\\Sauve\\Zotero\\storage\\I7RFXCN7\\Carlyon et al. - 2001 - Effects of attention and unilateral neglect on aud.pdf}
}

@article{cusackEffectsLocationFrequency2004,
  title = {Effects of {{Location}}, {{Frequency Region}}, and {{Time Course}} of {{Selective Attention}} on {{Auditory Scene Analysis}}},
  volume = {30},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2004-16645-001&site=ehost-live},
  doi = {10.1037/0096-1523.30.4.643},
  abstract = {Often, the sound arriving at the ears is a mixture from many different sources, but only 1 is of interest. To assist with selection, the auditory system structures the incoming input into streams, each of which ideally corresponds to a single source. Some authors have argued that this process of streaming is automatic and invariant, but recent evidence suggests it is affected by attention. In Experiments 1 and 2, it is shown that the effect of attention is not a general suppression of streaming on an unattended side of the ascending auditory pathway or in unattended frequency regions. Experiments 3 and 4 investigate the effect on streaming of physical gaps in the sequence and of brief switches in attention away from a sequence. The results demonstrate that after even short gaps or brief switches in attention, streaming is reset. The implications are discussed, and a hierarchical decomposition model is proposed. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-16},
  date = {2004-08},
  pages = {643-656},
  keywords = {Auditory Perception,Pitch (Frequency),Auditory Stimulation,streaming,auditory scene analysis,selective attention,auditory system,Time,frequency region,location,time course},
  author = {Cusack, Rhodri and Decks, John and Aikman, Genevieve and Carlyon, Robert P.},
  file = {D\:\\Sauve\\Zotero\\storage\\MZRQ922S\\Cusack et al. - 2004 - Effects of Location, Frequency Region, and Time Co.pdf}
}

@article{cusackPerceptualAsymmetriesAudition2003,
  title = {Perceptual {{Asymmetries}} in {{Audition}}},
  volume = {29},
  issn = {00961523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=10110558&site=ehost-live},
  abstract = {Visual feature extraction has been investigated using search experiments. Targets that contain a feature not present in the distractors are easier to detect than if they do not, leading to search asymmetries. If sounds are decomposed into features in the auditory system, there might be asymmetries in analogous tasks. Six experiments investigating this are described. Strong asymmetries were identified, with frequency-modulated targets easier to detect among pure-tone distractors than vice versa and longer sounds easier to select from short distractors than the reverse. It is demonstrated that this asymmetry is not a result of peripheral limitations. In contrast, no asymmetries were observed between high- and low-frequency tones or between short 3-tone sequences differing only in their temporal structure. The results are discussed with reference to models of perceptual grouping and attention, the applicability of analogies between vision and audition, and possible physiological correlates. The paradigm provides a new way in which to investigate auditory feature extraction.},
  number = {3},
  journaltitle = {Journal of Experimental Psychology. Human Perception \& Performance},
  shortjournal = {Journal of Experimental Psychology. Human Perception \& Performance},
  urldate = {2015-06-16},
  date = {2003-06},
  pages = {713},
  keywords = {Auditory Perception,VISUAL perception},
  author = {Cusack, Rhodri and Carlyon, Robert P.},
  file = {D\:\\Sauve\\Zotero\\storage\\UZ6Q7769\\Cusack and Carlyon - 2003 - Perceptual Asymmetries in Audition.pdf}
}

@article{cusackNeglectNotAuditory2000,
  title = {Neglect between but Not within Auditory Objects},
  volume = {12},
  issn = {0898-929X},
  doi = {10.1162/089892900563867},
  abstract = {Examined how well listeners can discriminate the spatial location of sounds. 12 Ss (aged 45–75 yrs old) participated in 3 experiments. In Exp 1a, Ss heard a sound and indicated its lateralization (left or right). In Exp 1b, identical sounds were used as in Exp 1a, but 3 were presented. Two sounds were presented at 1 location and 1 (the 2nd or the 3rd) on the opposite side. The S had to identify whether the 2nd or 3rd sound was the "odd man out". In Exp 2a, 1 sound was presented that was either a pure tone or changed in frequency in a sinusoidal manner. Ss were asked for a response of either "steady" or "warble". In Exp 2b, on each trial, Ss were presented with 2 pure tones. The tones had a fixed frequency difference, with either the 1st or 2nd randomly chosen to be the higher tone. Ss were asked which tone was higher. Exp 3 comprised 2 conditions involving the detection of changes in frequency within a sound. Results indicate that unilateral neglect, exclusively thought of as a disorder of visuospatial processing, involves a specific deficit in allocating attention between auditory objects separated only in time but not space. This deficit is restricted to comparisons between sounds: the patients' ability to make within-sound comparisons was similar to that of controls. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {6},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2000-11},
  pages = {1056-1065},
  keywords = {Attention,Auditory Perception,Auditory Localization,Time,Sensory Neglect,45–75 yr olds with unilateral neglect,Brain Damage,Brain Disorders,deficits in allocating attention between auditory objects in time or space},
  author = {Cusack, Rhodri and Carlyon, Robert P. and Robertson, Ian H.}
}

@article{thompsonObjectiveMeasurementBuildup2011,
  title = {An Objective Measurement of the Build-up of Auditory Streaming and of Its Modulation by Attention},
  volume = {37},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2011-07485-001&site=ehost-live},
  doi = {10.1037/a0021925},
  abstract = {Three experiments studied auditory streaming using sequences of alternating “ABA” triplets, where “A” and “B” were 50-ms tones differing in frequency by Δf semitones and separated by 75-ms gaps. Experiment 1 showed that detection of a short increase in the gap between a B tone and the preceding A tone, imposed on one ABA triplet, was better when the delay occurred early versus late in the sequence, and for Δf = 4 vs. Δf = 8. The results of this experiment were consistent with those of a subjective streaming judgment task. Experiment 2 showed that the detection of a delay 12.5 s into a 13.5-s sequence could be improved by requiring participants to perform a task on competing stimuli presented to the other ear for the first 10 s of that sequence. Hence, adding an additional task demand could improve performance via its effect on the perceptual organization of a sound sequence. The results demonstrate that attention affects streaming in an objective task and that the effects of build-up are not completely under voluntary control. In particular, even though build-up can impair performance in an objective task, participants are unable to prevent this from happening. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-16},
  date = {2011-08},
  pages = {1253-1262},
  keywords = {Attention,Auditory Perception,auditory streaming,auditory scene analysis,build-up},
  author = {Thompson, Sarah K. and Carlyon, Robert P. and Cusack, Rhodri},
  file = {D\:\\Sauve\\Zotero\\storage\\DDBVG2UF\\Thompson et al. - 2011 - An objective measurement of the build-up of audito.pdf}
}

@article{carlyonCrossmodalNonsensoryInfluences2003,
  title = {Cross-Modal and Non-Sensory Influences on Auditory Streaming},
  volume = {32},
  issn = {0301-0066},
  doi = {10.1068/p5035},
  abstract = {Carlyon et al (2001 Journal of Experimental Psychology: Human Perception and Performance 27 115-127) have reported that the buildup of auditory streaming is reduced when attention is diverted to a competing auditory stimulus. Here, we demonstrate that a reduction in streaming can also be obtained by attention to a visual task or by the requirement to count backwards in threes. In all conditions participants heard a 13 s sequence of tones, and, during the first 10 s saw a sequence of visual stimuli containing three, four, or five targets. The tone sequence consisted of twenty repeating triplets in an ABA-ABA ... order, where A and B represent tones of two different frequencies. In each sequence, three, four, or five tones were amplitude modulated. During the first 10 s of the sequence, participants either counted the number of visual targets, counted the number of (modulated) auditory targets, or counted backwards in threes from a specified number. They then made an auditory-streaming judgment about the last 3 s of the tone sequence: whether one or two streams were heard. The results showed more streaming when participants counted the auditory targets (and hence were attending to the tones throughout) than in either the 'visual' or 'counting-backwards' conditions. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {11},
  journaltitle = {Perception},
  shortjournal = {Perception},
  date = {2003},
  pages = {1393-1402},
  keywords = {Attention,Auditory Perception,Auditory Stimulation,auditory streaming,Cognitive Processes,Visual Attention,competing stimuli,counting,diverted attention,Divided Attention,streaming reduction,visual task attention},
  author = {Carlyon, Robert P. and Plack, Christopher J. and Fantini, Deborah A. and Cusack, Rhodri}
}

@article{cusackAuditoryMidlineSpatial2001,
  title = {Auditory Midline and Spatial Discrimination in Patients with Unilateral Neglect},
  volume = {37},
  issn = {0010-9452},
  doi = {10.1016/S0010-9452(08)70620-8},
  abstract = {Investigated auditory midline shift, hearing thresholds of each ear, and spatial discrimination performance in 10 right-brain-damaged patients (mean age 66 yrs) exhibiting unilateral neglect. The results of the auditory midline tracking task did not show a systematic shift of midline. Five of the patients showed midlines within the normal range. Patient discrimination thresholds were also found to be in the normal range. Furthermore, no evidence was found of worse discrimination performance on the left, which could otherwise have masked an underlying shift to the right. Results are consistent with previous research concluding that a shift in reference frame does not always accompany neglect. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {5},
  journaltitle = {Cortex: A Journal Devoted to the Study of the Nervous System and Behavior},
  shortjournal = {Cortex: A Journal Devoted to the Study of the Nervous System and Behavior},
  date = {2001-12},
  pages = {706-709},
  keywords = {Auditory Discrimination,Spatial Perception,Sensory Neglect,Brain Damage,auditory midline shift,Auditory Thresholds,hearing thresholds,right brain damage,spatial discrimination performance,unilateral neglect},
  author = {Cusack, Rhodri and Carlyon, Robert P. and Robertson, Ian H.}
}

@thesis{vannoordenTemporalCoherencePerception1975,
  location = {{Eindhoven}},
  title = {Temporal {{Coherence}} in the {{Perception}} of {{Tone Sequences}}},
  institution = {{Technical University Eindhoven}},
  date = {1975},
  keywords = {fission boundary,temporal coherence boundary},
  author = {van Noorden, Leon},
  options = {useprefix=true}
}

@article{vliegenRoleSpectralPeriodicity1999,
  title = {The Role of Spectral and Periodicity Cues in Auditory Stream Segregation, Measured Using a Temporal Discrimination Task},
  volume = {106},
  issn = {0001-4966},
  doi = {10.1121/1.427140},
  abstract = {Investigated if temporal discrimination for sequences of complex tones is adversely affected by a large f0 difference between the tones, when no detectable spectral differences are present. It was hypothesized that if f0 differences produce an obligatory or involuntary segregation of successive tones when the differences are sufficiently large, then temporal discrimination should be impaired by a large f0 difference in the tones whose timing is to be compared: pure tones; complex tones filtered into a single frequency region, differing only in f0; and complex tones with a fixed f0 but with spectral differences between successive tones. Listeners (aged 18–30 yrs) were divided into whether they were musical (had at least 4 yrs of formal musical training) or not. The results showed a clear effect of the frequency interval between the tones of a sequence on temporal-shift discrimination. The effect was larger for the conditions where spectral information was available, but it was statistically significant for all 3 conditions. Overall, the results suggest that spectral information is dominant in inducing (involuntary) segregation, but periodicity information can play a role. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {2},
  journaltitle = {Journal of the Acoustical Society of America},
  shortjournal = {Journal of the Acoustical Society of America},
  date = {1999-08},
  pages = {938-945},
  keywords = {Music,Pitch (Frequency),Auditory Discrimination,fundamental frequency differences between tones,temporal discrimination for complex tone sequences,Temporal Frequency},
  author = {Vliegen, Joyce and Moore, Brian C. J. and Oxenham, Andrew J.}
}

@article{mcdermottRecoveringSoundSources2011,
  title = {Recovering Sound Sources from Embedded Repetition},
  volume = {108},
  issn = {0027-8424},
  doi = {10.1073/pnas.1004765108},
  abstract = {Cocktail parties and other natural auditory environments present organisms with mixtures of sounds. Segregating individual sound sources is thought to require prior knowledge of source properties, yet these presumably cannot be learned unless the sources are segregated first. Here we show that the auditory system can bootstrap its way around this problem by identifying sound sources as repeating patterns embedded in the acoustic input. Due to the presence of competing sounds, source repetition is not explicit in the input to the ear, but it produces temporal regularities that listeners detect and use for segregation. We used a simple generative model to synthesize novel sounds with naturalistic properties. We found that such sounds could be segregated and identified if they occurred more than once across different mixtures, even when the same sounds were impossible to segregate in single mixtures. Sensitivity to the repetition of sound sources can permit their recovery in the absence of other segregation cues or prior knowledge of sounds, and could help solve the cocktail party problem. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  date = {2011-01-18},
  pages = {1188-1193},
  keywords = {Auditory Stimulation,acoustics,auditory system,ear,Ear (Anatomy),Sensitization,sound segregation,sound sensitivity,sound sources,Source Monitoring},
  author = {McDermott, Josh H. and Wrobleski, David and Oxenham, Andrew J.}
}

@article{mcdermottMusicPerceptionPitch2008,
  title = {Music Perception, Pitch, and the Auditory System},
  volume = {18},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2008.09.005},
  abstract = {The perception of music depends on many culture-specific factors, but is also constrained by properties of the auditory system. This has been best characterized for those aspects of music that involve pitch. Pitch sequences are heard in terms of relative as well as absolute pitch. Pitch combinations give rise to emergent properties not present in the component notes. In this review we discuss the basic auditory mechanisms contributing to these and other perceptual effects in music. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Current Opinion in Neurobiology},
  shortjournal = {Current Opinion in Neurobiology},
  date = {2008-08},
  pages = {452-463},
  keywords = {Pitch (Frequency),music perception,Auditory Cortex,auditory system,perceptual effects,pitch},
  author = {McDermott, Josh H. and Oxenham, Andrew J.}
}

@article{oxenhamMechanismsMechanicsAuditory2013,
  title = {Mechanisms and Mechanics of Auditory Masking},
  volume = {591},
  issn = {0022-3751},
  abstract = {This article provides an overview on mechanisms and mechanics of auditory masking. The articles traces the origins of masking down to the first stages of electro mechanical processing in the cochlea. The auditory system must then decompose the patterns of vibrations at the eardrums to provide us with information about the individual sound sources, such as their identity and location. Masking tends to occur when the tonotopic representation of one sound interferes with that of another. In this issue, Recio-Spinosa and Cooper (see record [rid]2013-17167-025[/rid]) show that many of the non-linear neural phenomena produced by the interaction of tones and noise, such as frequency-specific suppression of tones by noise and vice versa, have their origins in the mechanical response of the basilar membrane. Important remaining questions include the extent to which cochlear responses to complex sounds are altered by efferent control activated by prior sounds, or 'top-down' effects such as attention in awake, behaving animals, and the still controversial issue of how similar human cochlear tuning is to that of typical laboratory animals. (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  number = {10},
  journaltitle = {The Journal of Physiology},
  shortjournal = {The Journal of Physiology},
  date = {2013-05-15},
  pages = {2375-2375},
  keywords = {Cochlea,animals,Neurons,auditory masking,cochlear responses,neural phenomena,Responses},
  author = {Oxenham, Andrew J.}
}

@article{micheylPitchHarmonicityConcurrent2010,
  title = {Pitch, Harmonicity and Concurrent Sound Segregation: {{Psychoacoustical}} and Neurophysiological Findings},
  volume = {266},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2009.09.012},
  shorttitle = {Pitch, Harmonicity and Concurrent Sound Segregation},
  abstract = {Harmonic complex tones are a particularly important class of sounds found in both speech and music. Although these sounds contain multiple frequency components, they are usually perceived as a coherent whole, with a pitch corresponding to the fundamental frequency (F0). However, when two or more harmonic sounds occur concurrently, e.g., at a cocktail party or in a symphony, the auditory system must separate harmonics and assign them to their respective F0s so that a coherent and veridical representation of the different sounds sources is formed. Here we review both psychophysical and neurophysiological (single-unit and evoked-potential) findings, which provide some insight into how, and how well, the auditory system accomplishes this task. A survey of computational models designed to estimate multiple F0s and segregate concurrent sources is followed by a review of the empirical literature on the perception and neural coding of concurrent harmonic sounds, including vowels, as well as findings obtained using single complex tones with mistuned harmonics. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1-2},
  journaltitle = {Hearing Research},
  shortjournal = {Hearing Research},
  date = {2010-07},
  pages = {36-51},
  keywords = {Auditory Localization,acoustics,pitch perception,sound segregation,harmonicity,psychophysiology},
  author = {Micheyl, Christophe and Oxenham, Andrew J.}
}

@article{parncuttToneProfilesFollowing2000,
  title = {Tone Profiles Following Short Chord Progessions: {{Top}}-down or Bottom-Up?},
  volume = {18},
  issn = {0730-7829},
  shorttitle = {Tone Profiles Following Short Chord Progessions},
  abstract = {Examined the relationship between chroma-salience profiles of individual musical chords and tone profiles obtained after short chord progressions. In Exp 1, 15 musicians and 15 nonmusicians (mean ages 22 and 28 yrs) listened to pairs of chords and rated degree of similarity. In Exps 2 and 3, Ss rated the similarity of a probe tone to 2 or 3 chords. Responses were compared with predictions of the 3 models of: (1) a bottom-up stimulus model comprising the number of times each chroma occurred within the progression; (2) a top-down key model comprising a best-fitting key profile; and (3) an intermediate pitch model including both top-down and bottom-up components. Results show that for single chords, all predictors significantly matched tone profiles, except that the top-down model applied to the diminished triad. For pairs of chords, the pitch and key models consistently outperformed the stimulus model. The pitch and key models were successful for all progressions. The prediction ability of the pitch and key models were greater than for the stimulus model. Both primacy and recency effects were observed. Findings suggest that the pitch model was the most consistently successful model overall. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Fal  2000},
  pages = {25-57},
  keywords = {Musicians,Pitch Discrimination,implications of applicability of bottom-up stimulus vs top-down key vs intermediate pitch models,Models,musical chords chroma-salience vs tone profiles,musicians vs nonmusicians (mean ages 22 & 28 yrs)},
  author = {Parncutt, Richard and Bregman, Albert S.}
}

@article{dannenbringStreamSegregationIllusion1976,
  title = {Stream Segregation and the Illusion of Overlap},
  volume = {2},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1977-02337-001&site=ehost-live},
  doi = {10.1037/0096-1523.2.4.544},
  abstract = {When 2 different sounds continuously alternate at high speed, they segregate into 2 perceptual streams. Six experiments are reported which show that this segregation produced a loss of information regarding the sequential relations of the sounds so that they seemed perceptually to be overlapped in time. The segregation (and its contribution to perceived overlap) is shown to increase with the perceptual difference between the sounds; Ss were 63 graduate and undergraduate students. The mapping from perceptual difference to perceived overlap is not simple, however, since perceived overlap can also be affected by "perceived auditory continuity," another perceptual effect that responds differently to the perceptual difference between the 2 sounds. (15 ref) (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-16},
  date = {1976-11},
  pages = {544-555},
  keywords = {Auditory Perception,Auditory Stimulation,college students,Illusions (Perception),perceived overlap & perceived auditory continuity,segregation of alternating sounds at high speed into perceptual streams & sound differences},
  author = {Dannenbring, Gary L. and Bregman, Albert S.},
  file = {D\:\\Sauve\\Zotero\\storage\\633HPXJJ\\Dannenbring and Bregman - 1976 - Stream segregation and the illusion of overlap.pdf}
}

@article{french-st.georgeRolePredictabilitySequence1989,
  title = {Role of Predictability of Sequence in Auditory Stream Segregation},
  volume = {46},
  issn = {0031-5117},
  doi = {10.3758/BF03204992},
  abstract = {Evaluated the role of item predictability in auditory temporal coherence. 13 normal-hearing Ss were required to hold together long tonal sequences as single strings of notes. Temporal and spectral predictability of successive notes in a sequence varied as a function of experimental condition. As the frequency separation of the notes in the sequence increased, Ss found it more difficult to hold the sequence together as a single stream. Results indicate that the predictability of successive notes did not have a role in temporal coherence. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {4},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1989-10},
  pages = {384-386},
  keywords = {Prediction,Pitch (Frequency),adults,Memory,Temporal Frequency,auditory temporal coherence,temporal & spectral predictability & frequency separation of successive notes},
  author = {French-St. George, Marilyn and Bregman, Albert S.}
}

@article{bregmanAuditoryStreamingBuilding1978,
  title = {Auditory Streaming and the Building of Timbre},
  volume = {32},
  issn = {0008-4255},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1979-22752-001&site=ehost-live},
  doi = {10.1037/h0081664},
  abstract = {In a natural environment, the auditory system must analyze an incoming wave train to determine 2 things: (a) which series of frequency components arose over time from the same source and should be integrated into a sequential stream, and (b) which set of simultaneous components arose from one source and should be fused into a timbre structure. In 4 experiments with a total of 90 undergraduates and graduate students and researchers, each S judged the stream organization and the timbre of a repeating cycle formed by a pair of more or less synchronous pure tones, B and C, and a preceding pure tone, A, whose frequency was varied in its proximity to that of the upper tone of the BC pair. These experiments demonstrated that fusion and sequential organization of streams are carried out using 2 sorts of information that compete to determine the best perceptual description of the input. Proximal frequencies between sequential components promote a sequential organization, and the simultaneity of onset of frequency components promotes perceptual fusion. (French summary) (17 ref) (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  number = {1},
  journaltitle = {Canadian Journal of Psychology/Revue canadienne de psychologie},
  shortjournal = {Canadian Journal of Psychology/Revue canadienne de psychologie},
  urldate = {2015-06-16},
  date = {1978-03},
  pages = {19-31},
  keywords = {Pitch Discrimination,college & graduate students & researchers,Pattern Discrimination,perception of wave stream organization & timbre,preceding vs synchronous pure tone stimuli},
  author = {Bregman, Albert S. and Pinker, Steven},
  file = {D\:\\Sauve\\Zotero\\storage\\I58C3P9Z\\Bregman and Pinker - 1978 - Auditory streaming and the building of timbre.pdf}
}

@article{parncuttToneProfilesFollowing2000a,
  eprinttype = {jstor},
  eprint = {40285900},
  title = {Tone {{Profiles}} Following {{Short Chord Progressions}}: {{Top}}-down or {{Bottom}}-Up?},
  volume = {18},
  issn = {0730-7829},
  doi = {10.2307/40285900},
  shorttitle = {Tone {{Profiles}} Following {{Short Chord Progressions}}},
  abstract = {Three experiments explored the relationship between chroma-salience profiles of individual chords and tone profiles obtained after short chord progressions. Musicians' tone profiles for diatonic progressions of one, two, and three chords were compared with predictions of three models: a bottom- up stimulus model (number of times each chroma occurs in the progression), a top-down or schema-driven key model (best-fitting key profile of C. L. Krumhansl \& E. J. Kessler, 1982), and an intermediate pitch model that includes both top-down and bottom-up components (cumulative pitch salience; R. Parncutt, 1989, 1993). For single chords, all predictors significantly matched tone profiles, except the key model applied to the diminished triad. For pairs of chords, the pitch and key models consistently outperformed the stimulus model, consistent with the assumption that a (top- down) key had been established; in the pitch model, the second chord influenced the tone profile more than the first (recency effect). Progressions of three chords comprised forward (e. g., FG-C) and backward (C-G-F) cadences in major and minor keys. The pitch and key models were successful for all progressions, but the key model predicted the tonic of backward cadences in C major and minor to be F. Predictions of the stimulus model were clearly worse than those of the other models, especially for backward cadences. Both primacy and recency effects were observed. In summary, the pitch model was the most consistently successful model over all experiments. To successfully predict tone profiles following chord progressions, it was necessary to account not only for recency (and primacy) but also for variations in pitch salience within chords. Results are consistent with a model of tonality induction in which bottom-up processes interact in real time with top-down processes of two kinds: recognition of harmonic pitch patterns and recognition of key profiles.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2000-10-01},
  pages = {25-57},
  author = {Parncutt, Richard and Bregman, Albert S.},
  file = {D\:\\Sauve\\Zotero\\storage\\G8XR3IWH\\Parncutt and Bregman - 2000 - Tone Profiles following Short Chord Progressions .pdf}
}

@article{eerolaStatisticalFeaturesPerceived2001,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2001.18.3.275},
  title = {Statistical {{Features}} and {{Perceived Similarity}} of {{Folk Melodies}}},
  volume = {18},
  issn = {0730-7829},
  doi = {10.1525/mp.2001.18.3.275},
  abstract = {Listeners are sensitive to pitch distributional information in music (N. Oram \& L. L. Cuddy, 1995; C. L. Krumhansl, J. Louhivuori, P.Toiviainen, T. Järvinen, \& T. Eerola, 1999). However, it is uncertain whether frequency-based musical features are sufficient to explain the similarity judgments that underlie listeners' classification processes. A similarity rating experiment was designed to determine the effectiveness of these features in predicting listeners' similarity ratings. The material consisted of 15 melodies representing five folk music styles. A multiple regression analysis showed that the similarity of frequency-based musical properties could account for a moderate amount (40\%) of listeners' similarity ratings. A slightly better predictive rate (55\%) was achieved by using descriptive variables such as number of tones, rhythmic variability, and melodic predictability. The results suggest that both measures were able to capture some aspects of the structures that portray common salient dimensions to which listeners pay attention while categorizing melodies. Aikaisemmissa tutkimuksissa on osoitettu, että musiikin tilastollisilla tapahtumilla, kuten sävelten määrillä ja tyypillisillä intervalleilla, on merkitystä, kun kuulijat muodostavat käsityksiään musiikin rakenteesta (N. Oram \& L. L. Cuddy, 1995; C. L. Krumhansl, J. Louhivuori, P. Toiviainen, T. Järvinen, \& T. Eerola, 1999). Näiden piirteiden voidaan olettaa olevan tärkeitä myös musiikin luokittelussa. Toistaiseksi ei kuitenkaan tiedetä, miten hyvin tilastollisilla piirteillä voitaisiin musiikin luokittelua selittää. Tätä testattiin kuulijoille järjestetyn samanlaisuusarviointitehtävän avulla. Tutkimuksen materiaali koostui 15 melodiasta, jotka edustivat viittä eri kansanmusiikkityyliä. Regressioanalyysi paljasti, että musiikin tilastollisten piirteiden samanlaisuus pystyi selittämään kohtuullisen määrän (40\%) kuulijoiden antamista samanlaisuusarvioista. Hieman parempi selitysaste (55\%) saavutettiin kuvaavilla muuttujilla, joita olivat melodian laajuus ja ennakoitavuus sekä rytmin vaihtelevuus. Näin ollen tulokset antavat aiheen olettaa, että musiikin tilastolliset piirteet ja kuvailevat muuttujat vaikuttavat kuulijoiden luokittelupäätöksiin.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2001-03-01},
  pages = {275-296},
  author = {Eerola, Tuomas and Järvinen, Topi and Louhivuori, Jukka and Toiviainen, Petri},
  file = {D\:\\Sauve\\Zotero\\storage\\G86KG86S\\Eerola et al. - 2001 - Statistical Features and Perceived Similarity of F.pdf}
}

@article{povelPerceptualMechanismsMusic2001,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2001.19.2.169},
  title = {Perceptual {{Mechanisms}} in {{Music Processing}}},
  volume = {19},
  issn = {0730-7829},
  doi = {10.1525/mp.2001.19.2.169},
  abstract = {To gain a better understanding of the processes by which human listeners construct musical percepts within the Western tonal system, we conducted two experiments in which the perception of brief tone series was studied. The tone series consisted of (fragments from) different orderings of the collection C4 E4 F\#4 G4 Bþ4 and were preceded by two chords to induce a key. Two different tasks were used: (1) rating the melodic "goodness" of the tone series and (2) playing a few tones that complete the tone series. In Experiment 1, tone series of different lengths were presented in blocks. In Experiments 2a and 2b, increasing fragments of tone series were presented to examine the development of musical percepts. The majority of the data can be explained by two perceptual mechanisms: chord recognition and anchoring. Chord recognition is the mechanism that describes a series of tones in terms of a chord, a mental unit stored in long-term memory. Anchoring is the mechanism by which a tone is linked to a tone occurring later in the series. The paradigm appears to be a powerful tool for tracing perceptual mechanisms at work in the on-line processing of music.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2001-12-01},
  pages = {169-197},
  author = {Povel, Dirk-Jan and Jansen, Erik},
  file = {D\:\\Sauve\\Zotero\\storage\\NU3JA5MA\\Povel and Jansen - 2001 - Perceptual Mechanisms in Music Processing.pdf}
}

@article{londonCognitiveConstraintsMetric2002,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2002.19.4.529},
  title = {Cognitive {{Constraints}} on {{Metric Systems}}: {{Some Observations}} and {{Hypotheses}}},
  volume = {19},
  issn = {0730-7829},
  doi = {10.1525/mp.2002.19.4.529},
  shorttitle = {Cognitive {{Constraints}} on {{Metric Systems}}},
  abstract = {This paper is a music-theoretic discussion of various studies on rhythmic perception and performance and their ramifications for discussions of musical meter. Meter is defined as a stable and recurring pattern of hierarchically structured temporal expectations. Metrical patterns, although related to the pattern of interonset intervals present in the musical surface, are distinct from that pattern. Studies of subjective rhythmization, spontaneous tempo, pulse perception, durational discrimination, and so forth are discussed with respect to their implications for meter. Not only do there seem to be upper and lower bounds for musical meter (from ≅100 ms to ≅6 s, depending on context), but there also appear to be important thresholds within this range (around 200–250 ms, 500–700 ms, and 1.5–2.0 s). Interactions between beats (i.e., interonset intervals between expectancies occurring at the rate perceived as the tactus), beat subdivision, and changes in tempo are discussed, and it is hypothesized that beat perception may require (at least potentially) the perception of a concomitant level of subdivision. The interactions between beat interonset interval, subdivision interonset interval, and various thresholds may also explain (in part) some of the differences in the expressive and/or motional character of rhythmic figures (duplets versus triplets) at different tempos. Last, a broader discussion of systematic relationships in larger metrical systems with respect to tempo is given. It is shown that the choice of tempo systematically constrains the number and kind of metric patterns that are available to the listener.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2002-06-01},
  pages = {529-550},
  author = {London, Justin},
  file = {D\:\\Sauve\\Zotero\\storage\\W6M32J5Z\\London - 2002 - Cognitive Constraints on Metric Systems Some Obse.pdf}
}

@article{granotReMiFa2002,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2002.19.4.487},
  title = {Do {{Re Mi Fa Sol La Ti}}—{{Constraints}}, {{Congruity}}, and {{Musical Training}}: {{An Event}}-{{Related Brain Potentials Study}} of {{Musical Expectancies}}},
  volume = {19},
  issn = {0730-7829},
  doi = {10.1525/mp.2002.19.4.487},
  shorttitle = {Do {{Re Mi Fa Sol La Ti}}—{{Constraints}}, {{Congruity}}, and {{Musical Training}}},
  abstract = {In this study, event-related brain potentials (ERPs) are used to measure subjects' responses to violations of musical expectancies elicited in seventone sequences. We manipulated the structure of the sequence (strongly vs. weakly constraining), the congruity of the terminal tone (congruent vs. incongruent), the form of sequence presentation (both types intermixed vs. blocked), and subjects' musical training. The degree of expectancy violation was measured by the P300 component of the ERP known to be sensitive to subject's expectations. As predicted, musically trained subjects generate specific expectancies in strongly constraining sequences. Expectancies may also be generated in weakly constraining sequences, however, only when these are intermixed among strongly constraining sequences. Otherwise, musically trained subjects refrain from expectancy generation and rely on their knowledge of the structure of musical scales. Musically naive subjects respond, overall, slower and less accurately than trained subjects but show similar overt behavioral effects, suggesting that these differences reflect only varying degrees of processing efficacy. ERP data, on the other hand, suggest that processing differs significantly between musically trained and untrained subjects. One such difference is that musically naive subjects generate expectancies regardless of the level of constraints in the sequence.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2002-06-01},
  pages = {487-528},
  author = {Granot, Roni and Donchin, Emanuel},
  file = {D\:\\Sauve\\Zotero\\storage\\JCT8J2Q7\\Granot and Donchin - 2002 - Do Re Mi Fa Sol La Ti—Constraints, Congruity, and .pdf}
}

@article{povelHarmonicFactorsPerception2002,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2002.20.1.51},
  title = {Harmonic {{Factors}} in the {{Perception}} of {{Tonal Melodies}}},
  volume = {20},
  issn = {0730-7829},
  doi = {10.1525/mp.2002.20.1.51},
  abstract = {By common assumption, the first step in processing a tonal melody consists in setting up the appropriate metrical and harmonic frames required for the mental representation of the sequence of tones. Focusing on the generation of a harmonic frame, this study aims (a) to discover the factors that facilitate or interfere with the development of a harmonic interpretation, and (b) to test the hypothesis that goodness ratings of tone sequences largely depend on whether the listener succeeds in creating a suitable harmonic interpretation. In two experiments, listeners rated the melodic goodness of selected sequences of 10 and 13 tones and indicated which individual tones seemed not to fit. Results indicate that goodness ratings (a) are higher the more common the induced harmonic progression, (b) are strongly affected by the occurrence and position of nonchord tones: sequences without nonchord tones were rated highest, sequences with anchoring nonchord tones intermediately, and nonanchoring nonchord tones lowest. The explanation offered is compared with predictions derived from other theories, which leads to the conclusion that when a tone sequence is perceived as a melody, it is represented in terms of its underlying harmony, in which exact pitch-height characteristics play a minor role.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2002-09-01},
  pages = {51-85},
  author = {Povel, Dirk-Jan and Jansen, Erik},
  file = {D\:\\Sauve\\Zotero\\storage\\9VVJDB9M\\Povel and Jansen - 2002 - Harmonic Factors in the Perception of Tonal Melodi.pdf}
}

@article{temperleyParallelismFactorMetrical2002,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2002.20.2.117},
  title = {Parallelism as a {{Factor}} in {{Metrical Analysis}}},
  volume = {20},
  issn = {0730-7829},
  doi = {10.1525/mp.2002.20.2.117},
  abstract = {A model is proposed of the effect of parallelism on meter. It is wellknown that repeated patterns of pitch and rhythm can affect the perception of metrical structure. However, few attempts have been made either to define parallelism precisely or to characterize its effect on metrical analysis. The basic idea of the current model is that a repeated melodic pattern favors a metrical structure in which beats are placed at parallel points in each occurrence of the pattern. By this view, parallelism affects the period of the metrical structure (the distance between beats) rather than the phase (exactly where the beats occur). This model is implemented and incorporated into the metrical program of D. Temperley and D. Sleator (1999). Several examples of the model's output are presented; we examine problems with the model and discuss possible solutions.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2002-12-01},
  pages = {117-149},
  author = {Temperley, David and Bartlette, Christopher},
  file = {D\:\\Sauve\\Zotero\\storage\\A8PSNPPP\\Temperley and Bartlette - 2002 - Parallelism as a Factor in Metrical Analysis.pdf}
}

@article{pfordresherRoleMelodicRhythmic2003,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2003.20.4.431},
  title = {The {{Role}} of {{Melodic}} and {{Rhythmic Accents}} in {{Musical Structure}}},
  volume = {20},
  issn = {0730-7829},
  doi = {10.1525/mp.2003.20.4.431},
  abstract = {Two experiments investigated the perception of melodic and rhythmic accents in musical patterns. Musiclike patterns were created in which recurring melodic and/or rhythmic accents marked higher order periods that, when both accents were present, could differ in terms of period and/or phase according to the construct of joint accent structure (M. R. Jones, 1987). Listeners were asked to indicate the location of accents in these patterns by tapping to tone onsets. Each experiment pursued two main questions. First, are accents, as manipulated, salient to listeners? Second, do listeners track higher order time spans formed by melodic and rhythmic accents in a way that shows a sensitivity to interrelationships between melody and rhythm? Results supported affirmative answers to these questions in analyses of tapping locations and time spans between taps, respectively. Furthermore, results suggested that accents function as temporal landmarks that listeners can use when tracking the time structure of musical patterns, and that the complexity of this time structure arises from higher order time spans marked by different types of accents.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2003-06-01},
  pages = {431-464},
  author = {Pfordresher, Peter Q.},
  file = {D\:\\Sauve\\Zotero\\storage\\EB6HJP7D\\Pfordresher - 2003 - The Role of Melodic and Rhythmic Accents in Musica.pdf}
}

@article{cambouropoulosConceptualizingMusicCognitive2003,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2003.21.1.135},
  title = {Conceptualizing {{Music}}: {{Cognitive Structure}}, {{Theory}}, and {{Analysis}}},
  volume = {21},
  issn = {0730-7829},
  doi = {10.1525/mp.2003.21.1.135},
  shorttitle = {Conceptualizing {{Music}}},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2003-09-01},
  pages = {135-153},
  author = {Cambouropoulos, Emilios},
  file = {D\:\\Sauve\\Zotero\\storage\\9NSF8CQP\\Cambouropoulos - 2003 - Conceptualizing Music Cognitive Structure, Theory.pdf}
}

@article{davisonSelectiveAttentionTwoPart2003,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2003.21.1.3},
  title = {Selective {{Attention In Two}}-{{Part Counterpoint}}},
  volume = {21},
  issn = {0730-7829},
  doi = {10.1525/mp.2003.21.1.3},
  abstract = {In two experiments, the influence of unattended musical content on the processing of attended content was investigated by using melodic interval pairs. In Experiment 1, 2 two-note melodic fragments were played concurrently in separate pitch registers and with different timbres. Participants with music training were timed as they decided whether one melody was rising or falling while ignoring the other. Pitch direction (up or down) and interval type (major second and perfect fifth) of attended and unattended intervals were orthogonally combined. The measure of interference was the degree to which the unattended pattern influenced the processing time and the accuracy of judging the attended melody. Unattended voice influence from both types of intervals was observed on the attended voice for seconds, but not fifths, in Experiment 1. The second study replicated the first with interval pairs in overlapping pitch registers. Influence from the unattended voice was found for both seconds and fifths. Participants found the selection task more difficult when parts crossed and voices were moving in contrary motion rather than in the same direction. Implications for the emergence of global stimulus properties in the context of musical counterpoint are discussed. Both experiments showed that unattended musical content can affect processing of attended intervals and that pitch distance may serve to moderate selective attention. Received February 12, 1999, accepted June 24, 2003},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2003-09-01},
  pages = {3-20},
  author = {DAVISON, LYNN L. and BANKS, WILLIAM P.},
  file = {D\:\\Sauve\\Zotero\\storage\\2HRQGC7I\\DAVISON and BANKS - 2003 - Selective Attention In Two-Part Counterpoint.pdf}
}

@article{schulkindMusicalFeaturesThat2003,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2003.21.2.217},
  title = {Musical {{Features That Facilitate Melody Identification}}: {{How Do You Know It}}'s “{{Your}}” {{Song When They Finally Play It}}?},
  volume = {21},
  issn = {0730-7829},
  doi = {10.1525/mp.2003.21.2.217},
  shorttitle = {Musical {{Features That Facilitate Melody Identification}}},
  abstract = {What information do listeners use to “Name that Tune”? This question was investigated in a two-phase experiment. In Phase 1, the participants heard familiar melodies that were played on a note-by-note basis until they were identified. In Phase 2, each note of the melody was analyzed along a variety of musical dimensions. Multiple regression analyses determined which musical characteristics predicted identification performance. Identification was most strongly associated with notes located at phrase boundaries, notes that completed alternating sequences of rising and falling pitches, and metrically accented notes. As well, identification peaked after listeners heard moderate amounts of information (i.e., 5–7 notes). The data suggest that melody identification is a holistic, all-or-none process and that parallels can be drawn between melody and spoken word identification. Implications for current theories, future research, and the relationship between music perception and melody identification are discussed.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2003-12-01},
  pages = {217-249},
  author = {SCHULKIND, MATTHEW D. and POSNER, RACHEL J. and RUBIN, DAVID C.},
  file = {D\:\\Sauve\\Zotero\\storage\\7Z7FH8EH\\SCHULKIND et al. - 2003 - Musical Features That Facilitate Melody Identifica.pdf}
}

@article{mcadamsProblemSolvingStrategiesMusic2004,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2004.21.3.391},
  title = {Problem-{{Solving Strategies}} in {{Music Composition}}: {{A Case Study}}},
  volume = {21},
  issn = {0730-7829},
  doi = {10.1525/mp.2004.21.3.391},
  shorttitle = {Problem-{{Solving Strategies}} in {{Music Composition}}},
  abstract = {The composition of a piece of contemporary music for solo piano, 16-piece chamber orchestra, and 6-channel, computer-processed sound was tracked and documented from its initial conception to its concert premier. Notebooks, sketches, diagrams, recorded interviews, and the final score were used to address the solving of three compositional problems raised within the context of the piece. The first problem concerned the need to compose the five themes for the piece (23–100 s in duration) for both solo piano and chamber orchestra. Issues of performance constraints associated with the two media and on translation from a restricted to a more open timbral palette played an important role. The second problem involved composing the two major parts of the piece with similar temporal structures but vastly different ways of traversing the same thematic musical materials. Spatial, graphical representations and self-imposed graphic organization of the score were important factors in resolving this issue. The third problem involved conceiving of the computer component to accompany either of the two major parts, because the piece could be played with them in either order. The solution involved organizing the computer component into discrete parts that had fairly continuous textures and finalizing this component before the final composing of the instrumental components. Issues concerning the aspects of compositional problem-solving that are available for study, the types of representations used in problem solving,and the generalizability of such results to other pieces by the same composer or other composers are discussed.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2004-03-01},
  pages = {391-429},
  author = {McADAMS, STEPHEN},
  file = {D\:\\Sauve\\Zotero\\storage\\DNTIMGJE\\McADAMS - 2004 - Problem-Solving Strategies in Music Composition A.pdf}
}

@article{handelSoundSourceIdentification2004,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2004.21.4.587},
  title = {Sound {{Source Identification}}: {{The Possible Role}} of {{Timbre Transformations}}},
  volume = {21},
  issn = {0730-7829},
  doi = {10.1525/mp.2004.21.4.587},
  shorttitle = {Sound {{Source Identification}}},
  abstract = {Timbre is typically investigated as a perceptual attribute that differentiates instruments at one pitch. Yet the perceptual usefulness of timbre is that it allows listeners to recognize one instrument at different pitches. Using stimuli produced across the playing range by three wind instruments from two categories, woodwind and brass, we measured listeners' judgments of instrumental timbre across pitch in a dissimilarity task and measured listeners' ability to identify stimuli as being produced by the same or different instrument in a three-note oddball task. The resulting multidimensional scaling representation showed that Dimension 1 correlated with pitch, whereas Dimension 2 correlated with spectral centroid and separated the instrumental stimuli into the categories woodwind and brass. For three-note sequences, the task was extremely difficult for the woodwind pair, with listeners typically choosing the most dissimilarly pitched stimulus as coming from the oddball source. In contrast, the three-note sequences were easy for the woodwind-brass pairs. The results from these experiments illustrate the difficulty of extrapolating the timbre of a sound source across large differences in pitch.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2004-06-01},
  pages = {587-610},
  author = {Handel, Stephen and Erickson, Molly L.},
  file = {D\:\\Sauve\\Zotero\\storage\\7MI7CWDZ\\Handel and Erickson - 2004 - Sound Source Identification The Possible Role of .pdf}
}

@article{franklandParsingMelodyQuantification2004,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2004.21.4.499},
  title = {Parsing of {{Melody}}: {{Quantification}} and {{Testing}} of the {{Local Grouping Rules}} of {{Lerdahl}} and {{Jackendoff}}'s {{A Generative Theory}} of {{Tonal Music}}},
  volume = {21},
  issn = {0730-7829},
  doi = {10.1525/mp.2004.21.4.499},
  shorttitle = {Parsing of {{Melody}}},
  abstract = {In two experiments, the empirical parsing of melodies was compared with predictions derived from four grouping preference rules of A Generative Theory of Tonal Music (F. Lerdahl \& R. Jackendoff, 1983). In Experiment 1 (n = 123), listeners representing a wide range of musical training heard two familiar nursery-rhyme melodies and one unfamiliar tonal melody, each presented three times. During each repetition, listeners indicated the location of boundaries between units by pressing a key. Experiment 2 (n = 33) repeated Experiment 1 with different stimuli: one familiar and one unfamiliar nursery-rhyme melody, and one unfamiliar, tonal melody from the classical repertoire. In all melodies of both experiments, there was good within-subject consistency of boundary placement across the three repetitions (mean r = .54). Consistencies between Repetitions 2 and 3 were even higher (mean r = .63). Hence, Repetitions 2 and 3 were collapsed. After collapsing, there was high between-subjects similarity in boundary placement for each melody (mean r = .62), implying that all participants parsed the melodies in essentially the same (though not identical) manner. A role for musical training in parsing appeared only for the unfamiliar, classical melody of Experiment 2. The empirical parsing profiles were compared with the quantified predictions of Grouping Preference Rules 2a (the Rest aspect of Slur/Rest), 2b (Attack-point), 3a (Register change), and 3d (Length change). Based on correlational analyses, only Attack-point (mean r = .80) and Rest (mean r = .54) were necessary to explain the parsings of participants. Little role was seen for Register change (mean r = .14) or Length change (mean r = –.09). Solutions based on multiple regression further reduced the role for Register and Length change. Generally, results provided some support for aspects of A Generative Theory of Tonal Music, while implying that some alterations might be useful.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2004-06-01},
  pages = {499-543},
  author = {Frankland, Bradley W. and McAdams, Stephen and Cohen, Annabel J. and McAdams, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\4Q2ZQRX7\\Frankland et al. - 2004 - Parsing of Melody Quantification and Testing of t.pdf}
}

@book{schoenbergStructuralFunctionsHarmony1969,
  title = {Structural Functions of Harmony},
  number = {478},
  publisher = {{WW Norton \& Company}},
  date = {1969},
  author = {Schoenberg, Arnold and Stein, Leonard}
}

@article{collinsModelingPatternImportance2011,
  title = {Modeling Pattern Importance in {{Chopin}}’s {{Mazurkas}}},
  volume = {28},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.28.4.387},
  abstract = {This study relates various quantifiable characteristics of a musical pattern to subjective assessments of a pattern’s salience. Via score analysis and listening, twelve music undergraduates examined excerpts taken from Chopin’s mazurkas. They were instructed to rate already discovered patterns, giving high ratings to patterns that they thought were noticeable and/or important. Each undergraduate rated thirty specified patterns and ninety patterns were examined in total. Twenty-nine quantifiable attributes (some novel but most proposed previously) were determined for each pattern, such as the number of notes a pattern contained. A model useful for relating participants’ ratings to the attributes was determined using variable selection and cross-validation. Individual participants were much poorer than the model at predicting the consensus ratings of other participants. While the favored model contains only three variables, many variables were identified as having some predictive value if considered in isolation. Implications for music psychology, analysis, and information retrieval are discussed. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  date = {2011-04},
  pages = {387-414},
  keywords = {Music,Mathematical Modeling,Chopin's Mazurkas,modeling pattern,pattern's salience,Simulation},
  author = {Collins, Tom and Laney, Robin and Willis, Alistair and Garthwaite, Paul H.}
}

@article{lerdahlAtonalProlongationalStructure1989,
  title = {Atonal Prolongational Structure},
  volume = {4},
  issn = {0749-4467},
  url = {http://dx.doi.org/10.1080/07494468900640211},
  doi = {10.1080/07494468900640211},
  abstract = {The early atonal music of Schoenberg and his school is emblematic of the difficulties in comprehending twentieth-century music. Music theorists have tried to analyze this music either through a modified Schenkerian approach or through pitch-set theory. After discussing limitations in these approaches, this paper develops at length a different theory for atonal music, one that adapts Jackendoff's and my A Generative Theory of Tonal Music in appropriate ways. Analyses of three Schoenberg pieces (from op. 11 and op. 19) illustrate the theory. The paper closes with an informal discussion of atonal pitch space.},
  number = {1},
  journaltitle = {Contemporary Music Review},
  urldate = {2015-06-18},
  date = {1989-01-01},
  pages = {65-87},
  author = {Lerdahl, Fred},
  file = {D\:\\Sauve\\Zotero\\storage\\PDGTRA3F\\07494468900640211.html}
}

@article{deikeBuildupAuditoryStream2012,
  langid = {english},
  title = {The {{Build}}-up of {{Auditory Stream Segregation}}: {{A Different Perspective}}},
  volume = {3},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00461},
  shorttitle = {The {{Build}}-up of {{Auditory Stream Segregation}}},
  abstract = {The build-up of auditory stream segregation refers to the notion that sequences of alternating A and B sounds initially tend to be heard as a single stream, but with time appear to split into separate streams. The central assumption in the analysis of this phenomenon is that streaming sequences are perceived as one stream at the beginning by default. In the present study, we test the validity of this assumption and document its impact on the apparent build-up phenomenon. Human listeners were presented with ABAB sequences, where A and B were harmonic tone complexes of seven different fundamental frequency separations (Δf) ranging from 2 to 14 semitones. Subjects had to indicate, as promptly as possible, their initial percept of the sequences, as either "one stream" or "two streams," and any changes thereof during the sequences. We found that subjects did not generally indicate a one-stream percept at the beginning of streaming sequences. Instead, the first perceptual decision depended on Δf, with the probability of a one-stream percept decreasing, and that of a two-stream percept increasing, with increasing Δf. Furthermore, subjects required some time to make and report a decision on their perceptual organization. Taking this time into account, the resulting time courses of two-stream probabilities differ markedly from those suggested by the conventional analysis. A build-up-like increase in two-stream probability was found only for the Δf of six semitones. At the other Δf conditions no or only minor increases in two-stream probability occurred. These results shed new light on the build-up of stream segregation and its possible neural correlates.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  date = {2012},
  pages = {461},
  author = {Deike, Susann and Heil, Peter and Böckmann-Barthel, Martin and Brechmann, André},
  eprinttype = {pmid},
  eprint = {23118731},
  pmcid = {PMC3484653}
}

@audio{opolkoMcGillUniversityMaster2006,
  langid = {No Linguistic Content},
  location = {{[Montreal, Quebec, Canada]}},
  title = {McGill University master samples collection on DVD},
  publisher = {{McGill [University]}},
  date = {2006},
  editora = {Opolko, Frank and Wapnick, Joel and {McGill University}},
  editoratype = {collaborator}
}

@article{iversonAuditoryStreamSegregation1995,
  langid = {english},
  title = {Auditory Stream Segregation by Musical Timbre: Effects of Static and Dynamic Acoustic Attributes},
  volume = {21},
  issn = {0096-1523},
  shorttitle = {Auditory Stream Segregation by Musical Timbre},
  abstract = {Two experiments examined the influence of timbre on auditory stream segregation. In experiment 1, listeners heard sequences of orchestral tones equated for pitch and loudness, and they rated how strongly the instruments segregated. Multidimensional scaling analyses of these ratings revealed that segregation was based on the static and dynamic acoustic attributes that influenced similarity judgements in a previous experiment (P Iverson \& CL Krumhansl, 1993). In Experiment 2, listeners heard interleaved melodies and tried to recognize the melodies played by a target timbre. The results extended the findings of Experiment 1 to tones varying pitch. Auditory stream segregation appears to be influenced by gross differences in static spectra and by dynamic attributes, including attack duration and spectral flux. These findings support a gestalt explanation of stream segregation and provide evidence against peripheral channel model.},
  number = {4},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  date = {1995-08},
  pages = {751-763},
  keywords = {Music,acoustics,pitch perception,humans,Time Factors},
  author = {Iverson, P.},
  eprinttype = {pmid},
  eprint = {7643047}
}

@article{mackenFunctionalCharacteristicsInner1995,
  langid = {english},
  title = {Functional Characteristics of the Inner Voice and the Inner Ear: Single or Double Agency?},
  volume = {21},
  issn = {0278-7393},
  shorttitle = {Functional Characteristics of the Inner Voice and the Inner Ear},
  abstract = {Double agency theories of short-term memory posit the functional independence of a phonological store (inner ear) and articulatory process (inner voice). A series of 5 experiments challenges this view. Articulatory suppression during retention of 9-item lists gives rise to a changing-state effect similar to that shown for irrelevant speech. Also, vocalized suppression is more disruptive than silently mouthed suppression, but this difference arises from vocalization itself rather than from any auditory feedback to which it gives rise. Class similarity between the to-be-remembered items and the articulatory material is not a critical determinant, but the effect occurs only with tests of serial order. For mouthed suppression, the irrelevant speech effect is only attenuated with changing-state suppression. Also, the presence of changing-state irrelevant speech abolishes the changing-state effect of articulatory suppression. Functional equivalence of codes from auditory, visual, and articulatory sources is claimed.},
  number = {2},
  journaltitle = {Journal of Experimental Psychology. Learning, Memory, and Cognition},
  shortjournal = {J Exp Psychol Learn Mem Cogn},
  date = {1995-03},
  pages = {436-448},
  keywords = {Attention,humans,Adult,Female,Male,Feedback,Memory; Short-Term,Practice (Psychology),Retention (Psychology),Serial Learning,Verbal Behavior,Verbal Learning},
  author = {Macken, W. J. and Jones, D. M.},
  eprinttype = {pmid},
  eprint = {7738509}
}

@article{schulteDifferentModesPitch2002,
  title = {Different Modes of Pitch Perception and Learning-Induced Neuronal Plasticity of the Human Auditory Cortex},
  volume = {9},
  issn = {0792-8483},
  doi = {10.1155/NP.2002.161},
  abstract = {We designed a melody perception experiment involving eight harmonic complex tones of missing fundamental frequencies (hidden auditory object) to study the short-term neuronal plasticity of the auditory cortex. In this experiment, the fundamental frequencies of the complex tones followed the beginning of the virtual melody of the tune "Frère Jacques". The harmonics of the complex tones were chosen so that the spectral melody had an inverse contour when compared with the virtual one. Evoked magnetic fields were recorded contralaterally to the ear of stimulation from both hemispheres. After a base line measurement, the subjects were exposed repeatedly to the experimental stimuli for 1 hour a day. All subjects reported a sudden change in the perceived melody, indicating possible reorganization of the cortical processes involved in the virtual pitch formation. After this switch in perception, a second measurement was performed. Cortical sources of the evoked gamma-band activity were significantly stronger and located more medially after a switch in perception. Independent Component Analysis revealed enhanced synchronization in the gamma-band frequency range. Comparing the gamma-band activation of both hemispheres, no laterality effects were observed. The results indicate that the primary auditory cortices are involved in the process of virtual pitch perception and that their function is modifiable by laboratory manipulation. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {Neural Plasticity},
  shortjournal = {Neural Plasticity},
  date = {2002},
  pages = {161-175},
  keywords = {pitch perception,Auditory Cortex,Neural Plasticity,Learning,human auditory cortex,learning-induced neuronal plasticity},
  author = {Schulte, Michael and Knief, Arne and Seither-Preisler, Annemarie and Pantev, Christo}
}

@article{menningPlasticChangesAuditory2000,
  title = {Plastic Changes in the Auditory Cortex Induced by Intensive Frequency Discrimination Training},
  volume = {11},
  issn = {0959-4965},
  doi = {10.1097/00001756-200003200-00032},
  abstract = {Examined the slow auditory evoked (N1m) and mismatch field (MMF) as indicators of cortical plasticity through training of an auditory discrimination task. 10 healthy adults (aged 20–32 yrs) participated in daily pitch discrimination training, the goal of which was to detect deviants differing by progressively smaller frequency shifts from the standard stimulus. The standard stimulus of a 1000 Hz tone and the 3 deviant stimuli of 1050, 1010, and 1005 Hz were used in discrimination tests as magnetoencephalography data were recorded. Over 3 wks of training, frequency discrimination improved rapidly in the 1st wk, followed by small but constant improvements thereafter. Results show that N1m and MMF responses to the deviant stimuli increased in amplitude during training. This enhancement persisted until training was finished, but decreased 3 wks subsequently. Results suggest a plastic reorganization of the cortical representation for the trained frequencies. While the MMF seems to be a good measure of automatic deviation detection, the N1m may reflect even more basic changes of the frequency representation in the auditory cortex or possible modulatory influences. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {4},
  journaltitle = {NeuroReport: For Rapid Communication of Neuroscience Research},
  shortjournal = {NeuroReport: For Rapid Communication of Neuroscience Research},
  date = {2000-03},
  pages = {817-822},
  keywords = {Pitch Discrimination,Auditory Evoked Potentials,Auditory Cortex,Neural Plasticity,Neurophysiology,20–32 yr olds,pitch discrimination training,slow auditory evoked potentials & mismatch field plastic changes in auditory cortex},
  author = {Menning, Hans and Roberts, Larry E. and Pantev, Christo}
}

@article{pantevTimbrespecificEnhancementAuditory2001,
  title = {Timbre-Specific Enhancement of Auditory Cortical Representations in Musicians},
  volume = {12},
  issn = {0959-4965},
  doi = {10.1097/00001756-200101220-00041},
  abstract = {Neural imaging studies have shown that the brains of skilled musicians respond differently to musical stimuli than do the brains of non-musicians, particularly for musicians who commenced practice at an early age. Whether brain attributes related to musical skill are attributable to musical practice or are hereditary traits that influence the decision to train musically is a question. The authors here report that auditory cortical representations measured neuromagnetically for tones of different timbre (violin and trumpet) are enhanced compared to sine tones in adult violinists and trumpeters, preferentially for timbres of the instrument of training. Timbre specificity is predicted by a principle of use-dependent plasticity and imposes new requirements on nativistic accounts of brain attributes associated with musical skill. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {NeuroReport: For Rapid Communication of Neuroscience Research},
  shortjournal = {NeuroReport: For Rapid Communication of Neuroscience Research},
  date = {2001-01},
  pages = {169-174},
  keywords = {Musicians,Music,Auditory Discrimination,Auditory Cortex,Ability Level,auditory cortical representations,timbre in violin vs trumpet tones,trained violinists vs trumpeters},
  author = {Pantev, Christo and Roberts, Larry E. and Schulz, Matthias and Engelien, Almut and Ross, Bernhard}
}

@article{garridoDynamicCausalModeling2009,
  title = {Dynamic Causal Modeling of the Response to Frequency Deviants},
  volume = {101},
  issn = {0022-3077},
  doi = {10.1152/jn.90291.2008},
  abstract = {This article describes the use of dynamic causal modeling to test hypotheses about the genesis of evoked responses. Specifically, we consider the mismatch negativity (MMN), a well-characterized response to deviant sounds and one of the most widely studied evoked responses. There have been several mechanistic accounts of how the MMN might arise. It has been suggested that the MMN results from a comparison between sensory input and a memory trace of previous input, although others have argued that local adaptation, due to stimulus repetition, is sufficient to explain the MMN. Thus the precise mechanisms underlying the generation of the MMN remain unclear. This study tests some biologically plausible spatiotemporal dipole models that rest on changes in extrinsic top-down connections (that enable comparison) and intrinsic changes (that model adaptation). Dynamic causal modeling suggested that responses to deviants are best explained by changes in effective connectivity both within and between cortical sources in a hierarchical network of distributed sources. Our model comparison suggests that both adaptation and memory comparison operate in concert to produce the early (N1 enhancement) and late (MMN) parts of the response to frequency deviants. We consider these mechanisms in the light of predictive coding and hierarchical inference in the brain. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {5},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {Journal of Neurophysiology},
  date = {2009-05},
  pages = {2620-2631},
  keywords = {brain,Evoked Potentials,Stimulus Frequency,Causal Analysis,Dynamic causal modeling,evoked response genesis,frequency deviants},
  author = {Garrido, Marta I. and Kilner, James M. and Kiebel, Stefan J. and Friston, Karl J.}
}

@article{fogelsonFunctionalAnatomySchizophrenia2014,
  title = {The Functional Anatomy of Schizophrenia: {{A}} Dynamic Causal Modeling Study of Predictive Coding},
  volume = {158},
  issn = {0920-9964},
  doi = {10.1016/j.schres.2014.06.011},
  shorttitle = {The Functional Anatomy of Schizophrenia},
  abstract = {This paper tests the hypothesis that patients with schizophrenia have a deficit in selectively attending to predictable events. We used dynamic causal modeling (DCM) of electrophysiological responses–to predictable and unpredictable visual targets – to quantify the effective connectivity within and between cortical sources in the visual hierarchy in 25 schizophrenia patients and 25 age-matched controls. We found evidence for marked differences between normal subjects and schizophrenia patients in the strength of extrinsic backward connections from higher hierarchical levels to lower levels within the visual system. In addition, we show that not only do schizophrenia subjects have abnormal connectivity but also that they fail to adjust or optimize this connectivity when events can be predicted. Thus, the differential intrinsic recurrent connectivity observed during processing of predictable versus unpredictable targets was markedly attenuated in schizophrenia patients compared with controls, suggesting a failure to modulate the sensitivity of neurons responsible for passing sensory information of prediction errors up the visual cortical hierarchy. The findings support the proposed role of abnormal connectivity in the neuropathology and pathophysiology of schizophrenia. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {1-3},
  journaltitle = {Schizophrenia Research},
  shortjournal = {Schizophrenia Research},
  date = {2014-09},
  pages = {204-212},
  keywords = {Prediction,electroencephalography,EEG,Simulation,Dynamic causal modeling,Connectivity,Neuroanatomy,Pathophysiology,Schizophrenia},
  author = {Fogelson, Noa and Litvak, Vladimir and Peled, Avi and Fernandez-del-Olmo, Miguel and Friston, Karl}
}

@article{fristonFreeenergyPrincipleUnified2010,
  title = {The Free-Energy Principle: {{A}} Unified Brain Theory?},
  volume = {11},
  issn = {1471-003X},
  doi = {10.1038/nrn2787},
  shorttitle = {The Free-Energy Principle},
  abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories—optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nature Reviews Neuroscience},
  date = {2010-02},
  pages = {127-138},
  keywords = {brain,Theories,entropy,free energy principle,homeostasis,neuroscience,optimization,physics,Systems Neuroscience},
  author = {Friston, Karl}
}

@article{feldmanAttentionUncertaintyFreeenergy2010,
  title = {Attention, Uncertainty, and Free-Energy},
  volume = {4},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2010.00215},
  abstract = {We suggested recently that attention can be understood as inferring the level of uncertainty or precision during hierarchical perception. In this paper, we try to substantiate this claim using neuronal simulations of directed spatial attention and biased competition. These simulations assume that neuronal activity encodes a probabilistic representation of the world that optimizes free-energy in a Bayesian fashion. Because free-energy bounds surprise or the (negative) log-evidence for internal models of the world, this optimization can be regarded as evidence accumulation or (generalized) predictive coding. Crucially, both predictions about the state of the world generating sensory data and the precision of those data have to be optimized. Here, we show that if the precision depends on the states, one can explain many aspects of attention. We illustrate this in the context of the Posner paradigm, using the simulations to generate both psychophysical and electrophysiological responses. These simulated responses are consistent with attentional bias or gating, competition for attentional resources, attentional capture and associated speed-accuracy trade-offs. Furthermore, if we present both attended and nonattended stimuli simultaneously, biased competition for neuronal representation emerges as a principled and straightforward property of Bayes-optimal perception. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Frontiers in Human Neuroscience},
  date = {2010-12-02},
  keywords = {selective attention,Neurons,biased competition,Competition,directed spatial attention,free energy,neuronal simulation,uncertainty},
  author = {Feldman, Harriet and Friston, Karl J.}
}

@incollection{fristonPredictiveCodingFreeenergy2011,
  location = {{New York, NY, US}},
  title = {Predictive Coding: {{A}} Free-Energy Formulation},
  isbn = {978-0-19-539551-8},
  shorttitle = {Predictive Coding},
  abstract = {This chapter looks at prediction from the point of view of perception; namely, the fitting or inversion of internal models of sensory data by the brain. Critically, the nature of this inversion lends itself to a relatively simple neural network implementation that shares many formal similarities with real cortical hierarchies in the brain. The basic idea that the brain uses hierarchical inference has been described in a series of papers, which entertain the notion that the brain uses empirical Bayes for inference about its sensory input, given the hierarchical organization of cortical systems. Here, we focus on how neural networks could be configured to invert these models and deconvolve sensory causes from sensory input. This chapter comprises four sections. In the first, we introduce hierarchical dynamic models. Hierarchies induce empirical priors that provide constraints, which are exploited during inversion. In the second section, we consider model inversion in statistical terms. This summarizes the material in Friston et al. (2008). In the third section, we show how this inversion can be formulated as a simple gradient ascent using neuronal networks and, in the final section, we consider how evoked brain responses might be understood in terms of inference under hierarchical dynamic models of sensory input. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (chapter)},
  booktitle = {Predictions in the Brain: {{Using}} Our Past to Generate a Future.},
  publisher = {{Oxford University Press}},
  date = {2011},
  pages = {231-246},
  keywords = {predictive coding,brain,Prediction,Human Information Storage,Perception,cortical systems,inference,Neural Networks,Perceptual Stimulation,sensory data},
  author = {Friston, Karl J. and Kiebel, Stefan},
  editor = {Bar, Moshe and Bar, Moshe (Ed)}
}

@article{temperleyModelingMeterHarmony1999,
  title = {Modeling {{Meter}} and {{Harmony}}: {{A Preference}}-{{Rule Approach}}},
  volume = {23},
  issn = {0148-9267},
  url = {http://dx.doi.org/10.1162/014892699559616},
  doi = {10.1162/014892699559616},
  shorttitle = {Modeling {{Meter}} and {{Harmony}}},
  number = {1},
  journaltitle = {Comput. Music J.},
  urldate = {2015-06-18},
  date = {1999-01},
  pages = {10--27},
  author = {Temperley, David and Sleator, Daniel}
}

@article{cambouropoulosHowSimilarSimilar2009,
  title = {How Similar Is Similar?},
  volume = {Discussion Forum 4B},
  issn = {1029-8649},
  doi = {10.1177/102986490901300102},
  abstract = {In the first part of the paper a theoretical discussion is presented regarding the fundamental concept of similarity and its relation to cue abstraction and categorization. It is maintained that similarity is by definition context-dependent and strongly interrelated to cue abstraction and categorization. Emphasis is given to determining the "musical surface" that can act as a musically pertinent lowest level of abstraction on which similarity between musical entities can be measured. Then, each of these concepts is examined in more detail with respect to a number of research studies presented in the recent special issue of Musicæ Scientiæ on musical similarity (Discussion Forum 4A, 2007). Views claiming that a geometric piano-roll-like representation is the most appropriate choice for polyphonic pattern matching, or that musical repetition is structurally significant if at least fifty percent of a pattern is equivalent (i.e. if it is more similar than dissimilar), or that "dramatic disparities" between musical similarities and corresponding categories can be found in empirical studies, are critically re-examined with a view to clarifying the fundamental concept of similarity. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  journaltitle = {Musicae Scientiae},
  shortjournal = {Musicae Scientiae},
  date = {2009},
  pages = {7-24},
  keywords = {music perception,Cues,cue abstraction,musical similarity},
  author = {Cambouropoulos, Emilios}
}

@article{cambouropoulosPitchSpellingComputational2003,
  title = {Pitch Spelling: {{A}} Computational Model},
  volume = {20},
  issn = {0730-7829},
  doi = {10.1525/mp.2003.20.4.411},
  shorttitle = {Pitch Spelling},
  abstract = {In this article, cognitive and musicological aspects of pitch and pitch interval representations are explored via computational modeling. The specific task under investigation is pitch spelling, that is, how traditional score notation can be derived from a simple unstructured 12-tone representation (e.g., pitch-class set or MIDI pitch representation). This study provides useful insights both into the domain of pitch perception and into musicological aspects of score notation strategies. A computational model is described that transcribes polyphonic MIDI pitch files into the Western traditional music notation. Input to the proposed algorithm is merely a sequence of MIDI pitch numbers in the order they appear in a MIDI file. No a priori knowledge such as key signature, tonal centers, time signature, chords, or voice separation is required. Output of the algorithm is a sequence of "correctly" spelled pitches. The algorithm is based on an interval optimization approach that takes into account the frequency of occurrence of pitch intervals within the major-minor tonal scale framework. The algorithm was evaluated on 10 complete piano sonatas by Mozart and had a success rate of 98.8\% (634 pitches were spelled incorrectly out of a total of 54,418 notes)... (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Sum  2003},
  pages = {411-429},
  keywords = {Music,Pitch (Frequency),music perception,computational model,Models,Simulation,algorithm,Algorithms,music notation,pitch interval representations,pitch spelling,transcription,Written Communication},
  author = {Cambouropoulos, Emilios}
}

@article{cambouropoulosMusicalSurfaceChallenging2010,
  title = {The Musical Surface: {{Challenging}} Basic Assumptions},
  volume = {Special Issue 2010},
  issn = {1029-8649},
  shorttitle = {The Musical Surface},
  abstract = {This paper addresses problems and misconceptions pertaining to the notion of the musical surface, a notion that is commonly thought to be relatively straight-forward and is often taken as a given in computational and cognitive research. It is suggested that the musical surface is comprised of (complex) musical events perceived as wholes within coherent musical streams—the musical surface is not merely an unstructured sequence of atomic note events, such as score notes or a piano-roll representation. Additionally, it is maintained that the emergence of the musical surface involves rather complex mechanisms that require, not only multi-pitch extraction from the acoustic signal, but, the employment of cognitive processes such as beat-tracking, metre induction, chord identification and stream/voice separation. Such processes do not come into play after the surface has been formed, but are, rather, an integral part of the formation of the musical surface per se. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  journaltitle = {Musicae Scientiae},
  shortjournal = {Musicae Scientiae},
  series = {Understanding Musical Structure and Form: {{Papers}} in Honour of {{Irene Deliége}}},
  date = {2010},
  pages = {131-147},
  keywords = {Music,Musical Instruments,challenging basic assumptions,cognitive psychology,musical surface,piano roll representation},
  author = {Cambouropoulos, Emilios}
}

@article{cambouropoulosVoiceStreamPerceptual2008,
  title = {Voice and Stream: {{Perceptual}} and Computational Modeling of Voice Separation},
  volume = {26},
  issn = {0730-7829},
  doi = {10.1525/mp.2008.26.1.75},
  shorttitle = {Voice and Stream},
  abstract = {[Correction Notice: An erratum for this article was reported in Vol 26(3) of Music Perception (see record [rid]2009-01977-018[/rid]). In Figure 1 of the original article, the harmonic reduction is incorrect. The corrected version of the figure is provided in the current erratum.] Listeners are thought to be capable of perceiving multiple voices in music. This paper presents different views of what 'voice' means and how the problem of voice separation can be systematically described, with a view to understanding the problem better and developing a systematic description of the cognitive task of segregating voices in music. Well-established perceptual principles of auditory streaming are examined and then tailored to the more specific problem of voice separation in timbrally undifferentiated music. Adopting a perceptual view of musical voice, a computational prototype is developed that splits a musical score (symbolic musical data) into different voices. A single 'voice' may consist of one or more synchronous notes that are perceived as belonging to the same auditory stream. The proposed model is tested against a small dataset that acts as ground truth. The results support the theoretical viewpoint adopted in the paper. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  date = {2008-09},
  pages = {75-94},
  keywords = {Music,Voice,Perception,cognitive ability,cognitive task,musical voice,synchronous notes,voice perception,voice separation},
  author = {Cambouropoulos, Emilios}
}

@article{cambouropoulosMelodicCueAbstraction2001,
  title = {Melodic Cue Abstraction, Similarity and Category Formation: {{A}} Formal Model},
  volume = {18},
  issn = {0730-7829},
  doi = {10.1525/mp.2001.18.3.347},
  shorttitle = {Melodic Cue Abstraction, Similarity and Category Formation},
  abstract = {In the first part of this article, the notions of identity, similarity, categorization, and feature salience are explored; musical examples are provided at various stages of the discussion. Then, formal working definitions are proposed that bind these concepts together. These definitions readily lend themselves to the development of a formal model for clustering—the Unscramble algorithm—which, given a set of objects and an initial set of properties, generates a range of plausible categorizations for a given context. Finally, as a test case, the clustering algorithm is used to organize a number of melodic segments, taken from a monophonic piece by J. S. Bach, into motivic categories; the algorithm also determines a prototype for each cluster and uses these prototypical descriptions for membership prediction tasks. The results of the computational system are compared with the empirical results obtained for the same data in two earlier studies (I. Deliège, 1996, 1997). (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {3},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  series = {Similarity {{Perception}} ←→ {{Categorization}} ←→ {{Cue Abstraction}}},
  year = {Spr  2001},
  pages = {347-370},
  keywords = {Music,Models,Algorithms,categorization,Classification (Cognitive Process),clustering model,melodic cue abstraction,similarity,Stimulus Similarity,Unscramble algorithm},
  author = {Cambouropoulos, Emilios}
}

@article{bigandDividedAttentionMusic2000,
  title = {Divided Attention in Music},
  volume = {35},
  issn = {00207594},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=4424138&site=ehost-live},
  doi = {10.1080/002075900750047987},
  abstract = {Two models have been advanced to account for the apparent ease with which attention can be divided in music: a "divided attention" model postulates that listeners effectively manage to follow two or more melodic lines played simultaneously. According to a "figure-ground model," the harmonic coherence of Western polyphonies allows a listener to focus on one melody while staying aware of the other melody, which acts as a background. This figure-ground processing compensates for the inability to divide attention. The present study was designed to further investigate these two models. Participants were required to detect melodic errors in two familiar nursery tunes played simultaneously an octave apart. The divided-attention model predicted that this task would be easily performed by participants, irrespective of the key of the nursery tune. The figureground model predicted better performance when the keys of the tunes were identical or closely related. None of these predictions was fully supported by the data, leading us to propose a new "integrative model" of listening to polyphonic music. Deux modèles ont été proposés pour rendre compte de l'apparente facilité à partager son attention lors de l'écoute musicale: selon un modèle "d'attention partagée", l'auditeur parviendrait à suivre deux ou plusieurs lignes mélodiques jouées simultanément. Selon un modèle "figure-fond", la cohérence harmonique des polyphonies occidentales permettrait à l'auditeur de focaliser son attention sur une mélodie tout en restant attentif à l'autre mélodie qui agirait comme un fond harmonieux. Ce processus d'organisation de la polyphonie en figure-fond compenserait l'incapacité à diviser l'attention. L'étude présente fut conçue pour approfondir ces deux modèles. Les sujets devaient détecter des erreurs mélodiques dans deux mélodies populaires jouées simultanément à deux octaves différentes. Selon le modèle "d'attention partagée", cette tâche devait être facilement réalisée, quelque soit la tonalité respective des mélodies. Le modèle "figure-fond" prédisait de meilleures performances lorsque les tonalités des mélodies étaient identiques ou proches. Aucune de ces prédictions n'a été pleinement confirmée par les données ce qui conduit à développer un nouveau modèle "d'écoute intégrative" de la polyphonie.},
  number = {6},
  journaltitle = {International Journal of Psychology},
  shortjournal = {International Journal of Psychology},
  urldate = {2015-06-18},
  date = {2000-12},
  pages = {270-278},
  keywords = {Attention,FIGURE-ground perception,MUSIC psychology},
  author = {Bigand, E. and McAdams, S. and Forêt, S.},
  file = {D\:\\Sauve\\Zotero\\storage\\G9M7H7EN\\Bigand et al. - 2000 - Divided attention in music.pdf}
}

@article{fitchPerceptionProductionSyncopated2007,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2007.25.1.43},
  title = {Perception and Production of Syncopated Rhythms},
  date = {2007},
  author = {Fitch, W. Tecumseh and Rosenfeld, Andrew J.},
  file = {D\:\\Sauve\\Zotero\\storage\\VHZGRICN\\Fitch and Rosenfeld - 2007 - Perception and production of syncopated rhythms.pdf;D\:\\Sauve\\Zotero\\storage\\XGTBABS8\\cookieAbsent.html}
}

@inproceedings{toussaintComparisonRhythmicSimilarity2004,
  title = {A {{Comparison}} of {{Rhythmic Similarity Measures}}.},
  url = {http://jeff.cs.mcgill.ca/~godfried/teaching/mir-reading-assignments/Comparison-of-Rhythmic-Similarity-Measures.pdf},
  booktitle = {{{ISMIR}}},
  urldate = {2015-06-18},
  date = {2004},
  author = {Toussaint, Godfried T.},
  file = {D\:\\Sauve\\Zotero\\storage\\PAR8792E\\Toussaint - 2004 - A Comparison of Rhythmic Similarity Measures..pdf}
}

@incollection{shmulevichComplexityMeasuresMusical2000,
  location = {{Lisse, NL}},
  title = {Complexity Measures of Musical Rhythms},
  url = {http://www.nici.ru.nl/mmm/papers/mmm-16/mmm-16.html},
  booktitle = {Rhythm Perception and Production},
  publisher = {{Swets \& Zeitlinger}},
  urldate = {2015-06-18},
  date = {2000},
  pages = {239--244},
  author = {Shmulevich, Ilya and Povel, Dirk-Jan},
  editor = {Desain, P. and Windsor, L.},
  file = {D\:\\Sauve\\Zotero\\storage\\2RDQ77EM\\mmm-16.html}
}

@article{shmulevichPerceptualIssuesMusic2001,
  title = {Perceptual Issues in Music Pattern Recognition: {{Complexity}} of Rhythm and Key Finding},
  volume = {35},
  url = {http://link.springer.com/article/10.1023/A:1002629217152},
  shorttitle = {Perceptual Issues in Music Pattern Recognition},
  number = {1},
  journaltitle = {Computers and the Humanities},
  urldate = {2015-06-18},
  date = {2001},
  pages = {23--35},
  author = {Shmulevich, Ilya and Yli-Harja, Olli and Coyle, Edward and Povel, Dirk-Jan and Lemström, Kjell},
  file = {D\:\\Sauve\\Zotero\\storage\\D2H8QSPG\\Shmulevich et al. - 2001 - Perceptual issues in music pattern recognition Co.pdf;D\:\\Sauve\\Zotero\\storage\\S2M5SNU4\\A1002629217152.html}
}

@article{povelTheoreticalFrameworkRhythm1984,
  title = {A Theoretical Framework for Rhythm Perception},
  volume = {45},
  issn = {0340-0727},
  doi = {10.1007/BF00309709},
  abstract = {Presents a theoretical framework for predicting the perceived organization, the judged complexity, and the experienced rhythmical value of temporal sequences. Two simple assumptions form the basis of the framework. The notion of the "temporal grid" is proposed to specify the temporal structure of a sequence. Such a grid is a time scale consisting of isochronic intervals. Since a rhythmical pattern generally allows for several different possible grids, an "economy principle" is used for selecting the most efficient grid. Economy of description is determined by the number of tones covered by the grid and the ease of specifying the noncovered tones. Since this basic model cannot explain all relevant phenomena, it is extended to incorporate 3 additional factors: the starting point of a sequence, subjective accents, and tempo. (61 ref) (PsycINFO Database Record (c) 2013 APA, all rights reserved)},
  number = {4},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  date = {1984-03},
  pages = {315-337},
  keywords = {Auditory Perception,time perception,Models,Pattern Discrimination,theoretical model for perception of rhythm & temporal sequences},
  author = {Povel, Dirk-Jan}
}

@article{spadaAuditorySceneFMRI2014,
  title = {The Auditory Scene: {{An fMRI}} Study on Melody and Accompaniment in Professional Pianists},
  volume = {102},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.08.036},
  shorttitle = {The Auditory Scene},
  abstract = {The auditory scene is a mental representation of individual sounds extracted from the summed sound waveform reaching the ears of the listeners. Musical contexts represent particularly complex cases of auditory scenes. In such a scenario, melody may be seen as the main object moving on a background represented by the accompaniment. Both melody and accompaniment vary in time according to harmonic rules, forming a typical texture with melody in the most prominent, salient voice. In the present sparse acquisition functional magnetic resonance imaging study, we investigated the interplay between melody and accompaniment in trained pianists, by observing the activation responses elicited by processing: (1) melody placed in the upper and lower texture voices, leading to, respectively, a higher and lower auditory salience; (2) harmonic violations occurring in either the melody, the accompaniment, or both. The results indicated that the neural activation elicited by the processing of polyphonic compositions in expert musicians depends upon the upper versus lower position of the melodic line in the texture, and showed an overall greater activation for the harmonic processing of melody over accompaniment. Both these two predominant effects were characterized by the involvement of the posterior cingulate cortex and precuneus, among other associative brain regions. We discuss the prominent role of the posterior medial cortex in the processing of melodic and harmonic information in the auditory stream, and propose to frame this processing in relation to the cognitive construction of complex multimodal sensory imagery scenes. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
  issue = {Part 2},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  date = {2014-11-15},
  pages = {764-775},
  keywords = {Salience,brain,Music,auditory stream segregation,Auditory Stimulation,melody,Functional Magnetic Resonance Imaging,Accompaniment,Cingulate Cortex,Harmony},
  author = {Spada, Danilo and Verga, Laura and Iadanza, Antonella and Tettamanti, Marco and Perani, Daniela}
}

@article{uhligImportanceIntegrationTopdown2013,
  title = {The Importance of Integration and Top-down Salience When Listening to Complex Multi-Part Musical Stimuli},
  volume = {77},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2013.03.051},
  abstract = {In listening to multi-part music, auditory streams can be attended to either selectively or globally. More specifically, musicians rely on prioritized integrative attention which incorporates both stream segregation and integration to assess the relationship between concurrent parts. In this fMRI study, we used a piano duet to investigate which factors of a leader–follower relationship between parts grab the listener's attention and influence the perception of multi-part music. The factors considered included the structural relationship between melody and accompaniment as well as the temporal relationship (asynchronies) between parts. The structural relationship was manipulated by cueing subjects to the part of the duet that had to be prioritized. The temporal relationship was investigated by synthetically shifting the onset times of melody and accompaniment to either a consistent melody or accompaniment lead. The relative importance of these relationship factors for segregation and integration as attentional mechanisms was of interest. Participants were required to listen to the cued part and then globally assess if the prioritized stream was leading or following compared to the second stream. Results show that the melody is judged as more leading when it is globally temporally ahead whereas the accompaniment is not judged as leading when it is ahead. This bias may be a result of the interaction of salience of both leader–follower relationship factors. Interestingly, the corresponding interaction effect in the fMRI-data yields an inverse bias for melody in a fronto-parietal attention network. Corresponding parameter estimates within the dlPFC and right IPS show higher neural activity for attending to melody when listening to a performance without a temporal leader, pointing to an interaction of salience of both factors in listening to music. Both frontal and parietal activation implicate segregation and integration mechanisms and a top-down influence of salience on attention and the perception of leader–follower relations in music. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  date = {2013-08-15},
  pages = {52-61},
  keywords = {Attention,Music,Neural Networks,musical stimuli,neural activities,Stimulation},
  author = {Uhlig, Marie and Fairhurst, Merle T. and Keller, Peter E.}
}

@article{doraisamyPolyphonicMusicRetrieval2005,
  title = {Polyphonic {{Music Retrieval}}: {{The N}}-{{Gram Approach}}},
  volume = {39},
  issn = {01635840},
  shorttitle = {Polyphonic {{Music Retrieval}}},
  abstract = {This article presents an abstract of the paper Polyphonic Music Retrieval: The N-Gram Approach, by Shyamala Doraisamy. This Music Information Retrieval (MIR) study investigates the use of n-grams and textual Information Retrieval (IR) approaches for the retrieval and access of polyphonic music data. IR, synonymous with text IR, implies the task of retrieving documents or texts with information content that is relevant to user's information need. Techniques for full-music indexing of polyphonic music data with n-grams are investigated. A method to obtain n-grams from polyphonic music data is introduced. The information content of musical n-grams is extended to include rhythmic information in addition to intervallic information. The evaluation results of the indexing approaches proposed are presented, performed on a test collection developed using approximately 10,000 polyphonic MIDI files. Experiments show that different n-gramming strategies and encoding precision differ widely in their effectiveness. The retrieval performances of monophonic and polyphonic queries made to a polyphonic music collection were investigated using text retrieval performance measures.},
  number = {1},
  journaltitle = {SIGIR Forum},
  shortjournal = {SIGIR Forum},
  date = {2005-06},
  pages = {58-58},
  keywords = {Music,ABSTRACTS,INDEXING,INFORMATION retrieval,INFORMATION science,INFORMATION storage & retrieval systems},
  author = {Doraisamy, Shyamala}
}

@article{unykInfluenceExpectancyMelodic1987,
  title = {The Influence of Expectancy on Melodic Perception},
  volume = {7},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1989-10770-001&site=ehost-live},
  doi = {10.1037/h0094189},
  abstract = {Investigated the influence of fulfillment and violation of melodic expectancies on 27 college-trained musicians' ability to perceive, identify, and recall melodic patterns. Ss registered their expectations of melodic continuations by singing. These expectancies were used to generate 6 types of brief melodies that varied in their relationships to Ss' expectancies (e.g., fulfillment of strong expectancies). Test melodies were presented aurally for transcription. An analysis of variance (ANOVA) revealed that violations of strong expectancies led to more errors than expectancy fulfillment. Contour violations did not lead to more errors than interval-size violations. Error patterns suggest that the salience of contour in melody resisted the influence of expectancy on perception, but did not overcome it. (PsycINFO Database Record (c) 2014 APA, all rights reserved)},
  number = {1},
  journaltitle = {Psychomusicology: A Journal of Research in Music Cognition},
  shortjournal = {Psychomusicology: A Journal of Research in Music Cognition},
  urldate = {2015-06-18},
  year = {Spr  1987},
  pages = {3-23},
  keywords = {Auditory Perception,Music,Expectations,aurally presented violation vs fulfillment of melodic expectancy,college trained musicians,melodic perception},
  author = {Unyk, Anna M. and Carlsen, James C.},
  file = {D\:\\Sauve\\Zotero\\storage\\55SSSJXM\\Unyk and Carlsen - 1987 - The influence of expectancy on melodic perception.pdf}
}

@article{carlsenFactorsWhichInfluence1981,
  title = {Some Factors Which Influence Melodic Expectancy},
  volume = {1},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=1982-09503-001&site=ehost-live},
  doi = {10.1037/h0094276},
  abstract = {Using a sung continuation of 25 interrupted 2-tone melodic beginnings, melodic expectancies were obtained from 91 students (15–23 yrs) in professional music training schools in Hungary, Germany, and the US. Expectancy patterns included both conjunct and disjunct melodic motion ranging up to a minor 7th, but the majority of responses were only half and whole steps. Expectancy generating strength varied considerably between melodic beginnings. Differences in expectancy patterns were found as a function of melodic beginning and cultural milieu, but not of voice register nor of training level. (14 ref) (PsycINFO Database Record (c) 2014 APA, all rights reserved)},
  number = {1},
  journaltitle = {Psychomusicology: A Journal of Research in Music Cognition},
  shortjournal = {Psychomusicology: A Journal of Research in Music Cognition},
  urldate = {2015-06-18},
  year = {Spr  1981},
  pages = {12-29},
  keywords = {Music,Expectations,Stimulus Variability,15–23 yr old music students,interrupted 2-tone melodic beginnings,melodic expectancies},
  author = {Carlsen, James C.},
  file = {D\:\\Sauve\\Zotero\\storage\\AMR6HJ4G\\Carlsen - 1981 - Some factors which influence melodic expectancy.pdf}
}

@article{carlsenMusicalExpectancyPerspectives1982,
  title = {Musical Expectancy: {{Some}} Perspectives},
  volume = {71},
  issn = {0010-9894},
  shorttitle = {Musical Expectancy},
  abstract = {Identifies 2 forms of musical expectancy—one that is logical, knowledge-based, and indirectly experienced and another that is immediate and involves direct experience with the sensations of the moment. The author focuses on the immediate form of expectancy, which depends on previous experience and the schemata of the event itself. An expectancy time line is outlined that includes an antecedent stimulus, an anticipatory state, a consequent stimulus, and a postevent state. Also discussed is the occurrence of an expectancy violation. Implications for musical cognition are addressed in terms of categorical perception and concept formation. (34 ref) (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  journaltitle = {Bulletin of the Council for Research in Music Education},
  shortjournal = {Bulletin of the Council for Research in Music Education},
  year = {Sum  1982},
  pages = {4-14},
  keywords = {Music,Expectations,Perception,indirect vs immediate expectancy in music perception},
  author = {Carlsen, James C.}
}

@article{caclinAcousticCorrelatesTimbre2005,
  langid = {english},
  title = {Acoustic Correlates of Timbre Space Dimensions: A Confirmatory Study Using Synthetic Tones},
  volume = {118},
  issn = {0001-4966},
  shorttitle = {Acoustic Correlates of Timbre Space Dimensions},
  abstract = {Timbre spaces represent the organization of perceptual distances, as measured with dissimilarity ratings, among tones equated for pitch, loudness, and perceived duration. A number of potential acoustic correlates of timbre-space dimensions have been proposed in the psychoacoustic literature, including attack time, spectral centroid, spectral flux, and spectrum fine structure. The experiments reported here were designed as direct tests of the perceptual relevance of these acoustical parameters for timbre dissimilarity judgments. Listeners presented with carefully controlled synthetic tones use attack time, spectral centroid, and spectrum fine structure in dissimilarity rating experiments. These parameters thus appear as major determinants of timbre. However, spectral flux appears as a less salient timbre parameter, its salience depending on the number of other dimensions varying concurrently in the stimulus set. Dissimilarity ratings were analyzed with two different multidimensional scaling models (CLASCAL and CONSCAL), the latter providing psychophysical functions constrained by the physical parameters. Their complementarity is discussed.},
  number = {1},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  date = {2005-07},
  pages = {471-482},
  keywords = {Auditory Perception,Music,Acoustic Stimulation,humans,Adult,Middle Aged,Female,Male,Psychoacoustics,Recognition (Psychology)},
  author = {Caclin, Anne and McAdams, Stephen and Smith, Bennett K. and Winsberg, Suzanne},
  eprinttype = {pmid},
  eprint = {16119366}
}

@article{caclinInteractiveProcessingTimbre2008,
  title = {Interactive Processing of Timbre Dimensions: {{An}} Exploration with Event-Related Potentials},
  volume = {20},
  issn = {0898-929X},
  doi = {10.1162/jocn.2008.20001},
  shorttitle = {Interactive Processing of Timbre Dimensions},
  abstract = {Timbre characterizes the identity of a sound source. On psychoacoustic grounds, it has been described as a multidimensional perceptual attribute of complex sounds. Using Garner's interference paradigm, we found in a previous behavioral study that three timbral dimensions exhibited interactive processing. These timbral dimensions acoustically corresponded to attack time, spectral centroid, and spectrum fine structure. Here, using event-related potentials (ERPs), we sought neurophysiological correlates of the interactive processing of these dimensions of timbre. ERPs allowed us to dissociate several levels of interaction, at both early perceptual and late stimulus identification stages of processing. The cost of filtering out an irrelevant timbral dimension was accompanied by a late negative-going activity, whereas congruency effects between timbre dimensions were associated with interactions in both early sensory and late processing stages. ERPs also helped to determine the similarities and differences in the interactions displayed by the different pairs of timbre dimensions, revealing in particular variations in the latencies at which temporal and spectral timbre dimensions can interfere with the processing of another spectral timbre dimension. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2008-01},
  pages = {49-64},
  keywords = {Evoked Potentials,Auditory Perception,Auditory Stimulation,timbre,acoustics,Cognitive Processes,Neurophysiology,event related potentials,interactive processing,neurophysiological correlates,psychoacoustic grounds},
  author = {Caclin, Anne and McAdams, Stephen and Smith, Bennett K. and Giard, Marie-Hélène}
}

@article{barnesExpectancyAttentionTime2000,
  title = {Expectancy, Attention, and Time},
  volume = {41},
  issn = {0010-0285},
  doi = {10.1006/cogp.2000.0738},
  abstract = {Seven experiments examined the influence of contextual timing manipulations on prospective time judgments. Ss judged durations of standard vs comparison time intervals in the context of a preceding induction (context) sequence. In some experiments, the rate of the induction sequence was systematically manipulated relative to the range of to-be-judged standard time intervals; in others, the induction sequence was omitted. Time judgments were strongly influenced by the rate of an induction sequence with best performance occurring when the standard time interval ended as expected, given context rate. An expectancy profile, in the form of an inverted U, indicated that time estimation accuracy declined systematically as a standard interval differed from a context rate. A similar expectancy profile emerged when the context rate was based on a harmonic subdivision (one-half) of an expected standard interval. Results are discussed in terms of various stimulus-based models of prospective time judgments, including those which appeal to attentional periodicities and entrainment. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  date = {2000-11},
  pages = {254-311},
  keywords = {Attention,adults,time perception,Cognitive Processes,Expectations,Hypothesis Testing,induction of low-level involuntary temporal expectancies in sequence time structure attentional controls,Time Estimation},
  author = {Barnes, Ralph and Jones, Mari Riess}
}

@article{reppEffectsTempoTiming2002,
  title = {Effects of Tempo on the Timing of Simple Musical Rhythms},
  volume = {19},
  issn = {0730-7829},
  doi = {10.1525/mp.2002.19.4.565},
  abstract = {Investigated whether and how the timing of musical rhythms changes with tempo. 12 skilled pianists played a monophonic 8-bar melody in 21 different rhythmic versions at 4 different tempi. Within bars, the rhythms represented 2 isochronous patterns and all possible ordered pairs and triplets of different note values with ratios from the set \{3, 2, 11.\} The 3-note rhythms also occurred in each of 2 meters (3/4 and 6/8). Significant deviations from the notated interval ratios were observed in performances of most rhythms, even at the slowest tempo. The observed ratios of the 2-note rhythms changed little with tempo. By contrast, those of the 3-note rhythms showed increasing assimilation of the 2 longer intervals as tempo increased, while the relative duration of the short interval was barely affected by tempo. The results indicate that at fast tempi, the distinction between 3 different interval durations seems difficult to maintain. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {4},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Sum  2002},
  pages = {565-593},
  keywords = {Musicians,Music,music perception,pianists,Rhythm,time perception,interval durations,simple musical rhythms,Stimulus Intervals,Tempo,timing},
  author = {Repp, Bruno H. and Windsor, W. Luke and Desain, Peter}
}

@article{margulisSelectiveNeurophysiologicResponses2009,
  title = {Selective Neurophysiologic Responses to Music in Instrumentalists with Different Listening Biographies},
  volume = {30},
  issn = {1065-9471},
  doi = {10.1002/hbm.20503},
  abstract = {To appropriately adapt to constant sensory stimulation, neurons in the auditory system are tuned to various acoustic characteristics, such as center frequencies, frequency modulations, and their combinations, particularly those combinations that carry species-specific communicative functions. The present study asks whether such tunings extend beyond acoustic and communicative functions to auditory self-relevance and expertise. More specifically, we examined the role of the listening biography—an individual's long term experience with a particular type of auditory input—on perceptual-neural plasticity. Two groups of expert instrumentalists (violinists and flutists) listened to matched musical excerpts played on the two instruments (J.S. Bach Partitas for solo violin and flute) while their cerebral homodynamic responses were measured using fMRI. Our experimental design allowed for a comprehensive investigation of the neurophysiology (cerebral homodynamic responses as measured by fMRI) of auditory expertise (i.e., when violinists listened to violin music and when flutists listened to flute music) and non expertise (i.e., when subjects listened to music played on the other instrument). We found an extensive cerebral network of expertise, which implicates increased sensitivity to musical syntax (BA 44), timbre (auditory association cortex), and sound-motor interactions (pre central gyrus) when listening to music played on the instrument of expertise (the instrument for which subjects had a unique listening biography). These findings highlight auditory self-relevance and expertise as a mechanism of perceptual-neural plasticity, and implicate neural tuning that includes and extends beyond acoustic and communication-relevant structures. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Human Brain Mapping},
  date = {2009-01},
  pages = {267-275},
  keywords = {Musicians,plasticity,Music,music perception,Auditory Cortex,Neural Plasticity,Magnetic Resonance Imaging,Neurophysiology,auditory expertise,auditory neuroscience,fMRI,sensory neuroscience},
  author = {Margulis, Elizabeth Hellmuth and Mlsna, Lauren M. and Uppunda, Ajith K. and Parrish, Todd B. and Wong, Patrick C. M.}
}

@article{straitMusicalExperienceShapes2010,
  title = {Musical Experience Shapes Top-down Auditory Mechanisms: {{Evidence}} from Masking and Auditory Attention Performance},
  volume = {261},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2009.12.021},
  shorttitle = {Musical Experience Shapes Top-down Auditory Mechanisms},
  abstract = {A growing body of research suggests that cognitive functions, such as attention and memory, drive perception by tuning sensory mechanisms to relevant acoustic features. Long-term musical experience also modulates lower-level auditory function, although the mechanisms by which this occurs remain uncertain. In order to tease apart the mechanisms that drive perceptual enhancements in musicians, we posed the question: do well-developed cognitive abilities fine-tune auditory perception in a top-down fashion? We administered a standardized battery of perceptual and cognitive tests to adult musicians and non-musicians, including tasks either more or less susceptible to cognitive control (e.g., backward versus simultaneous masking) and more or less dependent on auditory or visual processing (e.g., auditory versus visual attention). Outcomes indicate lower perceptual thresholds in musicians specifically for auditory tasks that relate with cognitive abilities, such as backward masking and auditory attention. These enhancements were observed in the absence of group differences for the simultaneous masking and visual attention tasks. Our results suggest that long-term musical practice strengthens cognitive functions and that these functions benefit auditory skills. Musical training bolsters higher-level mechanisms that, when impaired, relate to language and literacy deficits. Thus, musical training may serve to lessen the impact of these deficits by strengthening the corticofugal system for hearing. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1-2},
  journaltitle = {Hearing Research},
  shortjournal = {Hearing Research},
  date = {2010-03},
  pages = {22-29},
  keywords = {acoustics,Musical Ability,auditory masking,cognitive ability,Attentional Capture,auditory attention performance,auditory mechanisms,musical experience,Perceptual Orientation,perceptual process},
  author = {Strait, Dana L. and Kraus, Nina and Parbery-Clark, Alexandra and Ashley, Richard},
  file = {D\:\\Sauve\\Zotero\\storage\\ZJG7BNG8\\Strait et al. - 2010 - Musical experience shapes top-down auditory mechan.pdf;D\:\\Sauve\\Zotero\\storage\\DUCF6PID\\S0378595509003116.html}
}

@article{sandellRolesSpectralCentroid1995,
  title = {Roles for Spectral Centroid and Other Factors in Determining 'blended' Instrument Pairings in Orchestration},
  volume = {13},
  issn = {0730-7829},
  abstract = {Explored principles and generalized rules for selecting instruments that attain a blend (fusion of multiple timbres into a single timbral image) in 3 perceptual experiments with a total of 28 musicians using natural-sounding instrument tones arranged in concurrently sounding pairs. The principal finding concerns the spectral centroid (CN) of the instruments (the midpoint of the spectral energy distribution). Blend worsened as a function of the overall CN height of the combination or as the amount of difference between the CNs of the 2 instruments increased. Slightly different results were found depending on whether the instruments were on the same pitch or separated by a minor third. For unisons, composite CN, attack similarity, and loudness envelope correlation accounted for 51\% of the variance of blend. For minor thirds, CN difference, composite CN, attack similarity, and synchrony of offset accounted for 63\% of the variance of blend. Data show that changes in CN affect blend even when that is the only aspect of the sound that is changing. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {2},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Win  1995},
  pages = {209-246},
  keywords = {Musicians,Auditory Perception,Musical Instruments,perception of blend of instruments,spectral centroid combinations or differences between instruments & pitch & attack similarity & synchrony},
  author = {Sandell, Gregory J.}
}

@article{thompsonModelingPerceivedRelationships1993,
  title = {Modeling Perceived Relationships between Melody, Harmony, and Key},
  volume = {53},
  issn = {0031-5117},
  doi = {10.3758/BF03211711},
  abstract = {Examined perceptual relationships between 4-voice harmonic sequences and single voices in 3 experiments (N\hspace{0.6em}=\hspace{0.6em}55). In Exp 1, Ss rated the extent to which single voices were musically consistent with harmonic sequences. When harmonic sequences did not change key, judgments were influenced by 3 sources of congruency: melody, chord progression, and key structure. When key changes occurred, sensitivity to sources of congruency was reduced. In Exp 2, Ss provided well-formedness ratings of the single voices and harmonic sequences. Consistency ratings were based not merely on well-formedness but on congruency in melody, chord progression, and key structure. In Exp 3, Ss rated the extent of modulation in harmonic sequences and in each voice of the sequences. Discrimination between modulation conditions was greater for single voices than for harmonic sequences. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  date = {1993-01},
  pages = {13-24},
  keywords = {Music,Auditory Discrimination,adults,perceptual relationships between melody & harmony & key in single voice vs 4 voice musical sequence},
  author = {Thompson, William F.}
}

@article{desainFormationRhythmicCategories2003,
  title = {The Formation of Rhythmic Categories and Metric Priming},
  volume = {32},
  issn = {0301-0066},
  doi = {10.1068/p3370},
  abstract = {Two experiments on categorical rhythm perception are reported, the object of which was to investigate how listeners perceive discrete rhythmic categories while listening to rhythms performed on a continuous time scale. This is studied by considering the space of all temporal patterns and how they, in perception, are partitioned into categories. This process of categorisation is formalised as the mapping from the continuous space of a series of time intervals to a discrete, symbolic domain of integer-ratio sequences. The methodological framework uses concepts from mathematics and psychology that allow precise characterisation of the results. In Exp 1, 29 Ss performed an identification task with 66 rhythmic stimuli. The results show that listeners do not just perceive the time intervals between onsets of sounds as placed in a homogeneous continuum. Instead, they can reliably identify rhythmic categories, as a chronotopic time clumping map reveals. In Exp 2, the effect of metric priming was studied by presenting the same stimuli but preceded with a duple or triple metre subdivision. It is shown that presenting patterns in the context of a metre has a large effect on rhythmic categorisation: the presence of a specific musical metre primes the perception of specific rhythmic patterns. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {3},
  journaltitle = {Perception},
  shortjournal = {Perception},
  date = {2003},
  pages = {341-365},
  keywords = {Musicians,Auditory Perception,Music,Rhythm,Pattern Discrimination,categorization,Classification (Cognitive Process),Tempo,auditory rhythm perception,metric priming,Priming,rhythmic meter},
  author = {Desain, Peter and Honing, Henkjan}
}

@article{sadakataBayesianWayRelate2006,
  title = {The {{Bayesian Way}} to {{Relate Rhythm Perception}} and {{Production}}},
  volume = {23},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.23.3.269},
  abstract = {Measurements of the perception and production of simple rhythmic patterns have been shown not to be in line in some cases. In this study it is demonstrated that a Bayesian approach provides a new way of understanding this difference, by formalizing the perceptual competition between mental representations and assuming possible nonuniform a priori probabilities of the rhythmic categories. Thus we can relate the two kinds of information and predict perception data from production data. In this approach, the contrast between rhythm perception and production data, taken from different studies in the literature, was shown almost to disappear, assembling independent prior probabilities from counts of patterns in corpora of musical scores, or from a theoretical measure of rhythmic complexity. The success of this Bayesian formalization may be interpreted as an optimal adaptation of our perceptual system to the environment in which the produced rhythms occur. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  date = {2006-02},
  pages = {269-286},
  keywords = {Auditory Perception,Music,music perception,Rhythm,Bayesian approach,musical scores,rhythm perception,rhythm production,rhythmic categories,rhythmic complexity,Statistical Probability},
  author = {Sadakata, Makiko and Desain, Peter and Honing, Henkjan}
}

@article{jongsmaExpectancyEffectsOmission2005,
  title = {Expectancy Effects on Omission Evoked Potentials in Musicians and Non-Musicians},
  volume = {42},
  issn = {0048-5772},
  doi = {10.1111/j.1469-8986.2005.00269.x},
  abstract = {An expanded omitted stimulus paradigm was investigated to determine whether expectancy would modulate the amplitude of the omission evoked potentials (OEPs). In addition, we examined the effects of musical expertise on OEPs. Trials started with 3-7 beats randomly and contained 5 omitted beats. Three types of trials (n - 90) were presented with 1, 2, or 3 beats occurring between omissions. A tap response at the end of each trial was used to determine timing accuracy. Clear OEPs were observed over midline sites. We found main omission effects with respect to an Nl50 and a P400 OEPs component, such that peak amplitudes diminished whenever the occurrence of an omitted stimulus could be expected. In addition, an N600 OEPs component emerged in response to expectedly omitted stimuli toward the end of each trial within the group of musicians. Thus, musical training seems to lead to more efficient and more refined processing of auditory temporal patterns. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Psychophysiology},
  shortjournal = {Psychophysiology},
  date = {2005-03},
  pages = {191-201},
  keywords = {Musicians,Auditory Evoked Potentials,musical expertise,Experience Level,Time Estimation,auditory expectancy,emplitude,Experimenter Expectations,omission evoked potentials,Omission Training,omitted stimulus,Response Amplitude},
  author = {Jongsma, Marijtje L. A. and Eichele, Tom and Quiroga, Rodrigo Quian and Jenks, Kathleen M. and Desain, Peter and Honing, Henkjan and Van Rijn, Clementina M.}
}

@article{desainComputationalModelingMusic1998,
  title = {Computational Modeling of Music Cognition: {{Problem}} or Solution?},
  volume = {16},
  issn = {0730-7829},
  shorttitle = {Computational Modeling of Music Cognition},
  abstract = {The central purpose of this paper is to elaborate on the methods for computational modeling of music cognition and to show how, although computational modeling should in principle be instrumental in the understanding of the structure of musical knowledge and the processes involved in music cognition, in practice it too often degenerates into a loose "if you want to understand the theory, here, look in my program" approach. The authors show how many issues cannot be decided by inspecting computer programs as they are written nowadays, and indicate a possible solution, giving examples from the field of music cognition research. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Fal  1998},
  pages = {151-166},
  keywords = {cognition,Music,Computational Modeling,Computer Simulation,computational modeling of music cognition,Computer Programming},
  author = {Desain, Peter and Honing, Henkjan and {vanThienen}, Huub and Windsor, Luke}
}

@article{jongsmaRhythmicContextInfluences2004,
  title = {Rhythmic Context Influences the Auditory Evoked Potentials of Musicians and Nonmusicians},
  volume = {66},
  issn = {0301-0511},
  doi = {10.1016/j.biopsycho.2003.10.002},
  abstract = {In this study, we investigated how rhythms are processed in the brain by measuring both behaviourally obtained ratings and auditory evoked potentials (AEPs) from the EEG. We presented probe beats on seven positions within a test bar. Two bars of either a duple- or triple meter rhythm preceded probe beats. We hypothesised that sequential processing would lead to meter effects at the 1/3 and 1/2 bar positions, whereas hierarchical processing would lead to context effects on the 1/3, 1/2 and 2/3 bar positions. We found that metric contexts affected behavioural ratings. This effect was more pronounced for rhythmic experts. In addition, both the AEP P3a and P3b component could be identified. Though metric context affected the P3a amplitudes, group effects were less clear. We found that the AEP P3a component is sensitive to violation of temporal expectancies. In addition, behavioural data and P3a correlation coefficients (CCs) suggest that temporal patterns are processed sequentially in nonmusicians but are processed in a hierarchical way in rhythmic experts. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Biological Psychology},
  shortjournal = {Biological Psychology},
  date = {2004-04},
  pages = {129-152},
  keywords = {brain,Musicians,Auditory Evoked Potentials,nonmusicians,Rhythm,rhythmic context influences},
  author = {Jongsma, Marijtje L. A. and Desain, Peter and Honing, Henkjan}
}

@article{yoshinoCognitiveModelingKey2004,
  title = {Cognitive Modeling of Key Interpretation in Melody Perception},
  volume = {46},
  issn = {0021-5368},
  doi = {10.1111/j.1468-5584.2004.00261.x},
  abstract = {We constructed a computational model that predicted a plausible key and the degree of tonality for any given melody sequence (pitch sequence), through a step-wise or incremental series of computations. In an experiment, 13 musically trained participants listened to 30 Bach fugue subjects and random tone sequences and were asked to identify plausible keys and rate the tonality of the keys. The incremental predictions of our computational model were compared to the behavior of the human participants, as well as to predictions derived from models developed by Krumhansl (1990) and Longuet-Higgins and Steedman (1971). Our computational model provided the best-fitting cognitive model of the process of participants' tonal organization in melody perception. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Japanese Psychological Research},
  shortjournal = {Japanese Psychological Research},
  date = {2004-11},
  pages = {283-297},
  keywords = {Pitch (Frequency),music perception,Models,cognitive modeling,melody perception,tonality,tone sequences},
  author = {Yoshino, Iwao and Abe, Jun-Ichi}
}

@article{songSyncopationScore2013,
  title = {Syncopation and the {{Score}}},
  volume = {8},
  url = {http://dx.doi.org/10.1371/journal.pone.0074692},
  doi = {10.1371/journal.pone.0074692},
  abstract = {The score is a symbolic encoding that describes a piece of music, written according to the conventions of music theory, which must be rendered as sound (e.g., by a performer) before it may be perceived as music by the listener. In this paper we provide a step towards unifying music theory with music perception in terms of the relationship between notated rhythm (i.e., the score) and perceived syncopation. In our experiments we evaluated this relationship by manipulating the score, rendering it as sound and eliciting subjective judgments of syncopation. We used a metronome to provide explicit cues to the prevailing rhythmic structure (as defined in the time signature). Three-bar scores with time signatures of 4/4 and 6/8 were constructed using repeated one-bar rhythm-patterns, with each pattern built from basic half-bar rhythm-components. Our manipulations gave rise to various rhythmic structures, including polyrhythms and rhythms with missing strong- and/or down-beats. Listeners (N{$\mkern1mu$}={$\mkern1mu$}10) were asked to rate the degree of syncopation they perceived in response to a rendering of each score. We observed higher degrees of syncopation in time signatures of 6/8, for polyrhythms, and for rhythms featuring a missing down-beat. We also found that the location of a rhythm-component within the bar has a significant effect on perceived syncopation. Our findings provide new insight into models of syncopation and point the way towards areas in which the models may be improved.},
  number = {9},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2015-06-17},
  date = {2013-09-10},
  pages = {e74692},
  author = {Song, Chunyang and Simpson, Andrew J. R. and Harte, Christopher A. and Pearce, Marcus T. and Sandler, Mark B.},
  file = {D\:\\Sauve\\Zotero\\storage\\CSJ526I3\\Song et al. - 2013 - Syncopation and the Score.pdf}
}

@article{temperleyUnifiedProbabilisticModel2009,
  title = {A {{Unified Probabilistic Model}} for {{Polyphonic Music Analysis}}},
  volume = {38},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298210902928495},
  doi = {10.1080/09298210902928495},
  abstract = {This article presents a probabilistic model of polyphonic music analysis. Taking a note pattern as input, the model combines three aspects of symbolic music analysis—metrical analysis, harmonic analysis, and stream segregation—into a single process, allowing it to capture the complex interactions between these structures. The model also yields an estimate of the probability of the note pattern itself; this has implications for the modelling of music transcription. I begin by describing the generative process that is assumed and the analytical process that is used to infer metrical, harmonic, and stream structures from a note pattern. I then present some tests of the model on metrical analysis and harmonic analysis, and discuss ongoing work to integrate the model into a transcription system.},
  number = {1},
  journaltitle = {Journal of New Music Research},
  urldate = {2015-06-17},
  date = {2009-03-01},
  pages = {3-18},
  author = {Temperley, David},
  file = {D\:\\Sauve\\Zotero\\storage\\WEQHK3ZP\\09298210902928495.html}
}

@article{dibbenPerceptionStructuralStability1999,
  eprinttype = {jstor},
  eprint = {40285794},
  title = {The {{Perception}} of {{Structural Stability}} in {{Atonal Music}}: {{The Influence}} of {{Salience}}, {{Stability}}, {{Horizontal Motion}}, {{Pitch Commonality}}, and {{Dissonance}}},
  volume = {16},
  issn = {0730-7829},
  doi = {10.2307/40285794},
  shorttitle = {The {{Perception}} of {{Structural Stability}} in {{Atonal Music}}},
  abstract = {Two experiments that investigate the perception of structural stability in atonal music are reported. The first experiment suggests that listeners may hear atonal music in terms of the relative structural importance of events and that listeners' hearing is greatly influenced by metrical and durational structure. A second experiment reveals that, even in the absence of clear rhythmic, timbral, dynamic, and motivic information, listeners infer relationships of relative structural stability between events at the musical surface. The effects of three main variables (pitch commonality, horizontal movement, and dissonance) and two salience criteria (register and parallelism) are considered. The results indicate that in the absence of a clearly differentiated surface structure, listeners' judgments of stability are influenced by the dissonance of chords and the horizontal movement of voices. It is concluded that salience (phenomenal accents), voice-leading, and dissonance are potentially important factors in the abstraction of relationships of relative structural importance, and hence to any inference of prolongational structure in atonal music.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1999-04-01},
  pages = {265-294},
  author = {Dibben, Nicola}
}

@article{princePitchTimeTonality2009,
  langid = {english},
  title = {Pitch and Time, Tonality and Meter: How Do Musical Dimensions Combine?},
  volume = {35},
  issn = {1939-1277},
  doi = {10.1037/a0016456},
  shorttitle = {Pitch and Time, Tonality and Meter},
  abstract = {The authors examined how the structural attributes of tonality and meter influence musical pitch-time relations. Listeners heard a musical context followed by probe events that varied in pitch class and temporal position. Tonal and metric hierarchies contributed additively to the goodness-of-fit of probes, with pitch class exerting a stronger influence than temporal position (Experiment 1), even when listeners attempted to ignore pitch (Experiment 2). Speeded classification tasks confirmed this asymmetry. Temporal classification was biased by tonal stability (Experiment 3), but pitch classification was unaffected by temporal position (Experiment 4). Experiments 5 and 6 ruled out explanations based on the presence of pitch classes and temporal positions in the context, unequal stimulus quantity, and discriminability. The authors discuss how typical Western music biases attention toward pitch and distinguish between dimensional discriminability and salience.},
  number = {5},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  date = {2009-10},
  pages = {1598-1617},
  keywords = {Attention,Music,pitch perception,time perception,humans,Adult,Female,Male,Analysis of Variance,Comprehension,Concept Formation,Discrimination (Psychology),Observer Variation,Reference Values,Young Adult},
  author = {Prince, Jon B. and Thompson, William F. and Schmuckler, Mark A.},
  eprinttype = {pmid},
  eprint = {19803659}
}

@article{raphaelFunctionalHarmonicAnalysis2004,
  title = {Functional {{Harmonic Analysis Using Probabilistic Models}}},
  volume = {28},
  issn = {01489267},
  doi = {10.1162/0148926041790676},
  abstract = {This article focuses on the study of algorithms for functional harmonic analysis. The harmonic analysis presented in this article is based on pitch and rhythm. The harmonic analysis is performed on a fixed musical period, or half measure. One significant difference of the approach from other models is the simultaneous recognition of chord and key. The hope here is that the more structured sequence of chord functions will help guide the analysis when the choice of key is ambiguous. For instance, repeated alternation of C-major and F-major triads argues strongly for the key of F when the chords are labeled as dominant and tonic, respectively. In contrast, a random rearrangement of the same pitches is more harmonically ambiguous. In this case, and many others, the identification of the intermediate functional labeling is important for understanding the harmonic structure. our analysis ends with the segmentation of the music into regions, labeled with key and functional chord. Although the authors do not attempt any deeper harmonic analysis, they acknowledge that a full harmonic understanding would require further study. Finally, they acknowledge that the notion of functional harmony is by no means universal. When music does not follow the assumptions of the model, the analysis will of course not be especially meaningful.},
  number = {3},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  date = {2004-23},
  pages = {45-52},
  keywords = {Rhythm,Algorithms,HARMONIC analysis (Mathematics),MATHEMATICAL analysis,PITCH (Rotational geometry)},
  author = {Raphael, Christopher and Stoddard, Joshua}
}

@book{huronToneVoiceDerivation2001,
  title = {Tone and {{Voice}}: {{A Derivation}} of the {{Rules}} of {{Voice}}-{{Leading}} from {{Perceptual Principles}}},
  shorttitle = {Tone and {{Voice}}},
  abstract = {The traditional rules of voice-leading in Western music are explicated using experimentally established perceptual principles. Six core principles are shown to account for the majority of voice-leading rules given in historical and contemporary music theory tracts. These principles are treated in a manner akin to axioms in a formal system from which the traditional rules of voice-leading are derived. Nontraditional rules arising from the derivation are shown to predict formerly unnoticed aspects of voice-leading practice. In addition to the core perceptual principles, several auxiliary principles are described. These auxiliary principles are occasionally linked to voice-leading practice and may be regarded as compositional “options ” that shape the music-making in perceptually unique ways. It is suggested that these auxiliary principles distinguish different types of part writing, such as polyphony, homophony, and close harmony. A theory is proposed to account for the aesthetic origin of voiceleading practices.},
  date = {2001},
  author = {Huron, David},
  file = {D\:\\Sauve\\Zotero\\storage\\9VS46AU2\\Huron - 2001 - Tone and Voice A Derivation of the Rules of Voice.pdf;D\:\\Sauve\\Zotero\\storage\\H4NQRADC\\summary.html}
}

@article{huronWhatMusicalFeature2001,
  title = {What Is a Musical Feature?},
  volume = {7},
  number = {4},
  journaltitle = {Music Theory Online},
  date = {2001},
  author = {Huron, David}
}

@article{huronAvoidancePartcrossingPolyphonic1991,
  title = {The Avoidance of Part-Crossing in Polyphonic Music: {{Perceptual}} Evidence and Musical Practice},
  volume = {9},
  issn = {0730-7829},
  shorttitle = {The Avoidance of Part-Crossing in Polyphonic Music},
  abstract = {A study of part-crossing in 105 polyphonic works by J. S. Bach showed a marked reluctance to have parts cross, even when the effects of pitch distribution (tessitura) were controlled. When the textural density increased beyond 2 concurrent voices, Bach became more vigilant to avoid part-crossing even though an increase in part-crossing was preordained. In light of evidence that perceptual confusion increases with the number of concurrent voices, these results are consistent with the hypothesis that Bach endeavored to minimize perceptual confusion as the density of auditory images increased. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {1},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Fal  1991},
  pages = {93-103},
  keywords = {Music,part crossing in polyphonic works of J. S. Bach},
  author = {Huron, David}
}

@article{huronEmpiricalMusicologyAims2006,
  title = {Empirical {{Musicology}}: {{Aims}}, {{Methods}}, {{Prospects}}/{{Statistics}} in {{Musicology}}},
  volume = {63},
  issn = {00274380},
  shorttitle = {Empirical {{Musicology}}},
  abstract = {The article reviews two books on musicology including "Empirical Musicology: Aims, Methods, Prospects," edited by Eric Clarke and Nicholas Cook and "Statistics in Musicology," by Jan Beran.},
  number = {1},
  journaltitle = {Notes},
  shortjournal = {Notes},
  date = {2006-09},
  pages = {93-95},
  keywords = {BERAN; Jan,BOOKS -- Reviews,CLARKE; Eric,EMPIRICAL Musicology: Aims; Methods; Prospects (Book),NONFICTION,STATISTICS in Musicology (Book)},
  author = {Huron, David and Vandermeer, Philip}
}

@article{huronMusicInformationProcessing2002,
  title = {Music {{Information Processing Using}} the {{Humdrum Toolkit}}: {{Concepts}}, {{Examples}}, and {{Lessons}}},
  volume = {26},
  issn = {01489267},
  doi = {10.1162/014892602760137158},
  shorttitle = {Music {{Information Processing Using}} the {{Humdrum Toolkit}}},
  abstract = {Provides information on the use of Humdrum toolkit in music information processing.  Function and capabilities of Humdrum; Forms of music-related information that are in predefined Humdrum representations; Function of the computer software tools in the Humdrum toolkit.},
  number = {2},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  date = {2002-22},
  pages = {11-26},
  keywords = {Music,INFORMATION storage & retrieval systems,COMPUTER software},
  author = {Huron, David}
}

@book{huronSweetAnticipationMusic2006,
  location = {{Cambridge, MA, US}},
  title = {Sweet Anticipation: {{Music}} and the Psychology of Expectation},
  isbn = {0-262-08345-0},
  shorttitle = {Sweet Anticipation},
  abstract = {The psychological theory of expectation that David Huron proposes in Sweet Anticipation grew out of the author's experimental efforts to understand how music evokes emotions. These efforts evolved into a general theory of expectation that will prove informative to readers interested in cognitive science and evolutionary psychology as well as those interested in music. The book describes a set of psychological mechanisms and illustrates how these mechanisms work in the case of music. All examples of notated music can be heard on the Web. Huron proposes that emotions evoked by expectation involve five functionally distinct response systems: reaction responses (which engage defensive reflexes); tension responses (where uncertainty leads to stress); prediction responses (which reward accurate prediction); imagination responses (which facilitate deferred gratification); and appraisal responses (which occur after conscious thought is engaged). For real-world events, these five response systems typically produce a complex mixture of feelings. The book identifies some of the aesthetic possibilities afforded by expectation and shows how common musical devices (such as syncopation, cadence, meter, tonality, and climax) exploit the psychological opportunities. The theory also provides new insights into the physiological psychology of awe, laughter, and spine-tingling chills. Huron traces the psychology of expectations from patterns of the physical and cultural world through our imperfectly learned heuristics used to predict that world to the phenomenal qualia we experience as we apprehend the world. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (jacket)},
  publisher = {{The MIT Press}},
  date = {2006},
  keywords = {Prediction,Music,Expectations,appraisal responses,Cognitive Appraisal,Emotional Responses,emotions,Imagination,imagination responses,Physiological Correlates,physiology,prediction responses,psychological mechanisms,Psychological Theories,psychological theory of expectation,Psychology,reaction responses,Stress Reactions,tension responses},
  author = {Huron, David}
}

@thesis{kimOnlineProcessingTonal2014,
  location = {{US}},
  title = {Online Processing of Tonal Melodies: {{Effects}} of Harmonic Expectations},
  shorttitle = {Online Processing of Tonal Melodies},
  abstract = {This dissertation explores how listeners perceive implied harmony in real-time while hearing tonal melodies. Previous studies have shown that the processing of chord sequences (explicit harmony) is based on the schematic knowledge of harmonic progression. In particular, the knowledge of harmony generates an expectation of what chord will follow, which facilitates the processing of the subsequent expected harmony. Such processes of harmonic perception is likely to happen in the processing of tonal melodies because they are constructed on the basis of harmonic structure. However, underlying processes of implied harmonic perception remains unexplored. This dissertation, therefore, addresses a set of questions regarding this issue: How is each tone of a tonal melody interpreted and integrated in terms of harmony?; how do harmonic expectations of 'what' chord will follow and 'when' it will occur act in the processing of tonal melodies? In three behavioral experiments, musically-trained participants listened to tonal melodies and responded to target tones by singing their pitches as quickly as possible. The first experiment focused on effects of 'what' and 'when' harmonic expectation. The target tones implied an expected or an unexpected chord; they occurred at expected or unexpected times. In the second experiment, the melodies were accompanied with chords for the purpose of comparing implied and explicit harmonic perception. The third experiment investigated the processing of non-chord tones. The results showed that sing-back reaction times (RTs) were shorter (1) for tones implying an expected chord and (2) for chord changes occurred at expected times, suggesting that harmonic expectations facilitate the processing of tonal melodies. Also, RTs became shorter over the presentation of successive target tones implying the same chord, suggesting that implied harmony becomes clearer as more tones belonging to a single chord are presented. In the fourth experiment, electric brain responses to each tone of a melody were measured. The amplitudes of two ERP components, the ERAN and the N5, reflected the processes of implied harmonic perception, validating the RT results. Both the ERAN and the N5 were larger for tones implying an unexpected chord than for tones implying an expected chord. The N5 was larger for the unexpected early change than for the expected on-time change. (PsycINFO Database Record (c) 2014 APA, all rights reserved)},
  institution = {{ProQuest Information \& Learning}},
  date = {2014},
  keywords = {Music,Auditory Evoked Potentials,Expectations,behavioral experiments,chord sequences,electric brain responses,expected harmony,expected on-time change,explicit harmonic perception,explicit harmony,harmonic expectations,harmonic perception,harmonic progression,harmonic structure,hearing tonal melodies,Online processing},
  author = {Kim, Jung Nyo}
}

@article{kimMelodyEffectsERANm2014,
  title = {Melody Effects on {{ERANm}} Elicited by Harmonic Irregularity in Musical Syntax},
  volume = {1560},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2014.02.045},
  abstract = {Recent studies have reported that early right anterior negativity (ERAN) and its magnetic counterpart (ERANm) are evoked by harmonic irregularity in Western tonal music; however, those studies did not control for differences of melody. Because melody and harmony have an interdependent relationship and because melody (in this study melody is represented by the highest voice part) in a chord sequence may dominate, there is controversy over whether ERAN (or ERANm) changes arise from melody or harmony differences. To separate the effects of melody differences and harmonic irregularity on ERANm responses, we designed two magnetoencephalography experiments and behavioral test. Participants were presented with three types of chord progression sequences (Expected, Intermediate, and Unexpected) with different harmonic regularities in which melody differences were or were not controlled. In the uncontrolled melody difference experiment, the unexpected chord elicited a significantly largest ERANm, but in the controlled melody difference experiment, the amplitude of the ERANm peak did not differ among the three conditions. However, ERANm peak latency was delayed more than that in the uncontrolled melody difference experiment. The behavioral results show the difference between the two experiments even if harmonic irregularity was discriminated in the uncontrolled melody difference experiment. In conclusion, our analysis reveals that there is a relationship between the effects of harmony and melody on ERANm. Hence, we suggest that a melody difference in a chord progression is largely responsible for the observed changes in ERANm, reaffirming that melody plays an important role in the processing of musical syntax. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  date = {2014-04-29},
  pages = {36-45},
  keywords = {Music,melody,mismatch negativity,magnetoencephalography,chord progression,early right anterior negativity,musical syntax,Syntax},
  author = {Kim, Chan Hee and Lee, Sojin and Kim, June Sic and Seol, Jaeho and Yi, Suk Won and Chung, Chun Kee}
}

@article{vuustRhythmicComplexityPredictive2014,
  langid = {english},
  title = {Rhythmic Complexity and Predictive Coding: A Novel Approach to Modeling Rhythm and Meter Perception in Music},
  volume = {5},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.01111},
  shorttitle = {Rhythmic Complexity and Predictive Coding},
  abstract = {Musical rhythm, consisting of apparently abstract intervals of accented temporal events, has a remarkable capacity to move our minds and bodies. How does the cognitive system enable our experiences of rhythmically complex music? In this paper, we describe some common forms of rhythmic complexity in music and propose the theory of predictive coding (PC) as a framework for understanding how rhythm and rhythmic complexity are processed in the brain. We also consider why we feel so compelled by rhythmic tension in music. First, we consider theories of rhythm and meter perception, which provide hierarchical and computational approaches to modeling. Second, we present the theory of PC, which posits a hierarchical organization of brain responses reflecting fundamental, survival-related mechanisms associated with predicting future events. According to this theory, perception and learning is manifested through the brain's Bayesian minimization of the error between the input to the brain and the brain's prior expectations. Third, we develop a PC model of musical rhythm, in which rhythm perception is conceptualized as an interaction between what is heard ("rhythm") and the brain's anticipatory structuring of music ("meter"). Finally, we review empirical studies of the neural and behavioral effects of syncopation, polyrhythm and groove, and propose how these studies can be seen as special cases of the PC theory. We argue that musical rhythm exploits the brain's general principles of prediction and propose that pleasure and desire for sensorimotor synchronization from musical rhythm may be a result of such mechanisms.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  date = {2014},
  pages = {1111},
  author = {Vuust, Peter and Witek, Maria A. G.},
  eprinttype = {pmid},
  eprint = {25324813},
  pmcid = {PMC4181238}
}

@article{temperleyAlgorithmHarmonicAnalysis1997,
  title = {An Algorithm for Harmonic Analysis},
  volume = {15},
  issn = {0730-7829},
  abstract = {An algorithm is proposed for performing harmonic analysis of tonal music. The algorithm begins with a representation of a piece as pitches and durations; it generates a representation in which the piece is divided into segments labeled with roots. This is a project of psychological interest, because much evidence exists that harmonic analysis is performed by trained and untrained listeners during listening; however, the perspective of the current project is computational rather than psychological, simply examining what has to be done computationally to produce "correct" analyses for pieces. One of the major innovations of the project is that pitches and chords are both represented on a spatial representation known as the "line of fifths", this is similar to the circle of fifths except that distinctions are made between different spellings of the same pitch class. The algorithm uses preference rules to evaluate different possible interpretations, selecting the interpretation that most satisfies the preference rules. The algorithm has been computationally implemented; examples of the program's output are given and discussed. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  year = {Fal  1997},
  pages = {31-68},
  keywords = {music perception,pitch perception,Algorithms,algorithm for harmonic analysis of tonal music},
  author = {Temperley, David}
}

@article{temperleyEvaluationSystemMetrical2004,
  title = {An {{Evaluation System}} for {{Metrical Models}}},
  volume = {28},
  issn = {01489267},
  doi = {10.1162/0148926041790621},
  abstract = {This article proposes an alternative approach to metrical evaluation. This builds on the score-time idea, but allows multiple metrical levels and also permits a more flexible comparison of metrical analyses. It should be stated at the outset that the system is intended primarily for symbolic-input models. It is hoped that the system presented here will facilitate the testing and comparison of metrical models. Although the system does have certain limitations, as discussed above, it offers a rational way of comparing metrical structures whose results accord reasonably well with intuition. The Kosta-Payne Corpus presented here also has limitations, especially the fact that it represents only common-practice music, which may not be fair to the aims of some metrical models. Of course, the note-address system could also be used with a different corpus, providing that the corpus is annotated with note-address information. Notice that the system does not in any way require note addresses with five levels; it could function perfectly well with, say, three levels. Perhaps the greatest limitation of the note-address system is that it is limited to symbolic input. This is unfortunate, given the considerable number of models proposed in recent years that take audio data as input. The note-address system could, in principle, be used with audio models.},
  number = {3},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  date = {2004-23},
  pages = {28-44},
  keywords = {Music,Rhythm,COMPUTER music,ELECTRONIC music,MUSICAL meter & rhythm},
  author = {Temperley, David}
}

@article{temperleyProbabilisticModelMelody2008,
  title = {A Probabilistic Model of Melody Perception},
  volume = {32},
  issn = {0364-0213},
  doi = {10.1080/03640210701864089},
  abstract = {This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. The model uses Bayesian reasoning: For any “surface” pattern and underlying “structure,” we can infer the structure maximizing P(structure|surface) based on knowledge of P(surface, structure). The probability of the surface can then be calculated as ∑ P(surface, structure), summed over all structures. In this case, the surface is a pattern of notes; the structure is a key. A generative model is proposed, based on three principles: (a) melodies tend to remain within a narrow pitch range; (b) note-to-note intervals within a melody tend to be small; and (c) notes tend to conform to a distribution (or key profile) that depends on the key. The model is tested in three ways. First, it is tested on its ability to identify the keys of a set of folk song melodies. Second, it is tested on a melodic expectation task in which it must judge the probability of different notes occurring given a prior context; these judgments are compared with perception data from a melodic expectation experiment. Finally, the model is tested on its ability to detect incorrect notes in melodies by assigning them lower probabilities than the original versions. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Cognitive Science},
  shortjournal = {Cognitive Science},
  date = {2008-03},
  pages = {418-444},
  keywords = {music perception,Cognitive Processes,Perception,pitch,Models,Statistical Probability,melody perception,Bayesian reasoning,probabilistic models,Reasoning,Speech Pitch},
  author = {Temperley, David}
}

@book{temperleyMusicProbability2007,
  location = {{Cambridge, MA, US}},
  title = {Music and Probability},
  isbn = {0-262-20166-6},
  abstract = {In Music and probability, David Temperley explores issues in music perception and cognition from a probabilistic perspective. The application of probabilistic ideas to music has been pursued only sporadically over the past four decades, but the time is ripe, Temperley argues, for a reconsideration of how probabilities shape music perception and even music itself. Recent advances in the application of probability theory to other domains of cognitive modeling, coupled with new evidence and theoretical insights about the working of the musical mind, have laid the groundwork for more fruitful investigations. Temperley proposes computational models for two basic cognitive processes, the perception of key and the perception of meter, using techniques of Bayesian probabilistic modeling. Drawing on his own research and surveying recent work by others, Temperley explores a range of further issues in music and probability, including transcription, phrase perception, pattern perception, harmony, improvisation, and musical styles. Music and probability--the first full-length book to explore the application of probabilistic techniques to musical issues--includes a concise survey of probability theory, with simple examples and a discussion of its application in other domains. Temperley relies most heavily on a Bayesian approach, which not only allows him to model the perception of meter and tonality but also sheds light on such perceptual processes detection, expectation, and pitch identification. Bayesian techniques also provide insights into such subtle and advanced issues as musical ambiguity, tension, and "grammaticality," and lead to interesting and novel predictions about compositional practice and differences between musical styles. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (jacket)},
  publisher = {{MIT Press}},
  date = {2007},
  keywords = {cognition,Music,Pitch (Frequency),melody,expectation,music perception,Rhythm,Cognitive Processes,Expectations,Mathematical Modeling,Perception,pitch,Statistical Probability,Bayesian probabilistic modeling,communicative pressure,composition,computational models,error detection,Errors,key,meter,style},
  author = {Temperley, David}
}

@article{temperleyModelingCommonpracticeRhythm2010,
  title = {Modeling Common-Practice Rhythm},
  volume = {27},
  issn = {0730-7829},
  doi = {10.1525/mp.2010.27.5.355},
  abstract = {This study explores ways of modeling the compositional processes involved in common-practice rhythm (as represented by European classical music and folk music). Six probabilistic models of rhythm were evaluated using the method of cross-entropy: according to this method, the best model is the one that assigns the highest probability to the data. Two corpora were used: a corpus of European folk songs (the Essen Folksong Collection) and a corpus of Mozart and Haydn string quartets. The model achieving lowest cross-entropy was the First-Order Metrical Duration Model, which chooses a metrical position for each note conditional on the position of the previous note. Second best was the Hierarchical Position Model, which decides at each beat whether or not to generate a note there, conditional on the note status of neighboring strong beats (i.e., whether or not they contain notes).When complexity (number of parameters) is also considered, it is argued that the Hierarchical Position Model is preferable overall. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {5},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  date = {2010-06},
  pages = {355-376},
  keywords = {Music,Rhythm,Mathematical Modeling,common practice rhythm},
  author = {Temperley, David}
}

@incollection{temperleyComputationalModelsMusic2013,
  location = {{San Diego, CA, US}},
  title = {Computational Models of Music Cognition},
  isbn = {978-0-12-381460-9},
  abstract = {In recent decades, computational research has assumed an increasingly important role in the study of cognition. Computer modeling is generally regarded as one of the three main approaches along with experimental psychology and neuroscience—that comprise the interdisciplinary field of "cognitive science." It is no surprise, then, that computational work has become an important part of the field of music cognition as well. What follows is a survey of some important research in computational modeling of music cognition. We begin with problems of perception or information processing—problems of extracting various kinds of information from music as it is heard. Here we focus primarily on two especially well-studied problems, keyfinding and meter-finding, but briefly consider several other problems as well. We then turn to three other broad issues: the modeling of musical experience, the modeling of performance, and the modeling of composition. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (chapter)},
  booktitle = {The Psychology of Music (3rd Ed.).},
  publisher = {{Elsevier Academic Press}},
  date = {2013},
  pages = {327-368},
  keywords = {Music,music perception,Cognitive Processes,Computational Modeling,Simulation,musical experience,Psychology,modeling of composition,modeling of performance,music cognition,Performance},
  author = {Temperley, David},
  editor = {Deutsch, Diana and Deutsch, Diana (Ed)}
}

@article{acevedoEffectsMetricalEncoding2014,
  title = {Effects of Metrical Encoding on Melody Recognition},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2014.31.4.372},
  abstract = {We report two experiments exploring whether matched metrical and motivic structure facilitate the recognition of melodic patterns. Eight tonal melodies were composed from binary (four-note) or ternary (three-note) motivic patterns, and were each presented within a metrical context that either matched or mismatched the pattern. On each trial, participants heard patterns twice and performed a same-different task; in half the trials, one pitch in the second presentation was altered. Performance was analyzed using signal detection analyses of sensitivity and response bias. In Experiment 1, expert listeners showed greater sensitivity to pitch change when metrical context matched motivic pattern structure than when they conflicted (an effect of metrical encoding) and showed no response bias. Novice listeners, however, did not show an effect of metrical encoding, exhibiting lower sensitivity and a bias toward responding “same.” In a second experiment using only novices, each trial contained five presentations of the standard followed by one presentation of the comparison. Sensitivity to changes improved relative to Experiment 1: evidence for metrical encoding – in the form of reduced response bias when meter and motive matched – was found. Results support the metrical encoding hypothesis and suggest that the use of metrical encoding may develop with expertise. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Music Perception},
  shortjournal = {Music Perception},
  date = {2014-04},
  pages = {372-386},
  keywords = {Auditory Perception,Music,music perception,Human Information Storage,meter,melody recognition,metrical encoding,Signal Detection (Perception),signal detection analyses,tonal melody},
  author = {Acevedo, Stefanie and Temperley, David and Pfordresher, Peter Q.}
}

@article{alluriLargescaleBrainNetworks2012,
  title = {Large-Scale Brain Networks Emerge from Dynamic Processing of Musical Timbre, Key and Rhythm},
  volume = {59},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2011.11.019},
  abstract = {We investigated the neural underpinnings of timbral, tonal, and rhythmic features of a naturalistic musical stimulus. Participants were scanned with functional Magnetic Resonance Imaging (fMRI) while listening to a stimulus with a rich musical structure, a modern tango. We correlated temporal evolutions of timbral, tonal, and rhythmic features of the stimulus, extracted using acoustic feature extraction procedures, with the fMRI time series. Results corroborate those obtained with controlled stimuli in previous studies and highlight additional areas recruited during musical feature processing. While timbral feature processing was associated with activations in cognitive areas of the cerebellum, and sensory and default mode network cerebrocortical areas, musical pulse and tonality processing recruited cortical and subcortical cognitive, motor and emotion-related circuits. In sum, by combining neuroimaging, acoustic feature extraction and behavioral methods, we revealed the large-scale cognitive, motor and limbic brain circuitry dedicated to acoustic feature processing during listening to a naturalistic stimulus. In addition to these novel findings, our study has practical relevance as it provides a powerful means to localize neural processing of individual acoustical features, be it those of music, speech, or soundscapes, in ecological settings. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  date = {2012-02-15},
  pages = {3677-3689},
  keywords = {brain,acoustics,Functional Magnetic Resonance Imaging,acoustic feature,amygdala,Biological Neural Networks,brain networks,dynamic processing,functional MRI,musical timbre},
  author = {Alluri, Vinoo and Toiviainen, Petri and Jääskeläinen, Iiro P. and Glerean, Enrico and Sams, Mikko and Brattico, Elvira}
}

@article{pressnitzerAuditorySceneAnalysis2011,
  title = {Auditory Scene Analysis: {{The}} Sweet Music of Ambiguity},
  volume = {5},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2011.00158},
  shorttitle = {Auditory Scene Analysis},
  abstract = {In this review paper aimed at the non-specialist, we explore the use that neuroscientists and musicians have made of perceptual illusions based on ambiguity. The pivotal issue is auditory scene analysis (ASA), or what enables us to make sense of complex acoustic mixtures in order to follow, for instance, a single melody in the midst of an orchestra. In general, ASA uncovers the most likely physical causes that account for the waveform collected at the ears. However, the acoustical problem is ill-posed and it must be solved from noisy sensory input. Recently, the neural mechanisms implicated in the transformation of ambiguous sensory information into coherent auditory scenes have been investigated using so-called bistability illusions (where an unchanging ambiguous stimulus evokes a succession of distinct percepts in the mind of the listener). After reviewing some of those studies, we turn to music, which arguably provides some of the most complex acoustic scenes that a human listener will ever encounter. Interestingly, musicians will not always aim at making each physical source intelligible, but rather express one or more melodic lines with a small or large number of instruments. By means of a few musical illustrations and by using a computational model inspired by neuro-physiological principles, we suggest that this relies on a detailed (if perhaps implicit) knowledge of the rules of ASA and of its inherent ambiguity. We then put forward the opinion that some degree perceptual ambiguity may participate in our appreciation of music. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Frontiers in Human Neuroscience},
  date = {2011-12-14},
  keywords = {Musicians,Auditory Perception,Music,auditory scene analysis,Illusions (Perception),ambiguity,Neurosciences,neuroscientists,perceptual illusions},
  author = {Pressnitzer, Daniel and Suied, Clara and Shamma, Shihab A.}
}

@article{horvathSensoryERPEffects2014,
  title = {Sensory {{ERP}} Effects in Auditory Distraction: {{Did}} We Miss the Main Event?},
  volume = {78},
  issn = {0340-0727},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=psyh&AN=2013-28166-001&site=ehost-live},
  doi = {10.1007/s00426-013-0507-7},
  shorttitle = {Sensory {{ERP}} Effects in Auditory Distraction},
  abstract = {Event-related potentials (ERPs) offer unique insights into processes related to involuntary attention changes triggered by rare, unpredictably occurring sensory events, that is, distraction. Contrasting ERPs elicited by distracters and frequent standard stimuli in oddball paradigms allowed the formulation of a three-stage model describing distraction-related processing: first, the distracting event is highlighted by a sensory filter. Second, attention is oriented towards the event, and finally, the task-optimal attention set is restored, or task priorities are changed. Although this model summarizes how distracting stimulus information is processed, not much is known about the cost of taking this exceptional route of processing. The present study demonstrates the impact of distraction on sensory processing. Participants performed a Go/NoGo tone-duration discrimination task, with infrequent pitch distracters. In the two parts of the experiment the duration-response mapping was reversed. Contrasts of distracter and standard ERPs revealed higher P3a- and reorienting negativity amplitudes for short than for long tones, independently from response type. To understand the cause of these asymmetries, short vs. long ERP contrasts were calculated. The ERP pattern showed that short standards elicited an attention-dependent offset response, which was abolished for short distracters. That is, the apparent P3a- and RON enhancements were caused by the removal of a task-related attentional sensory enhancement. This shows that the disruption of task-optimal attention set precedes the elicitation of the P3a, which suggests that P3a does not reflect a process driving the initial distraction-related attention change. (PsycINFO Database Record (c) 2014 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  urldate = {2015-06-17},
  date = {2014-05},
  pages = {339-348},
  keywords = {Attention,Evoked Potentials,Auditory Perception,auditory distraction,Distraction,Afferent Pathways,event-related potentials,involuntary attention,sensory processing},
  author = {Horváth, János},
  file = {D\:\\Sauve\\Zotero\\storage\\524N2EGD\\Horváth - 2014 - Sensory ERP effects in auditory distraction Did w.pdf}
}

@article{backerAttentionMemoryOrienting2014,
  title = {Attention to Memory: Orienting Attention to Sound Object Representations},
  volume = {78},
  issn = {03400727},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=95659905&site=ehost-live},
  doi = {10.1007/s00426-013-0531-7},
  shorttitle = {Attention to Memory},
  abstract = {Despite a growing acceptance that attention and memory interact, and that attention can be focused on an active internal mental representation (i.e., reflective attention), there has been a paucity of work focusing on reflective attention to 'sound objects' (i.e., mental representations of actual sound sources in the environment). Further research on the dynamic interactions between auditory attention and memory, as well as its degree of neuroplasticity, is important for understanding how sound objects are represented, maintained, and accessed in the brain. This knowledge can then guide the development of training programs to help individuals with attention and memory problems. This review article focuses on attention to memory with an emphasis on behavioral and neuroimaging studies that have begun to explore the mechanisms that mediate reflective attentional orienting in vision and more recently, in audition. Reflective attention refers to situations in which attention is oriented toward internal representations rather than focused on external stimuli. We propose four general principles underlying attention to short-term memory. Furthermore, we suggest that mechanisms involved in orienting attention to visual object representations may also apply for orienting attention to sound object representations.},
  number = {3},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  urldate = {2015-06-17},
  date = {2014-05},
  pages = {439-452},
  keywords = {Attention,Auditory Perception,Memory,BRAIN imaging,MENTAL representation,NEUROPLASTICITY,SOUNDS},
  author = {Backer, Kristina and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\XRJJ8B5U\\Backer and Alain - 2014 - Attention to memory orienting attention to sound .pdf}
}

@article{backerNeuralDynamicsUnderlying2015,
  title = {Neural Dynamics Underlying Attentional Orienting to Auditory Representations in Short-Term Memory},
  volume = {35},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.1487-14.2015},
  abstract = {Sounds are ephemeral. Thus, coherent auditory perception depends on “hearing” back in time: retrospectively attending that which was lost externally but preserved in short-term memory (STM). Current theories of auditory attention assume that sound features are integrated into a perceptual object, that multiple objects can coexist in STM, and that attention can be deployed to an object in STM. Recording electroencephalography from humans, we tested these assumptions, elucidating feature-general and feature-specific neural correlates of auditory attention to STM. Alpha/beta oscillations and frontal and posterior event-related potentials indexed feature-general top-down attentional control to one of several coexisting auditory representations in STM. Particularly, task performance during attentional orienting was correlated with alpha/low-beta desynchronization (i.e., power suppression). However, attention to one feature could occur without simultaneous processing of the second feature of the representation. Therefore, auditory attention to memory relies on both feature-specific and feature-general neural dynamics. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
  number = {3},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {The Journal of Neuroscience},
  date = {2015-01-21},
  pages = {1307-1318},
  keywords = {Attention,electroencephalography,Oscillatory Network,Short Term Memory,EEG,Neurons,alpha/beta oscillations,auditory,feature,object,Object Recognition},
  author = {Backer, Kristina C. and Binns, Malcolm A. and Alain, Claude}
}

@article{lawoAttentionActionRole2015,
  title = {Attention and Action: {{The}} Role of Response Mappings in Auditory Attention Switching},
  volume = {27},
  issn = {2044-5911},
  doi = {10.1080/20445911.2014.995669},
  shorttitle = {Attention and Action},
  abstract = {Switching auditory attention incurs a performance decrement (i.e. auditory attention-switch costs). Using an auditory attention-switching paradigm, we aimed to generalise these across different response mappings. In all three experiments, two number words, spoken by a female and male speaker, were presented dichotically via headphones. A visual cue indicated the gender of the to-be-attended speaker in each trial. The task was a magnitude judgement of the relevant number word (i.e. smaller vs. larger than 5). We additionally varied the interval between cue onset and auditory stimulus onset (cue-stimulus interval) to explore cue-based preparatory effects. In Experiment 1, attention switching was more costly with direct-verbal responses (e.g. ‘smaller’) than in the shadowing task (e.g. ‘three’). In Experiment 2, performance was largely similar for direct-verbal responses and abstract-verbal responses (e.g. ‘left’). In Experiment 3, performance was generally worse with abstract-verbal responses than with abstract-manual responses (e.g. left key press) and auditory attention-switch costs were similar for both response mappings. Overall, auditory switch costs occurred more or less invariably across response mappings in categorical (magnitude) judgements suggesting a minor role of the response mapping in auditory attention switching. Furthermore, verbal identity-based judgements (i.e. shadowing) generally seem to benefit from ideomotor compatibility. (PsycINFO Database Record (c) 2015 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Journal of Cognitive Psychology},
  shortjournal = {Journal of Cognitive Psychology},
  date = {2015-02},
  pages = {194-206},
  keywords = {Auditory Perception,selective attention,Responses,Auditory selective attention,Dichotic listening,Dichotic Stimulation,Response mapping,Shadowing,Task switching},
  author = {Lawo, Vera and Koch, Iring}
}

@article{lawoDissociableEffectsAuditory2014,
  title = {Dissociable Effects of Auditory Attention Switching and Stimulus-Response Compatibility},
  volume = {78},
  issn = {03400727},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=95659911&site=ehost-live},
  doi = {10.1007/s00426-014-0545-9},
  abstract = {Using a task-switching variant of dichotic listening, we examined the ability to intentionally switch auditory attention between two speakers. We specifically focused on possible interactions with stimulus-response compatibility. In each trial, two words, one spoken by a male and another by a female, were presented dichotically via headphones. In one experimental group, two animal names were presented, and the relevant animal had to be judged as smaller or larger than a sheep by pressing a left or right response key. In another group, two number words were presented and had to be judged as smaller or larger than 5. In each trial, a visual cue indicated the gender of the relevant speaker. Performance was worse when the gender of the relevant speaker switched from trial to trial. These switch costs were larger for animal names than for number words, suggesting stronger interference with slower access to semantic categories. Responses were slower if the side of the target stimulus (as defined by the relevant gender) was spatially incompatible with the required response (as defined by the size judgment). This stimulus-response compatibility effect did not differ across stimulus material and did not interact with attentional switch costs. These results indicate that auditory switch costs and stimulus-response compatibility effects are dissociable, referring to target selection and response selection, respectively.},
  number = {3},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  urldate = {2015-06-17},
  date = {2014-05},
  pages = {379-386},
  keywords = {Auditory Perception,AUDITORY attention,DICHOTIC listening tests,HEADPHONES,PERFORMANCE evaluation,SEMANTICS,TASK performance},
  author = {Lawo, Vera and Koch, Iring},
  file = {D\:\\Sauve\\Zotero\\storage\\BCDXFU4E\\Lawo and Koch - 2014 - Dissociable effects of auditory attention switchin.pdf}
}

@article{kochSwitchingCocktailParty2011,
  title = {Switching in the Cocktail Party: {{Exploring}} Intentional Control of Auditory Selective Attention},
  volume = {37},
  issn = {0096-1523},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2011-09201-001&site=ehost-live},
  doi = {10.1037/a0022189},
  shorttitle = {Switching in the Cocktail Party},
  abstract = {Using a novel variant of dichotic selective listening, we examined the control of auditory selective attention. In our task, subjects had to respond selectively to one of two simultaneously presented auditory stimuli (number words), always spoken by a female and a male speaker, by performing a numerical size categorization. The gender of the task-relevant speaker could change, as indicated by a visual cue prior to auditory stimulus onset. Three experiments show clear performance costs with instructed attention switches. Experiment 2 varied the cuing interval to examine advance preparation for an attention switch. Experiment 3 additionally isolated auditory switch costs from visual cue priming by using two cues for each gender, so that gender repetition could be indicated by a changed cue. Experiment 2 showed that switch costs decreased with prolonged cuing intervals, but Experiment 3 revealed that preparation did not affect auditory switch costs but only visual cue priming. Moreover, incongruent numerical categories in competing auditory stimuli produced interference and substantially increased error rates, suggesting continued processing of task-relevant information that often leads to responding to the incorrect auditory source. Together, the data show clear limitations in advance preparation of auditory attention switches and suggest a considerable degree of inertia in intentional control of auditory selection criteria. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  shortjournal = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2015-06-17},
  date = {2011-08},
  pages = {1140-1147},
  keywords = {Auditory Perception,Cues,selective attention,intention,Auditory selective attention,Dichotic listening,Task switching,advance preparation,Code Switching,switch costs},
  author = {Koch, Iring and Lawo, Vera and Fels, Janina and Vorländer, Michael},
  file = {D\:\\Sauve\\Zotero\\storage\\MNIKXJV2\\Koch et al. - 2011 - Switching in the cocktail party Exploring intenti.pdf}
}

@article{maidhofProcessingExpectancyViolations2010,
  title = {Processing Expectancy Violations during Music Performance and Perception: {{An ERP}} Study},
  volume = {22},
  issn = {0898-929X},
  doi = {10.1162/jocn.2009.21332},
  shorttitle = {Processing Expectancy Violations during Music Performance and Perception},
  abstract = {Musicians are highly trained motor experts with pronounced associations between musical actions and the corresponding auditory effects. However, the importance of auditory feedback for music performance is controversial, and it is unknown how feedback during music performance is processed. The present study investigated the neural mechanisms underlying the processing of auditory feedback manipulations in pianists. To disentangle effects of action-based and perception-based expectations, we compared feedback manipulations during performance to the mere perception of the same stimulus material. In two experiments, pianists performed bimanually sequences on a piano,while at random positions, the auditory feedback of single notes was manipulated, thereby creating a mismatch between an expected and actually perceived action effect (action condition). In addition, pianists listened to tone sequences containing the same manipulations (perception condition). The manipulations in the perception condition were either task-relevant (Experiment 1) or task-irrelevant (Experiment 2). In action and perception conditions, event-related potentials elicited by manipulated tones showed an early fronto-central negativity around 200 msec, presumably reflecting a feedback ERN/N200, followed by a positive deflection (P3a). The early negativity was more pronounced during the action compared to the perception condition. This shows that during performance, the intention to produce specific auditory effects leads to stronger expectancies than the expectancies built up during music perception. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {10},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2010-10},
  pages = {2401-2413},
  keywords = {Evoked Potentials,music perception,event related potentials,Expectations,auditory feedback,expectancy violations,music performance},
  author = {Maidhof, Clemens and Vavatzanidis, Niki and Prinz, Wolfgang and Rieger, Martina and Koelsch, Stefan}
}

@article{garridoEvokedBrainResponses2007,
  title = {Evoked Brain Responses Are Generated by Feedback Loops},
  volume = {104},
  issn = {0027-8424},
  doi = {10.1073/pnas.0706274105},
  abstract = {Neuronal responses to stimuli, measured electrophysiologically, unfold over several hundred milliseconds. Typically, they show characteristic waveforms with early and late components. It is thought that early or exogenous components reflect a perturbation of neuronal dynamics by sensory input bottom-up processing. Conversely, later, endogenous components have been ascribed to recurrent dynamics among hierarchically disposed cortical processing levels, top-down effects. Here, we show that evoked brain responses are generated by recurrent dynamics in cortical networks, and late components of event-related responses are mediated by backward connections. This evidence is furnished by dynamic causal modeling of mismatch responses, elicited in an oddball paradigm. We used the evidence for models with and without backward connections to assess their likelihood as a function of peristimulus time and show that backward connections are necessary to explain late components. Furthermore, we were able to quantify the contribution of backward connections to evoked responses and to source activity, again as a function of peristimulus time. These results link a generic feature of brain responses to changes in the sensorium and a key architectural component of functional anatomy; namely, backward connections are necessary for recurrent interactions among levels of cortical hierarchies. This is the theoretical cornerstone of most modern theories of perceptual inference and learning. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {52},
  journaltitle = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  date = {2007-12},
  pages = {20961-20966},
  keywords = {brain,Evoked Potentials,electrophysiology,Neurons,Feedback,evoked brain responses,feedback loops,neuronal responses},
  author = {Garrido, Marta I. and Kilner, James M. and Kiebel, Stefan J. and Friston, Karl J.}
}

@article{griffithsApproachesCorticalAnalysis2007,
  langid = {english},
  title = {Approaches to the Cortical Analysis of Auditory Objects},
  volume = {229},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2007.01.010},
  abstract = {We describe work that addresses the cortical basis for the analysis of auditory objects using 'generic' sounds that do not correspond to any particular events or sources (like vowels or voices) that have semantic association. The experiments involve the manipulation of synthetic sounds to produce systematic changes of stimulus features, such as spectral envelope. Conventional analyses of normal functional imaging data demonstrate that the analysis of spectral envelope and perceived timbral change involves a network consisting of planum temporale (PT) bilaterally and the right superior temporal sulcus (STS). Further analysis of imaging data using dynamic causal modelling (DCM) and Bayesian model selection was carried out in the right hemisphere areas to determine the effective connectivity between these auditory areas. Specifically, the objective was to determine if the analysis of spectral envelope in the network is done in a serial fashion (that is from HG to PT to STS) or parallel fashion (that is PT and STS receives input from HG simultaneously). Two families of models, serial and parallel (16 in total) that represent different hypotheses about the connectivity between HG, PT and STS were selected. The models within a family differ with respect to the pathway that is modulated by the analysis of spectral envelope. After the models are identified, Bayesian model selection procedure is then used to select the 'optimal' model from the specified models. The data strongly support a particular serial model containing modulation of the HG to PT effective connectivity during spectral envelope variation. Parallel work in neurological subjects addresses the effect of lesions to different parts of this network. We have recently studied in detail subjects with 'dystimbria': an alteration in the perceived quality of auditory objects distinct from pitch or loudness change. The subjects have lesions of the normal network described above with normal perception of pitch strength but abnormal perception of the analysis of spectral envelope change.},
  number = {1-2},
  journaltitle = {Hearing Research},
  shortjournal = {Hear. Res.},
  date = {2007-07},
  pages = {46-53},
  keywords = {cognition,Auditory Perception,Auditory Cortex,Magnetic Resonance Imaging,Acoustic Stimulation,Auditory Pathways,humans,Psychoacoustics,Auditory Perceptual Disorders,Bayes Theorem,Models; Neurological},
  author = {Griffiths, Timothy D. and Kumar, Sukhbinder and Warren, Jason D. and Stewart, Lauren and Stephan, Klaas Enno and Friston, Karl J.},
  eprinttype = {pmid},
  eprint = {17321704}
}

@article{rohrmeierPredictiveInformationProcessing2012,
  title = {Predictive Information Processing in Music Cognition. {{A}} Critical Review},
  volume = {83},
  issn = {0167-8760},
  doi = {10.1016/j.ijpsycho.2011.12.010},
  abstract = {Expectation and prediction constitute central mechanisms in the perception and cognition of music, which have been explored in theoretical and empirical accounts. We review the scope and limits of theoretical accounts of musical prediction with respect to feature-based and temporal prediction. While the concept of prediction is unproblematic for basic single-stream features such as melody, it is not straight-forward for polyphonic structures or higher-order features such as formal predictions. Behavioural results based on explicit and implicit (priming) paradigms provide evidence of priming in various domains that may reflect predictive behaviour. Computational learning models, including symbolic (fragment-based), probabilistic/graphical, or connectionist approaches, provide well-specified predictive models of specific features and feature combinations. While models match some experimental results, full-fledged music prediction cannot yet be modelled. Neuroscientific results regarding the early right-anterior negativity (ERAN) and mismatch negativity (MMN) reflect expectancy violations on different levels of processing complexity, and provide some neural evidence for different predictive mechanisms. At present, the combinations of neural and computational modelling methodologies are at early stages and require further research. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {International Journal of Psychophysiology},
  shortjournal = {International Journal of Psychophysiology},
  date = {2012-02},
  pages = {164-175},
  keywords = {cognition,Prediction,Music,music cognition,predictive information processing},
  author = {Rohrmeier, Martin A. and Koelsch, Stefan}
}

@article{parmentierCognitiveDeterminantsBehavioral2014,
  title = {The Cognitive Determinants of Behavioral Distraction by Deviant Auditory Stimuli: A Review},
  volume = {78},
  issn = {03400727},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=95659916&site=ehost-live},
  doi = {10.1007/s00426-013-0534-4},
  shorttitle = {The Cognitive Determinants of Behavioral Distraction by Deviant Auditory Stimuli},
  abstract = {Numerous studies have demonstrated that rare and unexpected changes in an otherwise repetitive or structured sound sequence ineluctably break through selective attention and impact negatively on performance in an unrelated task. While the electrophysiological responses to unexpected sounds have been extensively studied, behavioral distraction has received relatively less attention until recently. In this paper, I review work examining the cognitive underpinnings of behavioral distraction by deviant sounds and highlight some of its key determinants. Evidence indicates that deviance distraction (1) derives from the time penalty associated with the involuntary orientation of attention to and away from the deviant sound and from resulting effects such as the reactivation of the relevant task set upon the presentation of the target stimulus; and (2) is mediated by a number of factors (some increasing distraction, such as aging or induced emotions; some decreasing it, such as a memory load or cognitive control). Contrary to the received view that deviants ineluctably elicit distraction, recent work demonstrates that it is contingent upon auditory distractors acting as unspecific warning signals in the service of goal-oriented behavior, and that deviants do not elicit distraction because they are rare but because they violate the cognitive system's predictions (which can be manipulated through implicit rule learning or explicit cueing). Evidence is also presented indicating that the capture of attention by spoken deviant sounds is followed by an involuntary evaluation of their semantic properties, the outcome of which can be robust enough to linger in working memory and interfere with subsequent behavior. Finally, I review studies suggesting that behavioral deviance distraction is not the mere byproduct of the mismatch negativity, P3a and re-orientation negativity electrophysiological responses and highlight a number of outstanding questions for future research.},
  number = {3},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  urldate = {2015-06-17},
  date = {2014-05},
  pages = {321-338},
  keywords = {Attention,cognition,Auditory Perception,electrophysiology,TASK performance,DISTRACTION (Psychology),SOUND},
  author = {Parmentier, Fabrice},
  file = {D\:\\Sauve\\Zotero\\storage\\UD2D8G5M\\Parmentier - 2014 - The cognitive determinants of behavioral distracti.pdf}
}

@article{daltonAuditoryAttentionalCapture2014,
  title = {Auditory Attentional Capture: Implicit and Explicit Approaches},
  volume = {78},
  issn = {03400727},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=95659914&site=ehost-live},
  doi = {10.1007/s00426-014-0557-5},
  shorttitle = {Auditory Attentional Capture},
  abstract = {The extent to which distracting items capture attention despite being irrelevant to the task at hand can be measured either implicitly or explicitly (e.g., Simons, Trends Cogn Sci 4:147-155, ). Implicit approaches include the standard attentional capture paradigm in which distraction is measured in terms of reaction time and/or accuracy costs within a focal task in the presence (vs. absence) of a task-irrelevant distractor. Explicit measures include the inattention paradigm in which people are asked directly about their noticing of an unexpected task-irrelevant item. Although the processes of attentional capture have been studied extensively using both approaches in the visual domain, there is much less research on similar processes as they may operate within audition, and the research that does exist in the auditory domain has tended to focus exclusively on either an explicit or an implicit approach. This paper provides an overview of recent research on auditory attentional capture, integrating the key conclusions that may be drawn from both methodological approaches.},
  number = {3},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  urldate = {2015-06-17},
  date = {2014-05},
  pages = {313-320},
  keywords = {Attention,Auditory Perception,HEARING,AUDITORY attention,TASK performance,REACTION time},
  author = {Dalton, Polly and Hughes, Robert},
  file = {D\:\\Sauve\\Zotero\\storage\\UDJ59WG6\\Dalton and Hughes - 2014 - Auditory attentional capture implicit and explici.pdf}
}

@article{kondoEffectsSelfmotionAuditory2012,
  title = {Effects of Self-Motion on Auditory Scene Analysis},
  volume = {109},
  issn = {0027-8424},
  doi = {10.1073/pnas.1112852109},
  abstract = {Auditory scene analysis requires the listener to parse the incoming flow of acoustic information into perceptual “streams,” such as sentences from a single talker in the midst of background noise. Behavioral and neural data show that the formation of streams is not instantaneous; rather, streaming builds up over time and can be reset by sudden changes in the acoustics of the scene. Here, we investigated the effect of changes induced by voluntary head motion on streaming. We used a telepresence robot in a virtual reality setup to disentangle all potential consequences of head motion: changes in acoustic cues at the ears, changes in apparent source location, and changes in motor or attentional processes. The results showed that self-motion influenced streaming in at least two ways. Right after the onset of movement, self-motion always induced some resetting of perceptual organization to one stream, even when the acoustic scene itself had not changed. Then, after the motion, the prevalent organization was rapidly biased by the binaural cues discovered through motion. Auditory scene analysis thus appears to be a dynamic process that is affected by the active sensing of the environment. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {17},
  journaltitle = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {PNAS Proceedings of the National Academy of Sciences of the United States of America},
  date = {2012-04-24},
  pages = {6775-6780},
  keywords = {acoustics,auditory scene analysis,acoustic information,Motion Perception,self-motion effects},
  author = {Kondo, Hirohito M. and Pressnitzer, Daniel and Toshima, Iwaki and Kashino, Makio}
}

@article{arnottNeuralGeneratorsUnderlying2011,
  title = {Neural Generators Underlying Concurrent Sound Segregation},
  volume = {1387},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2011.02.062},
  abstract = {Although an object-based account of auditory attention has become an increasingly popular model for understanding how temporally overlapping sounds are segregated, relatively little is known about the cortical circuit that supports such ability. In the present study, we applied a beamformer spatial filter to magnetoencephalography (MEG) data recorded during an auditory paradigm that used inharmonicity to promote the formation of multiple auditory objects. Using this unconstrained, data-driven approach, the evoked field component linked with the perception of multiple auditory objects (i.e., the object-related negativity; ORNm), was found to be associated with bilateral auditory cortex sources that were distinct from those coinciding with the P1m, N1m, and P2m responses elicited by sound onset. The right hemispheric ORNm source in particular was consistently positioned anterior to the other sources across two experiments. These findings are consistent with earlier proposals of multiple auditory object detection being associated with generators in the auditory cortex and further suggest that these neural populations are distinct from the long latency evoked responses reflecting the detection of sound onset. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  date = {2011-04-28},
  pages = {116-124},
  keywords = {Evoked Potentials,Auditory Stimulation,Auditory Localization,magnetoencephalography,sound segregation,Biological Neural Networks,neural generators},
  author = {Arnott, Stephen R. and Bardouille, Tim and Ross, Bernhard and Alain, Claude}
}

@article{hengImpairedPerceptionTemporal2011,
  title = {Impaired Perception of Temporal Fine Structure and Musical Timbre in Cochlear Implant Users},
  volume = {280},
  issn = {0378-5955},
  doi = {10.1016/j.heares.2011.05.017},
  abstract = {Cochlear implant (CI) users demonstrate severe limitations in perceiving musical timbre, a psycho-acoustic feature of sound responsible for ‘tone color’ and one’s ability to identify a musical instrument. The reasons for this limitation remain poorly understood. In this study, we sought to examine the relative contributions of temporal envelope and fine structure for timbre judgments, in light of the fact that speech processing strategies employed by CI systems typically employ envelope extraction algorithms. We synthesized “instrumental chimeras” that systematically combined variable amounts of envelope and fine structure in 25\% increments from two different source instruments with either sustained or percussive envelopes. CI users and normal hearing (NH) subjects were presented with 150 chimeras and asked to determine which instrument the chimera more closely resembled in a single-interval two-alternative forced choice task. By combining instruments with similar and dissimilar envelopes, we controlled the valence of envelope for timbre identification and compensated for envelope reconstruction from fine structure information. Our results show that NH subjects utilize envelope and fine structure interchangeably, whereas CI subjects demonstrate overwhelming reliance on temporal envelope. When chimeras were created from dissimilar envelope instrument pairs, NH subjects utilized a combination of envelope (p = 0.008) and fine structure information (p = 0.009) to make timbre judgments. In contrast, CI users utilized envelope information almost exclusively to make timbre judgments (p {$<$} 0.001) and ignored fine structure information (p = 0.908). Interestingly, when the value of envelope as a cue was reduced, both NH subjects and CI users utilized fine structure information to make timbre judgments (p {$<$} 0.001), although the effect was quite weak in CI users. Our findings confirm that impairments in fine structure processing underlie poor perception of musical timbre in CI users. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {1-2},
  journaltitle = {Hearing Research},
  shortjournal = {Hearing Research},
  date = {2011-10},
  pages = {192-200},
  keywords = {Auditory Stimulation,acoustics,cochlear implants,Hearing Disorders,music perception,Musical Instruments,chimeras,musical instrument identification,musical timbre judgment,temporal envelope,temporal fine structure},
  author = {Heng, Joseph and Cantarero, Gabriela and Elhilali, Mounya and Limb, Charles J.}
}

@article{looiMelodiesFamiliarAustralian2003,
  title = {Melodies {{Familiar}} to the {{Australian Population Across}} a {{Range}} of {{Hearing Abilities}}},
  volume = {25},
  issn = {0157-1532},
  doi = {10.1375/audi.25.2.75.31118},
  abstract = {This study aimed to identify a small set of melodies that were judged as most familiar by the Australian adult population. Responses were obtained from a questionnaire completed by 88 men and women (49 with normal hearing or mild hearing losses, 14 hearing-aid users and 25 cochlear implant users). The questionnaire listed 57 potentially familiar melodies. For each melody, respondents were asked to indicate if they could recognise the name of the tune, recognise the tune if it were played to them, or sing that tune "in their head". After analysis of the responses, short lists of familiar melodies were compiled for each of the three participant groups, as well as for all respondents. The results showed that "Happy Birthday", "Jingle Bells", "Silent Night", "Twinkle Twinkle Little Star", "Waltzing Matilda" and "Old MacDonald" were consistently judged the most familiar melodies, both within each of the three participant groups and overall. Further analysis revealed that the cochlear implant users reported being able to sing tunes "in their head" more than they could recognise them aurally, whereas the results of the normal hearing or mild hearing-loss group showed the opposite pattern. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Australian and New Zealand Journal of Audiology},
  shortjournal = {Australian and New Zealand Journal of Audiology},
  date = {2003-11},
  pages = {75-83},
  keywords = {Auditory Perception,Music,Hearing Disorders,music perception,hearing abilities,Australian population,cochlear implant users,Familiarity,hearing aid users,Hearing Aids,hearing losses,melodies},
  author = {Looi, Valerie and Sucher, Catherine and McDermott, Hugh}
}

@article{koelschDifferentiatingERANMMN2001,
  title = {Differentiating {{ERAN}} and {{MMN}}: {{An ERP Study}}},
  volume = {12},
  abstract = {In the present study, the early right-anterior negativity (ERAN) elicited by harmonically inappropriate chords during listening to music was compared to the frequency mismatch negativity (MMN) and the abstract-feature MMN. Results revealed that the amplitude of the ERAN, in contrast to the MMN, is specifically dependent on the degree of harmonic appropriateness. Thus, the ERAN is correlated with the cognitive processing of complex rule-based information, i.e. with the application of music–syntactic rules. Moreover, results showed that the ERAN, compared to the abstract-feature MMN, had both a longer latency, and a larger amplitude. The combined findings indicate that ERAN and MMN reflect different mechanisms of pre-attentive irregularity detection, and that, although both components have several features in common, the ERAN does not easily fit into the classical MMN framework. The present ERPs thus provide evidence for a differentiation of cognitive processes underlying the fast and pre-attentive processing of auditory information.},
  number = {7},
  journaltitle = {NeuroReport: For Rapid Communication of Neuroscience Research},
  date = {2001},
  pages = {1385-1389},
  author = {Koelsch, Stefan and Gunter, Thomas C. and Schröger, Erich and Tervaniemi, Mari and Sammler, Daniela and Friederici, Angela D.}
}

@article{bratticoMusicalScaleProperties2006,
  title = {Musical Scale Properties Are Automatically Processed in the Human Auditory Cortex},
  volume = {1117},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S000689930602364X},
  doi = {10.1016/j.brainres.2006.08.023},
  abstract = {While listening to music, we immediately detect ‘wrong’ tones that do not match our expectations based on the prior context. This study aimed to determine whether such expectations can occur preattentively, as indexed by event-related potentials (ERPs), and whether these are modulated by attentional processes. To this end, we recorded ERPs in nonmusicians while they were presented with unfamiliar melodies, containing either a pitch deviating from the equal-tempered chromatic scale (out-of-tune) or a pitch deviating from the diatonic scale (out-of-key). ERPs were recorded in a passive experiment in which subjects were distracted from the sounds and in an active experiment in which they were judging how incongruous each melody was. In both the experiments, pitch incongruities elicited an early frontal negativity that was not modulated by attentional focus. This early negativity, closely corresponding to the mismatch negativity (MMN) of the ERPs, was mainly originated in the auditory cortex and occurred in response to both pitch violations but with larger amplitude for the more salient out-of-tune pitch than the less salient out-of-key pitch. Attentional processes leading to the conscious access of musical scale information were indexed by the late parietal positivity (resembling the P600 of the ERPs) elicited in response to both incongruous pitches in the active experiment only. Our results indicate that the relational properties of the musical scale are quickly and automatically extracted by the auditory cortex even before the intervention of focused attention.},
  number = {1},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  urldate = {2016-03-15},
  date = {2006-10-30},
  pages = {162-174},
  keywords = {Auditory Perception,Music,mismatch negativity,pitch,Event-related potential,Temporal cortex},
  author = {Brattico, Elvira and Tervaniemi, Mari and Näätänen, Risto and Peretz, Isabelle},
  file = {D\:\\Sauve\\Zotero\\storage\\SRHS86HD\\S000689930602364X.html}
}

@article{bratticoSimultaneousStorageTwo2002,
  title = {Simultaneous Storage of Two Complex Temporal Sound Patterns... : {{NeuroReport}}},
  volume = {13},
  url = {http://journals.lww.com/neuroreport/Fulltext/2002/10070/Simultaneous_storage_of_two_complex_temporal_sound.11.aspx},
  shorttitle = {Simultaneous Storage of Two Complex Temporal Sound Patterns...},
  abstract = {We wished to determine whether multiple sound patterns can be simultaneously represented in the temporary auditory buffer (auditory sensory memory), when subjects have no task related to the sounds. To this end we used the mismatch negativity (MMN) event-related potential, an electric brain respo...},
  number = {14},
  journaltitle = {Neuroreport: An International Journal for the Rapid Communication of Research in Neuroscience},
  urldate = {2016-03-14},
  date = {2002},
  pages = {1747-1751},
  author = {Brattico, Elvira and Winkler, Istvan and Näätänen, Risto and Paavilainen, Petri and Tervaniemi, Mari},
  file = {D\:\\Sauve\\Zotero\\storage\\BTKVKUMG\\Simultaneous_storage_of_two_complex_temporal_sound.11.html;D\:\\Sauve\\Zotero\\storage\\HM7NNH36\\Simultaneous_storage_of_two_complex_temporal_sound.11.html}
}

@article{paavilainenNeuronalPopulationsHuman1999,
  title = {Neuronal Populations in the Human Brain Extracting Invariant Relationships from Acoustic Variance},
  volume = {265},
  issn = {0304-3940},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394099002372},
  doi = {10.1016/S0304-3940(99)00237-2},
  abstract = {The ability to extract invariant relationships from physically varying stimulation is critical for example to categorical perception of complex auditory information such as speech and music. Human subjects were presented with tone pairs randomly varying over a wide frequency range, there being no physically constant tone pair at all. Instead, the invariant feature was either the direction of the tone pairs (ascending: the second tone was higher in frequency than the first tone) or the frequency ratio (musical interval) of the two tones. The subjects ignored the tone pairs, and instead attended a silent video. Occasional deviant pairs (either descending in direction or having a different frequency ratio) elicited the mismatch negativity (MMN) of the event-related potential, demonstrating the existence of neuronal populations which automatically (independently of attention) extract invariant relationships from acoustical variance.},
  number = {3},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  urldate = {2016-03-14},
  date = {1999-04-23},
  pages = {179-182},
  keywords = {music perception,mismatch negativity,Speech Perception,event-related potentials,Cognitive neuroscience,Invariance detection,Sensory memory},
  author = {Paavilainen, Petri and Jaramillo, Maria and Näätänen, Risto and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\TICQF2K4\\S0304394099002372.html}
}

@incollection{vuustNeuralUnderpinningsMusic2014,
  langid = {english},
  title = {Neural {{Underpinnings}} of {{Music}}: {{The Polyrhythmic Brain}}},
  isbn = {978-1-4939-1781-5 978-1-4939-1782-2},
  url = {http://link.springer.com/chapter/10.1007/978-1-4939-1782-2_18},
  shorttitle = {Neural {{Underpinnings}} of {{Music}}},
  abstract = {Musical rhythm, consisting of apparently abstract intervals of accented temporal events, has the remarkable ability to move our minds and bodies. Why do certain rhythms make us want to tap our feet, bop our heads or even get up and dance? And how does the brain process rhythmically complex rhythms during our experiences of music? In this chapter, we describe some common forms of rhythmic complexity in music and propose that the theory of predictive coding can explain how rhythm and rhythmic complexity are processed in the brain. We also consider how this theory may reveal why we feel so compelled by rhythmic tension in music. First, musical-theoretical and neuroscientific frameworks of rhythm are presented, in which rhythm perception is conceptualized as an interaction between what is heard (‘rhythm’) and the brain’s anticipatory structuring of music (‘the meter’). Second, three different examples of tension between rhythm and meter in music are described: syncopation, polyrhythm and groove. Third, we present the theory of predictive coding of music, which posits a hierarchical organization of brain responses reflecting fundamental, survival-related mechanisms associated with predicting future events. According to this theory, perception and learning is manifested through the brain’s Bayesian minimization of the error between the input to the brain and the brain’s prior expectations. Fourth, empirical studies of neural and behavioral effects of syncopation, polyrhythm and groove will be reported, and we propose how these studies can be seen as special cases of the predictive coding theory. Finally, we argue that musical rhythm exploits the brain’s general principles of anticipation and propose that pleasure from musical rhythm may be a result of such anticipatory mechanisms.},
  number = {829},
  booktitle = {Neurobiology of {{Interval Timing}}},
  series = {Advances in {{Experimental Medicine}} and {{Biology}}},
  publisher = {{Springer New York}},
  urldate = {2016-03-14},
  date = {2014},
  pages = {339-356},
  keywords = {Prediction,Music,rhythmic complexity,Neurosciences,Human Physiology,Pleasure},
  author = {Vuust, Peter and Gebauer, Line K. and Witek, Maria A. G.},
  editor = {Merchant, Hugo and de Lafuente, Victor},
  file = {D\:\\Sauve\\Zotero\\storage\\TXIDSFHI\\978-1-4939-1782-2_18.html},
  doi = {10.1007/978-1-4939-1782-2_18}
}

@article{vuustPracticedMusicalStyle2012,
  langid = {english},
  title = {Practiced Musical Style Shapes Auditory Skills},
  volume = {1252},
  issn = {1749-6632},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.2011.06409.x/abstract},
  doi = {10.1111/j.1749-6632.2011.06409.x},
  abstract = {Musicians’ processing of sounds depends highly on instrument, performance practice, and level of expertise. Here, we measured the mismatch negativity (MMN), a preattentive brain response, to six types of musical feature change in musicians playing three distinct styles of music (classical, jazz, and rock/pop) and in nonmusicians using a novel, fast, and musical sounding multifeature MMN paradigm. We found MMN to all six deviants, showing that MMN paradigms can be adapted to resemble a musical context. Furthermore, we found that jazz musicians had larger MMN amplitude than all other experimental groups across all sound features, indicating greater overall sensitivity to auditory outliers. Furthermore, we observed a tendency toward shorter latency of the MMN to all feature changes in jazz musicians compared to band musicians. These findings indicate that the characteristics of the style of music played by musicians influence their perceptual skills and the brain processing of sound features embedded in music.},
  number = {1},
  journaltitle = {Annals of the New York Academy of Sciences},
  urldate = {2016-03-14},
  date = {2012-04-01},
  pages = {139-146},
  keywords = {Musicians,Mismatch negativity (MMN),EEG,Learning,multifeature MMN paradigm,musical style},
  author = {Vuust, Peter and Brattico, Elvira and Seppänen, Miia and Näätänen, Risto and Tervaniemi, Mari},
  file = {D\:\\Sauve\\Zotero\\storage\\2CFBZIJ9\\abstract\;jsessionid=28858F0BEC535AB92271EE475858F251.html}
}

@article{takegataPreattentiveRepresentationFeature2005,
  title = {Preattentive Representation of Feature Conjunctions for Concurrent Spatially Distributed Auditory Objects},
  volume = {25},
  issn = {0926-6410},
  url = {http://www.sciencedirect.com/science/article/pii/S0926641005001564},
  doi = {10.1016/j.cogbrainres.2005.05.006},
  abstract = {The role of attention in conjoining features of an object has been a topic of much debate. Studies using the mismatch negativity (MMN), an index of detecting acoustic deviance, suggested that the conjunctions of auditory features are preattentively represented in the brain. These studies, however, used sequentially presented sounds and thus are not directly comparable with visual studies of feature integration. Therefore, the current study presented an array of spatially distributed sounds to determine whether the auditory features of concurrent sounds are correctly conjoined without focal attention directed to the sounds. Two types of sounds differing from each other in timbre and pitch were repeatedly presented together while subjects were engaged in a visual n-back working-memory task and ignored the sounds. Occasional reversals of the frequent pitch–timbre combinations elicited MMNs of a very similar amplitude and latency irrespective of the task load. This result suggested preattentive integration of auditory features. However, performance in a subsequent target-search task with the same stimuli indicated the occurrence of illusory conjunctions. The discrepancy between the results obtained with and without focal attention suggests that illusory conjunctions may occur during voluntary access to the preattentively encoded object representations.},
  number = {1},
  journaltitle = {Cognitive Brain Research},
  shortjournal = {Cognitive Brain Research},
  urldate = {2016-03-14},
  date = {2005-09},
  pages = {169-179},
  keywords = {Attention,Auditory Perception,Mismatch negativity (MMN),auditory memory,Event-related potential (ERP),Feature binding},
  author = {Takegata, Rika and Brattico, Elvira and Tervaniemi, Mari and Varyagina, Olga and Näätänen, Risto and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\V6RANEN8\\S0926641005001564.html}
}

@article{naatanenEarlySelectiveattentionEffect1978,
  langid = {english},
  title = {Early Selective-Attention Effect on Evoked Potential Reinterpreted},
  volume = {42},
  issn = {0001-6918},
  number = {4},
  journaltitle = {Acta Psychologica},
  shortjournal = {Acta Psychol (Amst)},
  date = {1978-07},
  pages = {313-329},
  keywords = {Attention,Evoked Potentials,Auditory Perception,electroencephalography,humans,Adult,Cerebral Cortex,Dominance; Cerebral},
  author = {Näätänen, R. and Gaillard, A. W. and Mäntysalo, S.},
  eprinttype = {pmid},
  eprint = {685709}
}

@article{pearceRoleExpectationProbabilistic2010,
  title = {The Role of Expectation and Probabilistic Learning in Auditory Boundary Perception: {{A}} Model Comparison},
  volume = {39},
  issn = {0301-0066},
  doi = {10.1068/p6507},
  shorttitle = {The Role of Expectation and Probabilistic Learning in Auditory Boundary Perception},
  abstract = {Grouping and boundary perception are central to many aspects of sensory processing in cognition. We present a comparative study of recently published computational models of boundary perception in music. In doing so, we make three contributions. First, we hypothesise a relationship between expectation and grouping in auditory perception, and introduce a novel information-theoretic model of perceptual segmentation to test the hypothesis. Although we apply the model to musical melody, it is applicable in principle to sequential grouping in other areas of cognition. Second, we address a methodological consideration in the analysis of ambiguous stimuli that produce different percepts between individuals. We propose and demonstrate a solution to this problem, based on clustering of participants prior to analysis. Third, we conduct the first comparative analysis of probabilistic-learning and rule-based models of perceptual grouping in music. In spite of having only unsupervised exposure to music, the model performs comparably to rule-based models based on expert musical knowledge, supporting a role for probabilistic learning in perceptual segmentation of music. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {10},
  journaltitle = {Perception},
  shortjournal = {Perception},
  date = {2010},
  pages = {1365-1389},
  keywords = {cognition,Auditory Perception,Expectations,auditory boundary perception,probabilistic learning,Probability Learning},
  author = {Pearce, M. T. and Müllensiefen, Daniel and Wiggins, Geraint A.},
  file = {D\:\\Sauve\\Zotero\\storage\\FI6EFGUU\\1367.html}
}

@article{knoschePerceptionPhraseStructure2005,
  langid = {english},
  title = {Perception of Phrase Structure in Music},
  volume = {24},
  issn = {1065-9471},
  doi = {10.1002/hbm.20088},
  abstract = {Neither music nor spoken language form uniform auditory streams, rather, they are structured into phrases. For the perception of such structures, the detection of phrase boundaries is crucial. We discovered electroencephalography (EEG) and magnetoencephalography (MEG) correlates for the perception of phrase boundaries in music. In EEG, this process was marked by a positive wave approximately between 500 and 600 ms after the offset of a phrase boundary with a centroparietal maximum. In MEG, we found major activity in an even broader time window (400-700 ms). Source localization revealed that likely candidates for the generation of the observed effects are structures in the limbic system, including anterior and posterior cingulate as well as posterior mediotemporal cortex. The timing and topography of the EEG effect bear some resemblance to a positive shift (closure positive shift, CPS) found for prosodic phrase boundaries during speech perception in an earlier study, suggesting that the underlying processes might be related. Because the brain structures, which possibly underlie the observed effects, are known to be involved in memory and attention processes, we suggest that the CPS may not reflect the detection of the phrase boundary as such, but those memory and attention related processes that are necessary to guide the attention focus from one phrase to the next, thereby closing the former and opening up the next phrase.},
  number = {4},
  journaltitle = {Human Brain Mapping},
  shortjournal = {Hum Brain Mapp},
  date = {2005-04},
  pages = {259-273},
  keywords = {brain,Auditory Perception,Music,electroencephalography,magnetoencephalography,humans,Adult,Female,Male,Brain Mapping,Evoked Potentials; Auditory},
  author = {Knösche, Thomas R. and Neuhaus, Christiane and Haueisen, Jens and Alter, Kai and Maess, Burkhard and Witte, Otto W. and Friederici, Angela D.},
  eprinttype = {pmid},
  eprint = {15678484}
}

@article{schulzEvidenceTraininginducedCrossmodal2003,
  title = {Evidence for Training-Induced Crossmodal Reorganization of Crossmodal Reorganization of Cortical Functions in Trumpet Players},
  volume = {14},
  url = {http://journals.lww.com/neuroreport/Fulltext/2003/01200/Evidence_for_training_induced_crossmodal.29.aspx},
  shorttitle = {Evidence for Training-Induced Crossmodal Reorganization of c...},
  abstract = {The aim of this study was to compare multimodal information processing in the somatosensory and auditory cortices and related multimodal areas in musicians (trumpet players) and non-musicians. Magnetoencephalographic activity (MEG) was recorded in response to five stimulus conditions from 10 professional trumpet players and nine musically untrained control subjects. Somatosensory and auditory stimuli were presented alone or in combination. Our data suggest that musicians, in general, process multisensory stimuli differently to the control group. When stimulating the lip in professional trumpet players, a multimodal interaction (expressed as difference between the multimodal response and the sum of unimodal responses) in the corresponding somatosensory cortex showed a positive peak at 33\,ms, which was not found in the control group. Conversely, the control group shows a significant interaction of opposite polarity around 60–80\,ms. We suggest that training-induced reorganization in musicians leads to a qualitatively different way to process multisensory information. It favors an early stage of cortical processing, which is modified by the connections between multimodal and auditory neurons from thalamus to primary somatosensory area.},
  number = {1},
  journaltitle = {NeuroReport: For Rapid Communication of Neuroscience Research},
  urldate = {2015-12-12},
  date = {2003},
  pages = {157-161},
  author = {{Schulz} and {Ross} and {Pantev}},
  file = {D\:\\Sauve\\Zotero\\storage\\HAAIUUX4\\Evidence_for_training_induced_crossmodal.29.html}
}

@article{baumannNetworkAudioMotor2007,
  title = {A Network for Audio–Motor Coordination in Skilled Pianists and Non-Musicians},
  volume = {1161},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899307012462},
  doi = {10.1016/j.brainres.2007.05.045},
  abstract = {Playing a musical instrument requires efficient auditory and motor processing. Fast feed forward and feedback connections that link the acoustic target to the corresponding motor programs need to be established during years of practice. The aim of our study is to provide a detailed description of cortical structures that participate in this audio–motor coordination network in professional pianists and non-musicians. In order to map these interacting areas using functional magnetic resonance imaging (fMRI), we considered cortical areas that are concurrently activated during silent piano performance and motionless listening to piano sound. Furthermore we investigated to what extent interactions between the auditory and the motor modality happen involuntarily. We observed a network of predominantly secondary and higher order areas belonging to the auditory and motor modality. The extent of activity was clearly increased by imagination of the absent modality. However, this network did neither comprise primary auditory nor primary motor areas in any condition. Activity in the lateral dorsal premotor cortex (PMd) and the pre-supplementary motor cortex (preSMA) was significantly increased for pianists. Our data imply an intermodal transformation network of auditory and motor areas which is subject to a certain degree of plasticity by means of intensive training.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  urldate = {2015-12-12},
  date = {2007-08-03},
  pages = {65-78},
  keywords = {plasticity,Music,fMRI,auditory,Crossmodal interaction,Motor,Sensory-motor integration},
  author = {Baumann, Simon and Koeneke, Susan and Schmidt, Conny F. and Meyer, Martin and Lutz, Kai and Jancke, Lutz},
  file = {D\:\\Sauve\\Zotero\\storage\\442UKH5P\\S0006899307012462.html;D\:\\Sauve\\Zotero\\storage\\CP6TTZH8\\S0006899307012462.html}
}

@article{fujiokaMusicalTrainingEnhances2004,
  title = {Musical Training Enhances Automatic Encoding of Melodic Contour and Interval Structure},
  volume = {16},
  issn = {0898-929X},
  doi = {10.1162/0898929041502706},
  abstract = {In music, melodic information is thought to be encoded in two forms, a contour code (up/down pattern of pitch changes) and an interval code (pitch distances between successive notes). A recent study recording the mismatch negativity (MMN) evoked by pitch contour and interval deviations in simple melodies demonstrated that people with no formal music education process both contour and interval information in the auditory cortex automatically. However, it is still unclear whether musical experience enhances both strategies of melodic encoding. We designed stimuli to examine contour and interval information separately. In the contour condition there were eight different standard melodies (presented on 80\% of trials), each consisting of five notes all ascending in pitch, and the corresponding deviant melodies (2096) were altered to descending on their final note. The interval condition used one five-note standard melody transposed to eight keys from trial to trial, and on deviant trials the last note was raised by one whole tone without changing the pitch contour. There was also a control condition, in which a standard tone (990.7 Hz) and a deviant tone (1111.0 Hz) were presented. The magnetic counterpart of the MMN (MMNm) from musicians and nonmusicians was obtained as the difference between the dipole moment in response to the standard and deviant trials recorded by magnetoencephalography. Significantly larger MMNm was present in musicians in both contour and interval conditions than in nonmusicians, whereas MMNm in the control condition was similar for both groups. The interval MMNm was larger than the contour MMNm in musicians. No hemispheric difference was found in either group. The results suggest that musical training enhances the ability to automatically register abstract changes in the relative pitch structure of melodies. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {6},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  date = {2004-07},
  pages = {1010-1021},
  keywords = {Music,Pitch (Frequency),Pitch Discrimination,musical training,music perception,mismatch negativity,automatic encoding,Human Information Storage,pitch,Form and Shape Perception,interval structure,melodic contour},
  author = {Fujioka, Takako and Trainor, Laurel J. and Ross, Bernhard and Kakigi, Ryusuke and Pantev, Christo},
  file = {D\:\\Sauve\\Zotero\\storage\\GPV74GHM\\Fujioka et al. - 2004 - Musical training enhances automatic encoding of me.pdf;D\:\\Sauve\\Zotero\\storage\\3D22Z7FV\\login.html;D\:\\Sauve\\Zotero\\storage\\GQKAC4QG\\0898929041502706.html}
}

@article{gutschalkNeuromagneticCorrelatesStreaming2005,
  title = {Neuromagnetic {{Correlates}} of {{Streaming}} in {{Human Auditory Cortex}}},
  volume = {25},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.0347-05.2005},
  abstract = {The brain is constantly faced with the challenge of organizing acoustic input from multiple sound sources into meaningful auditory objects or perceptual streams. The present study examines the neural bases of auditory stream formation using neuromagnetic and behavioral measures. The stimuli were sequences of alternating pure tones, which can be perceived as either one or two streams. In the first experiment, physical stimulus parameters were varied between values that promoted the perceptual grouping of the tone sequence into one coherent stream and values that promoted its segregation into two streams. In the second experiment, an ambiguous tone sequence produced a bistable percept that switched spontaneously between one- and two-stream percepts. The first experiment demonstrated a strong correlation between listeners' perception and long-latency ({$>$}60 ms) activity that likely arises in nonprimary auditory cortex. The second demonstrated a covariation between this activity and listeners' perception in the absence of physical stimulus changes. Overall, the results indicate a tight coupling between auditory cortical activity and streaming perception, suggesting that an explicit representation of auditory streams may be maintained within nonprimary auditory areas. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {22},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {The Journal of Neuroscience},
  date = {2005-06},
  pages = {5382-5388},
  keywords = {brain,Auditory Perception,Auditory Stimulation,Stimulus Parameters,Auditory Cortex,Behavioral Assessment,behavioral measures,Magnetism,neuromagnetic,Neurotransmission},
  author = {Gutschalk, Alexander and Micheyl, Christophe and Melcher, Jennifer R. and Rupp, André and Scherg, Michael and Oxenham, Andrew J.}
}

@article{masutomiSoundSegregationEmbedded2016,
  langid = {english},
  title = {Sound Segregation via Embedded Repetition Is Robust to Inattention.},
  volume = {42},
  issn = {1939-1277, 0096-1523},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xhp0000147},
  doi = {10.1037/xhp0000147},
  number = {3},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2016-02-24},
  date = {2016},
  pages = {386-400},
  author = {Masutomi, Keiko and Barascud, Nicolas and Kashino, Makio and McDermott, Josh H. and Chait, Maria}
}

@article{winklerMultistabilityAuditoryStream2012,
  langid = {english},
  title = {Multistability in Auditory Stream Segregation: A Predictive Coding View},
  volume = {367},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2011.0359},
  doi = {10.1098/rstb.2011.0359},
  shorttitle = {Multistability in Auditory Stream Segregation},
  number = {1591},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  urldate = {2016-02-24},
  date = {2012-04-05},
  pages = {1001-1012},
  author = {Winkler, I. and Denham, S. and Mill, R. and Bohm, T. M. and Bendixen, A.}
}

@article{robertsPrimitiveStreamSegregation2002,
  langid = {english},
  title = {Primitive Stream Segregation of Tone Sequences without Differences in Fundamental Frequency or Passband},
  volume = {112},
  issn = {00014966},
  url = {http://scitation.aip.org/content/asa/journal/jasa/112/5/10.1121/1.1508784},
  doi = {10.1121/1.1508784},
  number = {5},
  journaltitle = {The Journal of the Acoustical Society of America},
  urldate = {2016-02-24},
  date = {2002},
  pages = {2074},
  author = {Roberts, Brian and Glasberg, Brian R. and Moore, Brian C. J.}
}

@article{moorePropertiesAuditoryStream2012,
  langid = {english},
  title = {Properties of Auditory Stream Formation},
  volume = {367},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2011.0355},
  doi = {10.1098/rstb.2011.0355},
  number = {1591},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  urldate = {2016-02-24},
  date = {2012-04-05},
  pages = {919-931},
  author = {Moore, B. C. J. and Gockel, H. E.},
  file = {D\:\\Sauve\\Zotero\\storage\\Q455D3E9\\Moore and Gockel - 2012 - Properties of auditory stream formation.pdf;D\:\\Sauve\\Zotero\\storage\\7WHNLQPB\\919.html}
}

@article{gutschalkInteractionStreamingAttention2015,
  langid = {english},
  title = {Interaction of {{Streaming}} and {{Attention}} in {{Human Auditory Cortex}}},
  volume = {10},
  issn = {1932-6203},
  url = {http://dx.plos.org/10.1371/journal.pone.0118962},
  doi = {10.1371/journal.pone.0118962},
  number = {3},
  journaltitle = {PLOS ONE},
  urldate = {2016-02-24},
  date = {2015-03-18},
  pages = {e0118962},
  author = {Gutschalk, Alexander and Rupp, André and Dykstra, Andrew R.},
  editor = {Altmann, Christian Friedrich}
}

@article{demanyAuditoryStreamSegregation1982,
  langid = {english},
  title = {Auditory Stream Segregation in Infancy},
  volume = {5},
  issn = {01636383},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0163638382800362},
  doi = {10.1016/S0163-6383(82)80036-2},
  number = {2-4},
  journaltitle = {Infant Behavior and Development},
  urldate = {2016-02-24},
  date = {1982-01},
  pages = {261-276},
  author = {Demany, Laurent}
}

@article{bizleyWhatWhereHow2013,
  title = {The What, Where and How of Auditory-Object Perception},
  volume = {14},
  issn = {1471-003X, 1471-0048},
  url = {http://www.nature.com/doifinder/10.1038/nrn3565},
  doi = {10.1038/nrn3565},
  number = {10},
  journaltitle = {Nature Reviews Neuroscience},
  urldate = {2016-02-24},
  date = {2013-09-20},
  pages = {693-707},
  author = {Bizley, Jennifer K. and Cohen, Yale E.}
}

@article{bonnelDividedAttentionSimultaneous1998,
  langid = {english},
  title = {Divided Attention between Simultaneous Auditory and Visual Signals},
  volume = {60},
  issn = {0031-5117, 1532-5962},
  url = {http://www.springerlink.com/index/10.3758/BF03206027},
  doi = {10.3758/BF03206027},
  number = {2},
  journaltitle = {Perception \& Psychophysics},
  urldate = {2016-02-24},
  date = {1998-01},
  pages = {179-190},
  author = {Bonnel, Anne-Marie and Haftser, Ervin R.}
}

@article{szalardyForegroundbackgroundDiscriminationIndicated2013,
  langid = {english},
  title = {Foreground-Background Discrimination Indicated by Event-Related Brain Potentials in a New Auditory Multistability Paradigm: {{Auditory}} Foreground-Background Discrimination},
  volume = {50},
  issn = {00485772},
  url = {http://doi.wiley.com/10.1111/psyp.12139},
  doi = {10.1111/psyp.12139},
  shorttitle = {Foreground-Background Discrimination Indicated by Event-Related Brain Potentials in a New Auditory Multistability Paradigm},
  number = {12},
  journaltitle = {Psychophysiology},
  urldate = {2016-02-24},
  date = {2013-12},
  pages = {1239-1250},
  author = {Szalárdy, Orsolya and Winkler, István and Schröger, Erich and Widmann, Andreas and Bendixen, Alexandra}
}

@article{elhilaliInteractionAttentionBottomUp2009,
  langid = {english},
  title = {Interaction between {{Attention}} and {{Bottom}}-{{Up Saliency Mediates}} the {{Representation}} of {{Foreground}} and {{Background}} in an {{Auditory Scene}}},
  volume = {7},
  issn = {1545-7885},
  url = {http://dx.plos.org/10.1371/journal.pbio.1000129},
  doi = {10.1371/journal.pbio.1000129},
  number = {6},
  journaltitle = {PLoS Biology},
  urldate = {2016-02-24},
  date = {2009-06-16},
  pages = {e1000129},
  author = {Elhilali, Mounya and Xiang, Juanjuan and Shamma, Shihab A. and Simon, Jonathan Z.},
  editor = {Griffiths, Timothy D.}
}

@incollection{denhamStabilityPerceptualOrganisation2010,
  langid = {english},
  location = {{New York, NY}},
  title = {Stability of {{Perceptual Organisation}} in {{Auditory Streaming}}},
  isbn = {978-1-4419-5685-9 978-1-4419-5686-6},
  url = {http://link.springer.com/10.1007/978-1-4419-5686-6_44},
  booktitle = {The {{Neurophysiological Bases}} of {{Auditory Perception}}},
  publisher = {{Springer New York}},
  urldate = {2016-02-24},
  date = {2010},
  pages = {477-487},
  author = {Denham, Susan L. and Gyimesi, Kinga and Stefanics, Gábor and Winkler, István},
  editor = {Lopez-Poveda, Enrique A. and Palmer, Alan R. and Meddis, Ray}
}

@article{baylisVisualAttentionObjects1993,
  langid = {english},
  title = {Visual Attention and Objects: {{Evidence}} for Hierarchical Coding of Location.},
  volume = {19},
  issn = {0096-1523},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.19.3.451},
  doi = {10.1037/0096-1523.19.3.451},
  shorttitle = {Visual Attention and Objects},
  number = {3},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2016-02-24},
  date = {1993},
  pages = {451-470},
  author = {Baylis, Gordon C. and Driver, Jon}
}

@article{rossInfluenceMusicalGroove2016,
  langid = {english},
  title = {Influence of Musical Groove on Postural Sway.},
  volume = {42},
  issn = {1939-1277, 0096-1523},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xhp0000198},
  doi = {10.1037/xhp0000198},
  number = {3},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  urldate = {2016-02-24},
  date = {2016},
  pages = {308-319},
  author = {Ross, Jessica M. and Warlaumont, Anne S. and Abney, Drew H. and Rigoli, Lillian M. and Balasubramaniam, Ramesh}
}

@article{peretzMusicEmotionPerceptual1998,
  title = {Music and Emotion: Perceptual Determinants, Immediacy, and Isolation after Brain Damage},
  volume = {68},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027798000432},
  shorttitle = {Music and Emotion},
  number = {2},
  journaltitle = {Cognition},
  urldate = {2016-01-27},
  date = {1998},
  pages = {111--141},
  author = {Peretz, Isabelle and Gagnon, Lise and Bouchard, Bernard},
  file = {D\:\\Sauve\\Zotero\\storage\\29CD4RKX\\Peretz et al. - 1998 - Music and emotion perceptual determinants, immedi.pdf;D\:\\Sauve\\Zotero\\storage\\8Z6E7ZM5\\S0010027798000432.html}
}

@book{juslinHandbookMusicEmotion2011,
  title = {Handbook of Music and Emotion: {{Theory}}, Research, Applications},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=e250IFNvw1sC&oi=fnd&pg=PT6&dq=music+and+emotion&ots=U_cVJdhueJ&sig=WJjyrykHIdsA_xfMrv11w0GKDrE},
  shorttitle = {Handbook of Music and Emotion},
  publisher = {{Oxford University Press}},
  urldate = {2016-01-27},
  date = {2011},
  author = {Juslin, Patrik N. and Sloboda, John},
  file = {D\:\\Sauve\\Zotero\\storage\\EBWSH9IX\\books.html}
}

@article{schoonSensoryConsonance2005,
  langid = {english},
  title = {Sensory {{Consonance}}},
  volume = {23},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/23/2/15},
  doi = {10.1525/mp.2005.23.2.105},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2015-12-21},
  date = {2005-12-01},
  pages = {105-118},
  author = {Schöön, Daniele and Regnault, Pascaline and Ystad, Søølvi and Besson, Mireille},
  file = {D\:\\Sauve\\Zotero\\storage\\5QBQEQRH\\Schöön et al. - 2005 - Sensory Consonance.pdf;D\:\\Sauve\\Zotero\\storage\\XQVXUNUJ\\15.html}
}

@article{bratticoNeuralDiscriminationNonprototypical2009,
  langid = {english},
  title = {Neural Discrimination of Nonprototypical Chords in Music Experts and Laymen: An {{MEG}} Study},
  volume = {21},
  issn = {0898-929X},
  doi = {10.1162/jocn.2008.21144},
  shorttitle = {Neural Discrimination of Nonprototypical Chords in Music Experts and Laymen},
  abstract = {At the level of the auditory cortex, musicians discriminate pitch changes more accurately than nonmusicians. However, it is not agreed upon how sound familiarity and musical expertise interact in the formation of pitch-change discrimination skills, that is, whether musicians possess musical pitch discrimination abilities that are generally more accurate than in nonmusicians or, alternatively, whether they may be distinguished from nonmusicians particularly with respect to the discrimination of nonprototypical sounds that do not play a reference role in Western tonal music. To resolve this, we used magnetoencephalography (MEG) to measure the change-related magnetic mismatch response (MMNm) in musicians and nonmusicians to two nonprototypical chords, a "dissonant" chord containing a highly unpleasant interval and a "mistuned" chord including a mistuned pitch, and a minor chord, all inserted in a context of major chords. Major and minor are the most frequently used chords in Western tonal music which both musicians and nonmusicians are most familiar with, whereas the other chords are more rarely encountered in tonal music. The MMNm was stronger in musicians than in nonmusicians in response to the dissonant and mistuned chords, whereas no group difference was found in the MMNm strength to minor chords. Correspondingly, the length of musical training correlated with the MMNm strength for the dissonant and mistuned chords only. Our findings provide evidence for superior automatic discrimination of nonprototypical chords in musicians. Most likely, this results from a highly sophisticated auditory system in musicians allowing a more efficient discrimination of chords deviating from the conventional categories of tonal music.},
  number = {11},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {J Cogn Neurosci},
  date = {2009-11},
  pages = {2230-2244},
  keywords = {Attention,Music,Pitch Discrimination,magnetoencephalography,Acoustic Stimulation,humans,Adult,Female,Male,Recognition (Psychology),Discrimination (Psychology),Reference Values,Young Adult,Cerebral Cortex,Evoked Potentials; Auditory},
  author = {Brattico, Elvira and Pallesen, Karen Johanne and Varyagina, Olga and Bailey, Christopher and Anourova, Irina and Järvenpää, Miika and Eerola, Tuomas and Tervaniemi, Mari},
  eprinttype = {pmid},
  eprint = {18855547}
}

@article{bailesComparativeTimeSeries2012,
  langid = {english},
  title = {Comparative {{Time Series Analysis}} of {{Perceptual Responses}} to {{Electroacoustic Music}}},
  volume = {29},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/29/4/359},
  doi = {10.1525/mp.2012.29.4.359},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2015-12-21},
  date = {2012-04-01},
  pages = {359-375},
  keywords = {affect,electroacoustic music,perceptions of change,sound intensity,time series analysis},
  author = {Bailes, Freya and Dean, Roger T.},
  file = {D\:\\Sauve\\Zotero\\storage\\9GPUXVD7\\Bailes and Dean - 2012 - Comparative Time Series Analysis of Perceptual Res.pdf;D\:\\Sauve\\Zotero\\storage\\F88XPVA9\\359.full.html}
}

@article{massonUsingConfidenceIntervals2003,
  title = {Using Confidence Intervals for Graphically Based Data Interpretation},
  volume = {57},
  issn = {1878-7290(Electronic);1196-1961(Print)},
  doi = {10.1037/h0087426},
  abstract = {As a potential alternative to standard null hypothesis significance testing, we describe methods for graphical presentation of data--particularly condition means and their corresponding confidence intervals--for a wide range of factorial designs used in experimental psychology. We describe and illustrate confidence intervals specifically appropriate for between-subject versus within-subject factors. For designs involving more than two levels of a factor, we describe the use of contrasts for graphical illustration of theoretically meaningful components of main effects and interactions. These graphical techniques lend themselves to a natural and straightforward assessment of statistical power.},
  number = {3},
  journaltitle = {Canadian Journal of Experimental Psychology/Revue canadienne de psychologie expérimentale},
  date = {2003},
  pages = {203-220},
  keywords = {*Confidence Limits (Statistics),*Graphical Displays,*Null Hypothesis Testing,*Statistical Analysis,*Statistical Power,Statistical Significance},
  author = {Masson, Micheal E. J. and Loftus, Geoffrey R.}
}

@book{cummingUnderstandingNewStatistics2012,
  langid = {english},
  title = {Understanding the {{New Statistics}}: {{Effect Sizes}}, {{Confidence Intervals}}, and {{Meta}}-Analysis},
  isbn = {978-0-415-87968-2},
  shorttitle = {Understanding the {{New Statistics}}},
  abstract = {This is the first book to introduce the new statistics - effect sizes, confidence intervals, and meta-analysis - in an accessible way. It is chock full of practical examples and tips on how to analyze and report research results using these techniques. The book is invaluable to readers interested in meeting the new APA Publication Manual guidelines by adopting the new statistics - which are more informative than null hypothesis significance testing, and becoming widely used in many disciplines.Accompanying the book is the Exploratory Software for Confidence Intervals (ESCI) package, free software that runs under Excel and is accessible at www.thenewstatistics.com. The bookâe(tm)s exercises use ESCI's simulations, which are highly visual and interactive, to engage users and encourage exploration. Working with the simulations strengthens understanding of key statistical ideas. There are also many examples, and detailed guidance to show readers how to analyze their own data using the new statistics, and practical strategies for interpreting the results. A particular strength of the book is its explanation of meta-analysis, using simple diagrams and examples. Understanding meta-analysis is increasingly important, even at undergraduate levels, because medicine, psychology and many other disciplines now use meta-analysis to assemble the evidence needed for evidence-based practice.The bookâe(tm)s pedagogical program, built on cognitive science principles, reinforces learning:Boxes provide "evidence-based" advice on the most effective statistical techniques. Numerous examples reinforce learning, and show that many disciplines are using the new statistics.Graphs are tied in with ESCI to make important concepts vividly clear and memorable.Opening overviews and end of chapter take-home messages summarize key points.Exercises encourage exploration, deep understanding, and practical applications.This highly accessible book is intended as the core text for any course that emphasizes the new statistics, or as a supplementary text for graduate and/or advanced undergraduate courses in statistics and research methods in departments of psychology, education, human development , nursing, and natural, social, and life sciences. Researchers and practitioners interested in understanding the new statistics, and future published research, will also appreciate this book. A basic familiarity with introductory statistics is assumed.},
  pagetotal = {519},
  publisher = {{Routledge}},
  date = {2012},
  keywords = {Education / Statistics,Psychology / Statistics,Social Science / Statistics},
  author = {Cumming, Geoff}
}

@article{cummingNewStatisticsWhy2013,
  langid = {english},
  title = {The {{New Statistics Why}} and {{How}}},
  issn = {0956-7976, 1467-9280},
  url = {http://pss.sagepub.com/content/early/2013/11/07/0956797613504966},
  doi = {10.1177/0956797613504966},
  abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
  journaltitle = {Psychological Science},
  shortjournal = {Psychological Science},
  urldate = {2015-12-15},
  date = {2013-11-12},
  pages = {0956797613504966},
  keywords = {estimation,meta-analysis,Replication,research integrity,research methods,statistical analysis,the new statistics},
  author = {Cumming, Geoff},
  file = {D\:\\Sauve\\Zotero\\storage\\NNCV58U7\\Cumming - 2013 - The New Statistics Why and How.pdf;D\:\\Sauve\\Zotero\\storage\\KBBV2FF2\\0956797613504966.html},
  eprinttype = {pmid},
  eprint = {24220629}
}

@article{deikeDecisionMakingAmbiguity2015,
  title = {Decision Making and Ambiguity in Auditory Stream Segregation},
  volume = {9},
  issn = {1662-4548},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4531241/},
  doi = {10.3389/fnins.2015.00266},
  abstract = {Researchers of auditory stream segregation have largely taken a bottom-up view on the link between physical stimulus parameters and the perceptual organization of sequences of ABAB sounds. However, in the majority of studies, researchers have relied on the reported decisions of the subjects regarding which of the predefined percepts (e.g., one stream or two streams) predominated when subjects listened to more or less ambiguous streaming sequences. When searching for neural mechanisms of stream segregation, it should be kept in mind that such decision processes may contribute to brain activation, as also suggested by recent human imaging data. The present study proposes that the uncertainty of a subject in making a decision about the perceptual organization of ambiguous streaming sequences may be reflected in the time required to make an initial decision. To this end, subjects had to decide on their current percept while listening to ABAB auditory streaming sequences. Each sequence had a duration of 30 s and was composed of A and B harmonic tone complexes differing in fundamental frequency (ΔF). Sequences with seven different ΔF were tested. We found that the initial decision time varied non-monotonically with ΔF and that it was significantly correlated with the degree of perceptual ambiguity defined from the proportions of time the subjects reported a one-stream or a two-stream percept subsequent to the first decision. This strong relation of the proposed measures of decision uncertainty and perceptual ambiguity should be taken into account when searching for neural correlates of auditory stream segregation.},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  urldate = {2015-12-14},
  date = {2015-08-11},
  author = {Deike, Susann and Heil, Peter and Böckmann-Barthel, Martin and Brechmann, André},
  file = {D\:\\Sauve\\Zotero\\storage\\USIJU7EK\\Deike et al. - 2015 - Decision making and ambiguity in auditory stream s.pdf},
  eprinttype = {pmid},
  eprint = {26321899},
  pmcid = {PMC4531241}
}

@article{bendixenFeaturePredictabilityFlexibly2014,
  title = {Feature {{Predictability Flexibly Supports Auditory Stream Segregation}} or {{Integration}}},
  volume = {100},
  doi = {10.3813/AAA.918768},
  abstract = {Many sound sources emit series of discrete sounds. Auditory perception must bind these sounds together (stream integration) while separating them from sounds emitted by other sources (stream segregation). One cue for identifying successive sounds that belong together is the predictability between their feature values. Previous studies have demonstrated that independent predictable patterns appearing separately in two interleaved sound sequences support perceptual segregation. The converse case, whether a joint predictable pattern in a mixture of interleaved sequences supports perceptual integration, has not yet been put to a rigorous empirical test. This was mainly due to difficulties in manipulating the predictability of the full sequence independently of the predictability of the interleaved subsequences. The present study implemented such an independent manipulation. Listeners continuously indicated whether they perceived a tone sequence as integrated or segregated, while predictable patterns set up to support one or the other percept were manipulated without the participants' knowledge. Perceptual reports demonstrate that predictability supports stream segregation or integration depending on the type of predictable pattern that is present in the sequence. The effects of predictability were so pronounced as to qualitatively flip perception from predominantly (62\%) integrated to predominantly (73\%) segregated. These results suggest that auditory perception flexibly responds to encountered regular patterns, favoring predictable perceptual organizations over unpredictable ones. Besides underlining the role of predictability as a cue within auditory scene analysis, the present design also provides a general framework that accommodates previous investigations focusing on sub-comparisons within the present set of experimental manipulations. Results of intermediate conditions shed light on why some previous studies have obtained little to no effects of predictability on auditory scene analysis.},
  number = {5},
  journaltitle = {Acta Acustica united with Acustica},
  shortjournal = {Acta Acustica united with Acustica},
  date = {2014-09-01},
  pages = {888-899},
  author = {Bendixen, Alexandra and Denham, Susan L. and Winkler, István}
}

@article{sussman-fortEffectStimulusContext2014,
  title = {The Effect of Stimulus Context on the Buildup to Stream Segregation},
  volume = {8},
  issn = {1662-4548},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4010780/},
  doi = {10.3389/fnins.2014.00093},
  abstract = {Stream segregation is the process by which the auditory system disentangles the mixture of sound inputs into discrete sources that cohere across time. The length of time required for this to occur is termed the “buildup” period. In the current study, we used the buildup period as an index of how quickly sounds are segregated into constituent parts. Specifically, we tested the hypothesis that stimulus context impacts the timing of the buildup and, therefore, affects when stream segregation is detected. To measure the timing of the buildup we recorded the Mismatch Negativity component (MMN) of event-related brain potentials (ERPs), during passive listening, to determine when the streams were neurophysiologically segregated. In each condition, a pattern of repeating low (L) and high (H) tones (L-L-H) was presented in trains of stimuli separated by silence, with the H tones forming a simple intensity oddball paradigm and the L tones serving as distractors. To determine the timing of the buildup, probe tones occurred in two positions of the trains, early (within the buildup period) and late (past the buildup period). The context was manipulated by presenting roving vs. non-roving frequencies across trains in two conditions. MMNs were elicited by intensity probe tones in the Non-Roving condition (early and late positions) and the Roving condition (late position only) indicating that neurophysiologic segregation occurred faster in the Non-Roving condition. This suggests a shorter buildup period when frequency was repeated from train to train. Overall, our results demonstrate that the dynamics of the environment influence the way in which the auditory system extracts regularities from the input. The results support the hypothesis that the buildup to segregation is highly dependent upon stimulus context and that the auditory system works to maintain a consistent representation of the environment when no new information suggests that reanalyzing the scene is necessary.},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  urldate = {2015-12-14},
  date = {2014-04-29},
  author = {Sussman-Fort, Jonathan and Sussman, Elyse},
  file = {D\:\\Sauve\\Zotero\\storage\\DESM8DC2\\Sussman-Fort and Sussman - 2014 - The effect of stimulus context on the buildup to s.pdf},
  eprinttype = {pmid},
  eprint = {24808822},
  pmcid = {PMC4010780}
}

@article{bigandAreWeExperienced2006,
  langid = {english},
  title = {Are We "Experienced Listeners"? {{A}} Review of the Musical Capacities That Do Not Depend on Formal Musical Training},
  volume = {100},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2005.11.007},
  shorttitle = {Are We "Experienced Listeners"?},
  abstract = {The present paper reviews a set of studies designed to investigate different aspects of the capacity for processing Western music. This includes perceiving the relationships between a theme and its variations, perceiving musical tensions and relaxations, generating musical expectancies, integrating local structures in large-scale structures, learning new compositional systems and responding to music in an emotional (affective) way. The main focus of these studies was to evaluate the influence of intensive musical training on these capacities. The overall set of data highlights that some musical capacities are acquired through exposure to music without the help of explicit training. These capacities reach such a degree of sophistication that they enable untrained listeners to respond to music as "musically experienced listeners" do.},
  number = {1},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  date = {2006-05},
  pages = {100-130},
  keywords = {cognition,Auditory Perception,Music,pitch perception,humans,Learning,Psychoacoustics,emotions},
  author = {Bigand, E. and Poulin-Charronnat, B.},
  eprinttype = {pmid},
  eprint = {16412412}
}

@article{vollmannInstrumentSpecificUsedependent2014,
  title = {Instrument Specific Use-Dependent Plasticity Shapes the Anatomical Properties of the Corpus Callosum: A Comparison between Musicians and Non-Musicians},
  volume = {8},
  issn = {1662-5153},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4100438/},
  doi = {10.3389/fnbeh.2014.00245},
  shorttitle = {Instrument Specific Use-Dependent Plasticity Shapes the Anatomical Properties of the Corpus Callosum},
  abstract = {Long-term musical expertise has been shown to be associated with a number of functional and structural brain changes, making it an attractive model for investigating use-dependent plasticity in humans. Physiological interhemispheric inhibition (IHI) as examined by transcranial magnetic stimulation has been shown to be correlated with anatomical properties of the corpus callosum as indexed by fractional anisotropy (FA). However, whether or not IHI or the relationship between IHI and FA in the corpus callosum can be modified by different musical training regimes remains largely unknown. We investigated this question in musicians with different requirements for bimanual finger movements (piano and string players) and non-expert controls. IHI values were generally higher in musicians, but differed significantly from non-musicians only in string players. IHI was correlated with FA in the posterior midbody of the corpus callosum across all participants. Interestingly, subsequent analyses revealed that this relationship may indeed be modulated by different musical training regimes. Crucially, while string players had greater IHI than non-musicians and showed a positive structure-function relationship, the amount of IHI in pianists was comparable to that of non-musicians and there was no significant structure-function relationship. Our findings indicate instrument specific use-dependent plasticity in both functional (IHI) and structural (FA) connectivity of motor related brain regions in musicians.},
  journaltitle = {Frontiers in Behavioral Neuroscience},
  shortjournal = {Front Behav Neurosci},
  urldate = {2015-12-12},
  date = {2014-07-16},
  author = {Vollmann, Henning and Ragert, Patrick and Conde, Virginia and Villringer, Arno and Classen, Joseph and Witte, Otto W. and Steele, Christopher J.},
  file = {D\:\\Sauve\\Zotero\\storage\\49AKJ6CV\\Vollmann et al. - 2014 - Instrument specific use-dependent plasticity shape.pdf},
  eprinttype = {pmid},
  eprint = {25076879},
  pmcid = {PMC4100438}
}

@article{tervaniemiMusiciansSameDifferent2009,
  langid = {english},
  title = {Musicians—{{Same}} or {{Different}}?},
  volume = {1169},
  issn = {1749-6632},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.2009.04591.x/abstract},
  doi = {10.1111/j.1749-6632.2009.04591.x},
  abstract = {In the neuroscience of music, musicians have traditionally been treated as a unified group, as if the demands set by their musical activities would be more or less equal in terms of perceptual, cognitive, and motor functions. However, obviously, their musical preferences differentiate them to a higher degree, for instance, in terms of the instrument they choose and the music genre they are mostly engaged with as well as their practicing style. This diversity in musicians’ profiles has been recently taken into account in several empirical endeavors. The present contribution will review the evidence available about the various neurocognitive profiles these different kinds of musicians display.},
  number = {1},
  journaltitle = {Annals of the New York Academy of Sciences},
  urldate = {2015-12-12},
  date = {2009-07-01},
  pages = {151-156},
  keywords = {musical expertise,NEUROPLASTICITY,musical knowledge},
  author = {Tervaniemi, Mari},
  file = {D\:\\Sauve\\Zotero\\storage\\8MG76TC9\\abstract.html}
}

@article{drostWhenHearingTurns2005,
  title = {When Hearing Turns into Playing: {{Movement}} Induction by Auditory Stimuli in Pianists},
  volume = {58},
  issn = {0272-4987},
  url = {http://dx.doi.org/10.1080/02724980443000610},
  doi = {10.1080/02724980443000610},
  shorttitle = {When Hearing Turns into Playing},
  abstract = {In this study, pianists were tested for learned associations between actions (movements on the piano) and their perceivable sensory effects (piano tones). Actions were examined that required the playing of two-tone sequences (intervals) in a four-choice paradigm. In Experiment 1, the intervals to be played were denoted by visual note stimuli. Concurrently with these imperative stimuli, task-irrelevant auditory distractor intervals were presented (“potential” action effects, congruent or incongruent). In Experiment 2, imperative stimuli were coloured squares, in order to exclude possible influences of spatial relationships of notes, responses, and auditory stimuli. In both experiments responses in the incongruent conditions were slower than those in the congruent conditions. Also, heard intervals actually “induced” false responses. The reaction time effects were more pronounced in Experiment 2. In nonmusicians (Experiment 3), no evidence for interference could be observed. Thus, our results show that in expert pianists potential action effects are able to induce corresponding actions, which demonstrates the existence of acquired action–effect associations in pianists.},
  number = {8},
  journaltitle = {The Quarterly Journal of Experimental Psychology Section A},
  urldate = {2015-12-12},
  date = {2005-11-01},
  pages = {1376-1389},
  author = {Drost, Ulrich C. and Rieger, Martina and Brass, Marcel and Gunter, Thomas C. and Prinz, Wolfgang},
  file = {D\:\\Sauve\\Zotero\\storage\\GIUSMGZG\\02724980443000610.html}
}

@article{stewartActionPerceptionCoupling2013,
  title = {Action–Perception Coupling in Pianists: {{Learned}} Mappings or Spatial Musical Association of Response Codes ({{SMARC}}) Effect?},
  volume = {66},
  issn = {1747-0218},
  url = {http://dx.doi.org/10.1080/17470218.2012.687385},
  doi = {10.1080/17470218.2012.687385},
  shorttitle = {Action–Perception Coupling in Pianists},
  abstract = {The principle of common coding suggests that a joint representation is formed when actions are repeatedly paired with a specific perceptual event. Musicians are occupationally specialized with regard to the coupling between actions and their auditory effects. In the present study, we employed a novel paradigm to demonstrate automatic action–effect associations in pianists. Pianists and nonmusicians pressed keys according to aurally presented number sequences. Numbers were presented at pitches that were neutral, congruent, or incongruent with respect to pitches that would normally be produced by such actions. Response time differences were seen between congruent and incongruent sequences in pianists alone. A second experiment was conducted to determine whether these effects could be attributed to the existence of previously documented spatial/pitch compatibility effects. In a “stretched” version of the task, the pitch distance over which the numbers were presented was enlarged to a range that could not be produced by the hand span used in Experiment 1. The finding of a larger response time difference between congruent and incongruent trials in the original, standard, version compared with the stretched version, in pianists, but not in nonmusicians, indicates that the effects obtained are, at least partially, attributable to learned action effects.},
  number = {1},
  journaltitle = {The Quarterly Journal of Experimental Psychology},
  urldate = {2015-12-12},
  date = {2013-01-01},
  pages = {37-50},
  author = {Stewart, Lauren and Verdonschot, Rinus G. and Nasralla, Patrick and Lanipekun, Jennifer},
  file = {D\:\\Sauve\\Zotero\\storage\\A6VJA2WN\\17470218.2012.html},
  eprinttype = {pmid},
  eprint = {22712516}
}

@article{reppPerformedObservedKeyboard2009,
  title = {Performed or Observed Keyboard Actions Affect Pianists' Judgements of Relative Pitch},
  volume = {62},
  issn = {1747-0218},
  url = {http://dx.doi.org/10.1080/17470210902745009},
  doi = {10.1080/17470210902745009},
  abstract = {Action can affect visual perception if the action's expected sensory effects resemble a concurrent unstable or deviant event. To determine whether action can also change auditory perception, participants were required to play pairs of octave-ambiguous tones by pressing successive keys on a piano or computer keyboard and to judge whether each pitch interval was rising or falling. Both pianists and nonpianist musicians gave significantly more “rising” responses when the order of key presses was left-to-right than when it was right-to-left, in accord with the pitch mapping of the piano. However, the effect was much larger in pianists. Pianists showed a similarly large effect when they passively observed the experimenter pressing keys on a piano keyboard, as long as the keyboard faced the participant. The results suggest that acquired action–effect associations can affect auditory perceptual judgement.},
  number = {11},
  journaltitle = {The Quarterly Journal of Experimental Psychology},
  urldate = {2015-12-12},
  date = {2009-11-01},
  pages = {2156-2170},
  author = {Repp, Bruno H. and Knoblich, Günther},
  file = {D\:\\Sauve\\Zotero\\storage\\8T6USC9F\\17470210902745009.html}
}

@article{drostInstrumentSpecificityExperienced2007,
  title = {Instrument Specificity in Experienced Musicians},
  volume = {60},
  issn = {1747-0218},
  url = {http://dx.doi.org/10.1080/17470210601154388},
  doi = {10.1080/17470210601154388},
  abstract = {Previous studies have shown that experienced pianists have acquired integrated action–effect (A–E) associations. In the present study, we were interested in how specific these associations are for the own instrument by investigating pianists and guitarists. A–E associations were examined by testing whether the perception of a “potential” action–effect has an influence on actions. Participants played chords on their instrument in response to visual stimuli, while they were presented task-irrelevant auditory distractors (congruent or incongruent) in varying instrument timbre. In Experiment 1, pianists exhibited an interference effect with timbres of their own instrument category (keyboard instruments: piano and organ). In Experiment 2 guitarists showed an interference effect only with guitar timbre. Thus, integrated A–E associations primarily seem to consist of a specific component on a sensory-motor level involving the own instrument. Additionally, categorical knowledge about how an instrument is played seems to be involved.},
  number = {4},
  journaltitle = {The Quarterly Journal of Experimental Psychology},
  urldate = {2015-12-12},
  date = {2007-04-01},
  pages = {527-533},
  author = {Drost, Ulrich C. and Rieger, Martina and Prinz, Wolfgang},
  file = {D\:\\Sauve\\Zotero\\storage\\I9JB6A3T\\17470210601154388.html}
}

@article{rusconiSpatialRepresentationPitch2006,
  title = {Spatial Representation of Pitch Height: The {{SMARC}} Effect},
  volume = {99},
  issn = {0010-0277},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027705000260},
  doi = {10.1016/j.cognition.2005.01.004},
  shorttitle = {Spatial Representation of Pitch Height},
  abstract = {Through the preferential pairing of response positions to pitch, here we show that the internal representation of pitch height is spatial in nature and affects performance, especially in musically trained participants, when response alternatives are either vertically or horizontally aligned. The finding that our cognitive system maps pitch height onto an internal representation of space, which in turn affects motor performance even when this perceptual attribute is irrelevant to the task, extends previous studies on auditory perception and suggests an interesting analogy between music perception and mathematical cognition. Both the basic elements of mathematical cognition (i.e. numbers) and the basic elements of musical cognition (i.e. pitches), appear to be mapped onto a mental spatial representation in a way that affects motor performance.},
  number = {2},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  urldate = {2015-12-12},
  date = {2006-03},
  pages = {113-129},
  keywords = {Pitch height,SMARC effect,SNARC effect,Spatial representation,Stimulus-Response Compatability},
  author = {Rusconi, Elena and Kwan, Bonnie and Giordano, Bruno L. and Umiltà, Carlo and Butterworth, Brian},
  file = {D\:\\Sauve\\Zotero\\storage\\983BC7JD\\S0010027705000260.html}
}

@article{parsonsBrainBasisPiano2005,
  title = {The Brain Basis of Piano Performance},
  volume = {43},
  issn = {0028-3932},
  url = {http://www.sciencedirect.com/science/article/pii/S0028393204002830},
  doi = {10.1016/j.neuropsychologia.2004.11.007},
  abstract = {Performances of memorized piano compositions unfold via dynamic integrations of motor, perceptual, cognitive, and emotive operations. The functional neuroanatomy of such elaborately skilled achievements was characterized in the present study by using 150-water positron emission tomography to image blindfolded pianists performing a concerto by J.S. Bach. The resulting brain activity was referenced to that for bimanual performance of memorized major scales. Scales and concerto performances both activated primary motor cortex, corresponding somatosensory areas, inferior parietal cortex, supplementary motor area, motor cingulate, bilateral superior and middle temporal cortex, right thalamus, anterior and posterior cerebellum. Regions specifically supporting the concerto performance included superior and middle temporal cortex, planum polare, thalamus, basal ganglia, posterior cerebellum, dorsolateral premotor cortex, right insula, right supplementary motor area, lingual gyrus, and posterior cingulate. Areas specifically implicated in generating and playing scales were posterior cingulate, middle temporal, right middle frontal, and right precuneus cortices, with lesser increases in right hemispheric superior temporal, temporoparietal, fusiform, precuneus, and prefrontal cortices, along with left inferior frontal gyrus. Finally, much greater deactivations were present for playing the concerto than scales. This seems to reflect a deeper attentional focus in which tonically active orienting and evaluative processes, among others, are suspended. This inference is supported by observed deactivations in posterior cingulate, parahippocampus, precuneus, prefrontal, middle temporal, and posterior cerebellar cortices. For each of the foregoing analyses, a distributed set of interacting localized functions is outlined for future test.},
  number = {2},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  series = {Movement, {{Action}} and {{Consciousness}}: {{Toward}} a {{Physiology}} of {{Intentionality A Special Issue}} in {{Honour}} of {{Marc Jeannerod}}},
  urldate = {2015-12-12},
  date = {2005},
  pages = {199-215},
  keywords = {music cognition,music performance,Functional neuroimaging,Piano performance},
  author = {Parsons, Lawrence M. and Sergent, Justine and Hodges, Donald A. and Fox, Peter T.},
  file = {D\:\\Sauve\\Zotero\\storage\\WGIR3K26\\S0028393204002830.html}
}

@article{menonNeuralCorrelatesTimbre2002,
  title = {Neural {{Correlates}} of {{Timbre Change}} in {{Harmonic Sounds}}},
  volume = {17},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811902912954},
  doi = {10.1006/nimg.2002.1295},
  abstract = {Timbre is a major structuring force in music and one of the most important and ecologically relevant features of auditory events. We used sound stimuli selected on the basis of previous psychophysiological studies to investigate the neural correlates of timbre perception. Our results indicate that both the left and right hemispheres are involved in timbre processing, challenging the conventional notion that the elementary attributes of musical perception are predominantly lateralized to the right hemisphere. Significant timbre-related brain activation was found in well-defined regions of posterior Heschl's gyrus and superior temporal sulcus, extending into the circular insular sulcus. Although the extent of activation was not significantly different between left and right hemispheres, temporal lobe activations were significantly posterior in the left, compared to the right, hemisphere, suggesting a functional asymmetry in their respective contributions to timbre processing. The implications of our findings for music processing in particular and auditory processing in general are discussed.},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2015-12-12},
  date = {2002-12},
  pages = {1742-1754},
  keywords = {timbre,fMRI,middle temporal gyrus,superior temporal gyrus,superior temporal sulcus,temporal lobe},
  author = {Menon, V. and Levitin, D. J. and Smith, B. K. and Lembke, A. and Krasnow, B. D. and Glazer, D. and Glover, G. H. and McAdams, S.},
  file = {D\:\\Sauve\\Zotero\\storage\\MS5WJDFR\\S1053811902912954.html}
}

@article{shahinMusicTrainingLeads2008,
  title = {Music Training Leads to the Development of Timbre-Specific Gamma Band Activity},
  volume = {41},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811908001092},
  doi = {10.1016/j.neuroimage.2008.01.067},
  abstract = {Oscillatory gamma band activity (GBA, 30–100~Hz) has been shown to correlate with perceptual and cognitive phenomena including feature binding, template matching, and learning and memory formation. We hypothesized that if GBA reflects highly learned perceptual template matching, we should observe its development in musicians specific to the timbre of their instrument of practice. EEG was recorded in adult professional violinists and amateur pianists as well as in 4- and 5-year-old children studying piano in the Suzuki method before they commenced music lessons and 1 year later. The adult musicians showed robust enhancement of induced (non-time-locked) GBA, specifically to their instrument of practice, with the strongest effect in professional violinists. Consistent with this result, the children receiving piano lessons exhibited increased power of induced GBA for piano tones with 1 year of training, while children not taking lessons showed no effect. In comparison to induced GBA, evoked (time-locked) gamma band activity (30–90~Hz, ∼ 80~ms latency) was present only in adult groups. Evoked GBA was more pronounced in musicians than non-musicians, with synchronization equally exhibited for violin and piano tones but enhanced for these tones compared to pure tones. Evoked gamma activity may index the physical properties of a sound and is modulated by acoustical training, while induced GBA may reflect higher perceptual learning and is shaped by specific auditory experiences.},
  number = {1},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2015-12-12},
  date = {2008-05-15},
  pages = {113-122},
  keywords = {plasticity,Development,Auditory Cortex,Gamma band activity,Music training,Timbre-specificity},
  author = {Shahin, Antoine J. and Roberts, Larry E. and Chau, Wilkin and Trainor, Laurel J. and Miller, Lee M.},
  file = {D\:\\Sauve\\Zotero\\storage\\SQU429P9\\S1053811908001092.html}
}

@article{nortonAreTherePreexisting2005,
  title = {Are There Pre-Existing Neural, Cognitive, or Motoric Markers for Musical Ability?},
  volume = {59},
  issn = {0278-2626},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262605000898},
  doi = {10.1016/j.bandc.2005.05.009},
  abstract = {Adult musician’s brains show structural enlargements, but it is not known whether these are inborn or a consequence of long-term training. In addition, music training in childhood has been shown to have positive effects on visual–spatial and verbal outcomes. However, it is not known whether pre-existing advantages in these skills are found in children who choose to study a musical instrument nor is it known whether there are pre-existing associations between music and any of these outcome measures that could help explain the training effects. To answer these questions, we compared 5- to 7-year-olds beginning piano or string lessons (n = 39) with 5- to 7-year-olds not beginning instrumental training (n = 31). All children received a series of tests (visual–spatial, non-verbal reasoning, verbal, motor, and musical) and underwent magnetic resonance imaging. We found no pre-existing neural, cognitive, motor, or musical differences between groups and no correlations (after correction for multiple analyses) between music perceptual skills and any brain or visual–spatial measures. However, correlations were found between music perceptual skills and both non-verbal reasoning and phonemic awareness. Such pre-existing correlations suggest similarities in auditory and visual pattern recognition as well a sharing of the neural substrates for language and music processing, most likely due to innate abilities or implicit learning during early development. This baseline study lays the groundwork for an ongoing longitudinal study addressing the effects of intensive musical training on brain and cognitive development, and making it possible to look retroactively at the brain and cognitive development of those children who emerge showing exceptional musical talent.},
  number = {2},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain and Cognition},
  urldate = {2015-12-12},
  date = {2005-11},
  pages = {124-134},
  keywords = {brain,cognition,Development,Music},
  author = {Norton, Andrea and Winner, Ellen and Cronin, Karl and Overy, Katie and Lee, Dennis J. and Schlaug, Gottfried},
  file = {D\:\\Sauve\\Zotero\\storage\\9QRMNE5M\\S0278262605000898.html}
}

@article{kendallPerceptualScalingSimultaneous1991,
  eprinttype = {jstor},
  eprint = {40285519},
  title = {Perceptual {{Scaling}} of {{Simultaneous Wind Instrument Timbres}}},
  volume = {8},
  issn = {0730-7829},
  doi = {10.2307/40285519},
  abstract = {Timbral similarities among wind instrument duos were studied. Flute, oboe, E턬 alto saxophone, B턬{$>$} clarinet, and Bl{$>$} trumpet instrumentalists performed in all possible duo pairings (dyads). Source material included B턬4 unisons, unison melody, major thirds, and harmonized melody. Nonunison combinations had each instrument of the pair as the soprano, creating a total of six contexts. Music major and nonmusic major subjects rated the similarity of all possible pairs of dyads in each of the six contexts. Classical multidimensional scaling (MDS) was performed; contexts were treated as " subjects" in an individual differences scaling (INDSCAL) analysis of composite data. The resulting spaces had two stable, interpretable dimensions. From verbal attribute rating experiments ( Kendall \& Carterette, in preparation, a), these were identified as " nasal" vs. " not nasal," and " rich" vs. " brilliant." A third dimension was interpreted as "simple" vs. "complex."Extrema in the space were associated with three of the five instruments: Trumpet (brilliant), saxophone (rich), and oboe ( nasal). Data that were amalgamated over contexts and plotted in two dimensions yielded a circumplicial configuration. Implications for orchestration are discussed and a theoretical model of timbre combinations and groupings is presented.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1991},
  pages = {369-404},
  author = {Kendall, Roger A. and Carterette, Edward C.}
}

@article{lakatosCommonPerceptualSpace2000,
  langid = {english},
  title = {A Common Perceptual Space for Harmonic and Percussive Timbres},
  volume = {62},
  issn = {0031-5117},
  abstract = {The goal of a series of listening tests was to better isolate the principal dimensions of timbre, using a wide range of timbres and converging psychophysical techniques. Expert musicians and nonmusicians rated the timbral similarity of three sets of pitched and percussive instruments. Multidimensional scaling analyses indicated that both centroid and rise time comprise the principal acoustic factors across all stimulus sets and that musicians and nonmusicians did not differ significantly in their weighting of these factors. Clustering analyses revealed that participants also categorized percussive and, to a much lesser extent, pitched timbres according to underlying physical-acoustic commonalties. The findings demonstrate that spectral centroid and rise time represent principal perceptual dimensions of timbre, independent of musical training, but that the tendency to group timbres according to source properties increases with acoustic complexity.},
  number = {7},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Percept Psychophys},
  date = {2000-10},
  pages = {1426-1439},
  keywords = {Music,Pitch Discrimination,humans,Adult,Female,Male,Psychoacoustics,Adolescent,Sound Spectrography},
  author = {Lakatos, S.},
  eprinttype = {pmid},
  eprint = {11143454}
}

@article{schrogerBridgingPredictionAttention2015,
  title = {Bridging Prediction and Attention in Current Research on Perception and Action},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315006885},
  doi = {10.1016/j.brainres.2015.08.037},
  abstract = {Prediction and attention are fundamental brain functions in the service of perception and action. Theories on prediction relate to neural (mental) models inferring about (present or future) sensory or action-related information, whereas theories of attention are about the control of information flow underlying perception and action. Both concepts are related and not always clearly distinguishable. The special issue includes current research on prediction and attention in various subfields of perception and action. It especially considers interactions between predictive and attentive processes, which constitute a newly emerging and highly interesting field of research. As outlined in this editorial, the contributions in this special issue allow specifying as well as bridging concepts on prediction and attention. The joint consideration of prediction and attention also reveals common functional principles of perception and action.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {1-13},
  author = {Schröger, Erich and Kotz, Sonja A. and SanMiguel, Iria},
  file = {D\:\\Sauve\\Zotero\\storage\\R8IZ2VVE\\Schröger et al. - 2015 - Bridging prediction and attention in current resea.pdf;D\:\\Sauve\\Zotero\\storage\\BKKK3Q8F\\S0006899315006885.html}
}

@article{allenmarkRepetitionPrimingResults2015,
  title = {Repetition Priming Results in Sensitivity Attenuation},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315002279},
  doi = {10.1016/j.brainres.2015.03.030},
  abstract = {Repetition priming refers to the change in the ability to perform a task on a stimulus as a consequence of a former encounter with that very same item. Usually, repetition results in faster and more accurate performance. In the present study, we used a contrast discrimination protocol to assess perceptual sensitivity and response bias of Gabor gratings that are either repeated (same orientation) or alternated (different orientation). We observed that contrast discrimination performance is worse, not better, for repeated than for alternated stimuli. In a second experiment, we varied the probability of stimulus repetition, thus testing whether the repetition effect is due to bottom-up or top-down factors. We found that it is top-down expectation that determines the effect. We discuss the implication of these findings for repetition priming and related phenomena as sensory attenuation.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {211-217},
  keywords = {Prediction,Repetition suppression,SDT},
  author = {Allenmark, Fredrik and Hsu, Yi-Fang and Roussel, Cedric and Waszak, Florian},
  file = {D\:\\Sauve\\Zotero\\storage\\JGHXTQVF\\Allenmark et al. - 2015 - Repetition priming results in sensitivity attenuat.pdf;D\:\\Sauve\\Zotero\\storage\\IKH8ZJ5J\\S0006899315002279.html}
}

@article{alhoTopdownControlledBottomup2015,
  title = {Top-down Controlled and Bottom-up Triggered Orienting of Auditory Attention to Pitch Activate Overlapping Brain Networks},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899314017570},
  doi = {10.1016/j.brainres.2014.12.050},
  abstract = {A number of previous studies have suggested segregated networks of brain areas for top-down controlled and bottom-up triggered orienting of visual attention. However, the corresponding networks involved in auditory attention remain less studied. Our participants attended selectively to a tone stream with either a lower pitch or higher pitch in order to respond to infrequent changes in duration of attended tones. The participants were also required to shift their attention from one stream to the other when guided by a visual arrow cue. In addition to these top-down controlled cued attention shifts, infrequent task-irrelevant louder tones occurred in both streams to trigger attention in a bottom-up manner. Both cued shifts and louder tones were associated with enhanced activity in the superior temporal gyrus and sulcus, temporo-parietal junction, superior parietal lobule, inferior and middle frontal gyri, frontal eye field, supplementary motor area, and anterior cingulate gyrus. Thus, the present findings suggest that in the auditory modality, unlike in vision, top-down controlled and bottom-up triggered attention activate largely the same cortical networks. Comparison of the present results with our previous results from a similar experiment on spatial auditory attention suggests that fronto-parietal networks of attention to location or pitch overlap substantially. However, the auditory areas in the anterior superior temporal cortex might have a more important role in attention to the pitch than location of sounds.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {136-145},
  keywords = {Attention,pitch,fMRI,auditory,Bottom-up triggered,Top-down controlled},
  author = {Alho, Kimmo and Salmi, Juha and Koistinen, Sonja and Salonen, Oili and Rinne, Teemu},
  file = {D\:\\Sauve\\Zotero\\storage\\N52RM9RX\\Alho et al. - 2015 - Top-down controlled and bottom-up triggered orient.pdf;D\:\\Sauve\\Zotero\\storage\\3Q2DN43B\\S0006899314017570.html}
}

@article{hadenPredictiveProcessingPitch2015,
  title = {Predictive Processing of Pitch Trends in Newborn Infants},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315001778},
  doi = {10.1016/j.brainres.2015.02.048},
  abstract = {The notion of predictive sound processing suggests that the auditory system prepares for upcoming sounds once it has detected regular features within a sequence. Here we investigated whether predictive processes are operating at birth in the human auditory system. Event-related potentials (ERP) were recorded from healthy newborns to occasional ascending pitch steps occurring in the 2nd or the 5th position within trains of tones with otherwise monotonously descending pitch. If the trains were processed in a predictive manner only deviant pitch steps occurring in the later train position would elicit the discriminative mismatch response (MMR). Deviants delivered in the 5th but not in the 2nd position of the tone trains elicited a significant MMR response. These results suggest that newborns represent pitch trends within sound sequences and they process them in a predictive manner.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {14-20},
  keywords = {ERP,pitch,Abstract rule,MMN,MMR,Newborn infant,Predictive processing},
  author = {Háden, Gábor P. and Németh, Renáta and Török, Miklós and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\WRSHJMWJ\\Háden et al. - 2015 - Predictive processing of pitch trends in newborn i.pdf;D\:\\Sauve\\Zotero\\storage\\TRVBS3KB\\S0006899315001778.html}
}

@article{grubertDoesVisualWorking2015,
  title = {Does Visual Working Memory Represent the Predicted Locations of Future Target Objects? {{An}} Event-Related Brain Potential Study},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S000689931401364X},
  doi = {10.1016/j.brainres.2014.10.011},
  shorttitle = {Does Visual Working Memory Represent the Predicted Locations of Future Target Objects?},
  abstract = {During the maintenance of task-relevant objects in visual working memory, the contralateral delay activity (CDA) is elicited over the hemisphere opposite to the visual field where these objects are presented. The presence of this lateralised CDA component demonstrates the existence of position-dependent object representations in working memory. We employed a change detection task to investigate whether the represented object locations in visual working memory are shifted in preparation for the known location of upcoming comparison stimuli. On each trial, bilateral memory displays were followed after a delay period by bilateral test displays. Participants had to encode and maintain three visual objects on one side of the memory display, and to judge whether they were identical or different to three objects in the test display. Task-relevant memory and test stimuli were located in the same visual hemifield in the no-shift task, and on opposite sides in the horizontal shift task. CDA components of similar size were triggered contralateral to the memorized objects in both tasks. The absence of a polarity reversal of the CDA in the horizontal shift task demonstrated that there was no preparatory shift of memorized object location towards the side of the upcoming comparison stimuli. These results suggest that visual working memory represents the locations of visual objects during encoding, and that the matching of memorized and test objects at different locations is based on a comparison process that can bridge spatial translations between these objects.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {258-266},
  keywords = {Prediction,selective attention,Event-related brain potential,Preparation,Working memory},
  author = {Grubert, Anna and Eimer, Martin},
  file = {D\:\\Sauve\\Zotero\\storage\\DCQVHIH5\\Grubert and Eimer - 2015 - Does visual working memory represent the predicted.pdf;D\:\\Sauve\\Zotero\\storage\\ZEEH2JWG\\S000689931401364X.html}
}

@article{cornellaSpatialAuditoryRegularity2015,
  title = {Spatial Auditory Regularity Encoding and Prediction: {{Human}} Middle-Latency and Long-Latency Auditory Evoked Potentials},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315003078},
  doi = {10.1016/j.brainres.2015.04.018},
  shorttitle = {Spatial Auditory Regularity Encoding and Prediction},
  abstract = {By encoding acoustic regularities present in the environment, the human brain can generate predictions of what is likely to occur next. Recent studies suggest that deviations from encoded regularities are detected within 10–50 ms after stimulus onset, as indicated by electrophysiological effects in the middle latency response (MLR) range. This is upstream of previously known long-latency (LLR) signatures of deviance detection such as the mismatch negativity (MMN) component. In the present study, we created predictable and unpredictable contexts to investigate MLR and LLR signatures of the encoding of spatial auditory regularities and the generation of predictions from these regularities. Chirps were monaurally delivered in an either regular (predictable: left–right–left–right) or a random (unpredictable left/right alternation or repetition) manner. Occasional stimulus omissions occurred in both types of sequences. Results showed that the Na component (peaking at 34 ms after stimulus onset) was attenuated for regular relative to random chirps, albeit no differences were observed for stimulus omission responses in the same latency range. In the LLR range, larger chirp-and omission-evoked responses were elicited for the regular than for the random condition, and predictability effects were more prominent over the right hemisphere. We discuss our findings in the framework of a hierarchical organization of spatial regularity encoding.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {21-30},
  keywords = {Prediction,Auditory regularity,Long latency response,Middle latency response},
  author = {Cornella, M. and Bendixen, A. and Grimm, S. and Leung, S. and Schröger, E. and Escera, C.},
  file = {D\:\\Sauve\\Zotero\\storage\\BV8M3VT5\\Cornella et al. - 2015 - Spatial auditory regularity encoding and predictio.pdf;D\:\\Sauve\\Zotero\\storage\\JKV7SDXP\\S0006899315003078.html}
}

@article{guoEffectsSupervisedLearning2015,
  title = {The Effects of Supervised Learning on Event-Related Potential Correlates of Music-Syntactic Processing},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315000669},
  doi = {10.1016/j.brainres.2015.01.046},
  abstract = {Humans process music even without conscious effort according to implicit knowledge about syntactic regularities. Whether such automatic and implicit processing is modulated by veridical knowledge has remained unknown in previous neurophysiological studies. This study investigates this issue by testing whether the acquisition of veridical knowledge of a music-syntactic irregularity (acquired through supervised learning) modulates early, partly automatic, music-syntactic processes (as reflected in the early right anterior negativity, ERAN), and/or late controlled processes (as reflected in the late positive component, LPC). Excerpts of piano sonatas with syntactically regular and less regular chords were presented repeatedly (10 times) to non-musicians and amateur musicians. Participants were informed by a cue as to whether the following excerpt contained a regular or less regular chord. Results showed that the repeated exposure to several presentations of regular and less regular excerpts did not influence the ERAN elicited by less regular chords. By contrast, amplitudes of the LPC (as well as of the P3a evoked by less regular chords) decreased systematically across learning trials. These results reveal that late controlled, but not early (partly automatic), neural mechanisms of music-syntactic processing are modulated by repeated exposure to a musical piece.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {232-246},
  keywords = {musical training,musical syntax,Implicit knowledge,Supervised learning,Veridical knowledge},
  author = {Guo, Shuang and Koelsch, Stefan},
  file = {D\:\\Sauve\\Zotero\\storage\\M5JNACVD\\Guo and Koelsch - 2015 - The effects of supervised learning on event-relate.pdf;D\:\\Sauve\\Zotero\\storage\\6JR57NZK\\S0006899315000669.html}
}

@article{sulykosAsymmetricEffectAutomatic2015,
  title = {Asymmetric Effect of Automatic Deviant Detection: {{The}} Effect of Familiarity in Visual Mismatch Negativity},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315001353},
  doi = {10.1016/j.brainres.2015.02.035},
  shorttitle = {Asymmetric Effect of Automatic Deviant Detection},
  abstract = {The visual mismatch negativity (vMMN) component is regarded as a prediction error signal elicited by events violating the sequential regularities of environmental stimulation. The aim of the study was to investigate the effect of familiarity on the vMMN. Stimuli were patterns comprised of familiar (N) or unfamiliar (И) letters. In a passive oddball paradigm, letters (N and И) were presented as either standard or deviant in separate conditions. VMMNs emerged in both conditions; peak latency of vMMN was shorter to the И deviant compared to the vMMN elicited by the N deviant. To test the orientation-specific effect of the oblique lines on the vMMN, we introduced a control experiment. In the control experiment, the patterns were constructed solely from oblique lines, identical to the oblique lines of the N and И stimuli. Contrary to the first experiment, there was no significant difference between the vMNNs elicited by the two orientations. Therefore, the differences in vMMNs to И and N deviants are not attributable to the physical difference between the И and N stimuli. Consequently, the vMMN is sensitive to the familiarity of the stimuli.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {108-117},
  keywords = {Familiarity,Search asymmetry,Visual change-detection,Visual mismatch negativity,Visual search},
  author = {Sulykos, István and Kecskés-Kovács, Krisztina and Czigler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\2JR3V6FX\\Sulykos et al. - 2015 - Asymmetric effect of automatic deviant detection .pdf;D\:\\Sauve\\Zotero\\storage\\SN5FU7AG\\S0006899315001353.html}
}

@article{vandersteenSensorimotorSynchronizationTempochanging2015,
  title = {Sensorimotor Synchronization with Tempo-Changing Auditory Sequences: {{Modeling}} Temporal Adaptation and Anticipation},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315000876},
  doi = {10.1016/j.brainres.2015.01.053},
  shorttitle = {Sensorimotor Synchronization with Tempo-Changing Auditory Sequences},
  abstract = {The current study investigated the human ability to synchronize movements with event sequences containing continuous tempo changes. This capacity is evident, for example, in ensemble musicians who maintain precise interpersonal coordination while modulating the performance tempo for expressive purposes. Here we tested an ADaptation and Anticipation Model (ADAM) that was developed to account for such behavior by combining error correction processes (adaptation) with a predictive temporal extrapolation process (anticipation). While previous computational models of synchronization incorporate error correction, they do not account for prediction during tempo-changing behavior. The fit between behavioral data and computer simulations based on four versions of ADAM was assessed. These versions included a model with adaptation only, one in which adaptation and anticipation act in combination (error correction is applied on the basis of predicted tempo changes), and two models in which adaptation and anticipation were linked in a joint module that corrects for predicted discrepancies between the outcomes of adaptive and anticipatory processes. The behavioral experiment required participants to tap their finger in time with three auditory pacing sequences containing tempo changes that differed in the rate of change and the number of turning points. Behavioral results indicated that sensorimotor synchronization accuracy and precision, while generally high, decreased with increases in the rate of tempo change and number of turning points. Simulations and model-based parameter estimates showed that adaptation mechanisms alone could not fully explain the observed precision of sensorimotor synchronization. Including anticipation in the model increased the precision of simulated sensorimotor synchronization and improved the fit of model to behavioral data, especially when adaptation and anticipation mechanisms were linked via a joint module based on the notion of joint internal models. Overall results suggest that adaptation and anticipation mechanisms both play an important role during sensorimotor synchronization with tempo-changing sequences.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {66-87},
  keywords = {computational model,Error correction,Predictive internal models,Sensorimotor synchronization,Temporal adaptation,Temporal anticipation},
  author = {van der Steen, M. C. (Marieke) and Jacoby, Nori and Fairhurst, Merle T. and Keller, Peter E.},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\35VH2J3I\\van der Steen et al. - 2015 - Sensorimotor synchronization with tempo-changing a.pdf;D\:\\Sauve\\Zotero\\storage\\KHJDRVWZ\\S0006899315000876.html}
}

@article{kimuraAutomaticPredictionRegarding2015,
  title = {Automatic Prediction Regarding the next State of a Visual Object: {{Electrophysiological}} Indicators of Prediction Match and Mismatch},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315000244},
  doi = {10.1016/j.brainres.2015.01.013},
  shorttitle = {Automatic Prediction Regarding the next State of a Visual Object},
  abstract = {Behavioral phenomena such as representational momentum suggest that the brain can automatically predict the next state of a visual object, based on sequential rules embedded in its preceding spatiotemporal context. To identify electrophysiological indicators of automatic visual prediction in terms of prediction match and mismatch, we recorded event-related brain potentials (ERPs) while participants passively viewed three types of task-irrelevant sequences of a bar stimulus: (1) an oddball sequence, which contained a sequential rule defined by stimulus repetition, providing repetition-rule-conforming (standard) and -violating (deviant) stimuli; (2) a rotating-oddball sequence, which contained a sequential rule defined by stimulus change (i.e., rotation), providing change-rule-conforming (regular) and -violating (irregular) stimuli; and (3) a random sequence, which did not contain a sequential rule, providing a neutral (control) stimulus. This protocol allowed us to expect that (1) an ERP effect that reflects a prediction-mismatch process should be exclusively observed in both the deviant-minus-control and irregular-minus-control comparisons and (2) an ERP effect that reflects a prediction-match process should be exclusively observed in both the standard-minus-control and regular-minus-control comparisons. The results showed that the ERP effect that met the criterion for prediction mismatch was an occipito-temporal negative deflection at around 170–300 ms (visual mismatch negativity), while the ERP effect that met the criterion for prediction match was a frontal/central negative deflection at around 150–270 ms (probably, the reduction of P2). These two contrasting ERP effects support a hypothetical view that automatic visual prediction would involve both an increase in the neural response to prediction-incongruent (i.e., novel) events and a decrease in the neural response to prediction-congruent (i.e., redundant) events.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {31-44},
  keywords = {Automatic visual prediction,Event-related brain potential (ERP),Match,Mismatch,Sequential rule},
  author = {Kimura, Motohiro and Takeda, Yuji},
  file = {D\:\\Sauve\\Zotero\\storage\\325HPGU8\\Kimura and Takeda - 2015 - Automatic prediction regarding the next state of a.pdf;D\:\\Sauve\\Zotero\\storage\\NW5J9GCJ\\S0006899315000244.html}
}

@article{bendixenNoiseOcclusionDiscrete2015,
  title = {Noise Occlusion in Discrete Tone Sequences as a Tool towards Auditory Predictive Processing?},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315005272},
  doi = {10.1016/j.brainres.2015.06.045},
  abstract = {The notion of predictive coding is a common feature of many theories of auditory information processing. Experimental demonstrations of predictive auditory processing often rest on omitting predictable input in order to uncover the prediction made by the brain. Findings show that auditory cortical activity elicited by the omission of a predictable tone resembles the activity elicited by the actual tone. Here we attempted to extend this approach towards using noises instead of omissions in order to capture a more prevalent case of degraded sensory input. By applying a subtraction approach to remove ERP effects of the noise itself, auditory cortical activity elicited “behind” the noise was uncovered. We hypothesized that ERPs elicited behind noise stimuli covering predictable tones should be more similar to ERPs elicited by the actual tones than when the same comparison is made for unpredictable tones. ERP results during passive listening partly confirm this hypothesis, but also point towards some methodological caveats in this particular approach towards studying neural correlates of predictive auditory processing due to contributions from predictability-unrelated factors. A follow-up active listening condition indicated that participants were not more likely to perceive the tone sequence as continuous when a predictable tone was covered with noise than when this pertained to an unpredictable tone. Overall, the noise-based paradigm in its present form was not shown to be successful in revealing predictive processing in perceptual judgments or early neural correlates of sound processing. We discuss these findings in the contexts of predictive processing and illusory auditory continuity.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {97-107},
  keywords = {Predictability,expectation,Continuity illusion,Human event-related potential (ERP),P50},
  author = {Bendixen, Alexandra and Duwe, Susann and Reiche, Martin},
  file = {D\:\\Sauve\\Zotero\\storage\\USRTRTHF\\Bendixen et al. - 2015 - Noise occlusion in discrete tone sequences as a to.pdf;D\:\\Sauve\\Zotero\\storage\\3WD63H4B\\S0006899315005272.html}
}

@article{horvathActionrelatedAuditoryERP2015,
  title = {Action-Related Auditory {{ERP}} Attenuation: {{Paradigms}} and Hypotheses},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315002735},
  doi = {10.1016/j.brainres.2015.03.038},
  shorttitle = {Action-Related Auditory {{ERP}} Attenuation},
  abstract = {A number studies have shown that the auditory N1 event-related potential (ERP) is attenuated when elicited by self-induced or self-generated sounds. Because N1 is a correlate of auditory feature- and event-detection, it was generally assumed that N1-attenuation reflected the cancellation of auditory re-afference, enabled by the internal forward modeling of the predictable sensory consequences of the given action. Focusing on paradigms utilizing non-speech actions, the present review summarizes recent progress on action-related auditory attenuation. Following a critical analysis of the most widely used, contingent paradigm, two further hypotheses on the possible causes of action-related auditory ERP attenuation are presented. The attention hypotheses suggest that auditory ERP attenuation is brought about by a temporary division of attention between the action and the auditory stimulation. The pre-activation hypothesis suggests that the attenuation is caused by the activation of a sensory template during the initiation of the action, which interferes with the incoming stimulation. Although each hypothesis can account for a number of findings, none of them can accommodate the whole spectrum of results. It is suggested that a better understanding of auditory ERP attenuation phenomena could be achieved by systematic investigations of the types of actions, the degree of action–effect contingency, and the temporal characteristics of action–effect contingency representation-buildup and -deactivation.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {54-65},
  keywords = {Attention,Prediction,HEARING,Event-related potentials (ERP),Forward modeling},
  author = {Horváth, János},
  file = {D\:\\Sauve\\Zotero\\storage\\E3FNS5MB\\Horváth - 2015 - Action-related auditory ERP attenuation Paradigms.pdf;D\:\\Sauve\\Zotero\\storage\\6WUE6FB6\\S0006899315002735.html}
}

@article{gregoriouOscillatorySynchronyMechanism2015,
  title = {Oscillatory Synchrony as a Mechanism of Attentional Processing},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315000840},
  doi = {10.1016/j.brainres.2015.02.004},
  abstract = {The question of how the brain selects which stimuli in our visual field will be given priority to enter into perception, to guide our actions and to form our memories has been a matter of intense research in studies of visual attention. Work in humans and animal models has revealed an extended network of areas involved in the control and maintenance of attention. For many years, imaging studies in humans constituted the main source of a systems level approach, while electrophysiological recordings in non-human primates provided insight into the cellular mechanisms of visual attention. Recent technological advances and the development of sophisticated analytical tools have allowed us to bridge the gap between the two approaches and assess how neuronal ensembles across a distributed network of areas interact in visual attention tasks. A growing body of evidence suggests that oscillatory synchrony plays a crucial role in the selective communication of neuronal populations that encode the attended stimuli. Here, we discuss data from theoretical and electrophysiological studies, with more emphasis on findings from humans and non-human primates that point to the relevance of oscillatory activity and synchrony for attentional processing and behavior. These findings suggest that oscillatory synchrony in specific frequencies reflects the biophysical properties of specific cell types and local circuits and allows the brain to dynamically switch between different spatio-temporal patterns of activity to achieve flexible integration and selective routing of information along selected neuronal populations according to behavioral demands.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {165-182},
  keywords = {Attention,synchrony,Alpha,Beta,Gamma,Interareal communication,Oscillations},
  author = {Gregoriou, Georgia G and Paneri, Sofia and Sapountzis, Panagiotis},
  file = {D\:\\Sauve\\Zotero\\storage\\8T2QU5NE\\Gregoriou et al. - 2015 - Oscillatory synchrony as a mechanism of attentiona.pdf;D\:\\Sauve\\Zotero\\storage\\TWX8V48B\\S0006899315000840.html}
}

@article{bauerAuditoryDynamicAttending2015,
  title = {The Auditory Dynamic Attending Theory Revisited: {{A}} Closer Look at the Pitch Comparison Task},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315003285},
  doi = {10.1016/j.brainres.2015.04.032},
  shorttitle = {The Auditory Dynamic Attending Theory Revisited},
  abstract = {The dynamic attending theory as originally proposed by Jones, 1976. Psychol. Rev. 83(5), 323–355 posits that tone sequences presented at a regular rhythm entrain attentional oscillations and thereby facilitate the processing of sounds presented in phase with this rhythm. The increased interest in neural correlates of dynamic attending requires robust behavioral indicators of the phenomenon. Here we aimed to replicate and complement the most prominent experimental implementation of dynamic attending (Jones et al., 2002. Psychol. Sci. 13(4), 313–319). The paradigm uses a pitch comparison task in which two tones, the initial and the last of a longer series, have to be compared. In-between the two, distractor tones with variable pitch are presented, at a regular pace. A comparison tone presented in phase with the entrained rhythm is hypothesized to lead to better behavioral performance. Aiming for a conceptual replication, four different variations of the original paradigm were created which were followed by an exact replication attempt. Across all five experiments, only 40 of the 140 tested participants showed the hypothesized pattern of an inverted U-shaped profile in task accuracy, and the group average effects did not replicate the pattern reported by Jones et al., 2002. Psychol. Sci. 13(4), 313–319 in any of the five experiments. However, clear evidence for a relationship between musicality and overall behavioral performance was found. This study casts doubt on the suitability of the pitch comparison task for demonstrating auditory dynamic attending. We discuss alternative tasks that have been shown to support dynamic attending theory, thus lending themselves more readily to studying its neural correlates.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {198-210},
  keywords = {Replication,Entrainment,Musicality,Temporal expectation},
  author = {Bauer, Anna-Katharina R. and Jaeger, Manuela and Thorne, Jeremy D. and Bendixen, Alexandra and Debener, Stefan},
  file = {D\:\\Sauve\\Zotero\\storage\\3ZNUZ42G\\Bauer et al. - 2015 - The auditory dynamic attending theory revisited A.pdf;D\:\\Sauve\\Zotero\\storage\\ABWWEZKJ\\S0006899315003285.html}
}

@article{parmentierSemanticAftermathDistraction2015,
  title = {The Semantic Aftermath of Distraction by Deviant Sounds: {{Crosstalk}} Interference Is Mediated by the Predictability of Semantic Congruency},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315000542},
  doi = {10.1016/j.brainres.2015.01.034},
  shorttitle = {The Semantic Aftermath of Distraction by Deviant Sounds},
  abstract = {Rare changes in a stream of otherwise repeated task-irrelevant sounds break through selective attention and disrupt performance in an unrelated visual task. This deviance distraction effect emerges because deviant sounds violate the cognitive system’s predictions. In this study we sought to examine whether predictability also mediate the so-called semantic effect whereby behavioral performance suffers from the clash between the involuntary semantic evaluation of irrelevant sounds and the voluntary processing of visual targets (e.g., when participants must categorize a right visual arrow following the presentation of the deviant sound “left”). By manipulating the conditional probabilities of the congruent and incongruent deviant sounds in a left/right arrow categorization task, we elicited implicit predictions about the upcoming target and related response. We observed a linear increase of the semantic effect with the proportion of congruent deviant trials (i.e., as deviant sounds increasingly predicted congruent targets). We conclude that deviant sounds affect response times based on a combination of crosstalk interference and two types of prediction violations: stimulus violations (violations of predictions regarding the identity of upcoming irrelevant sounds) and semantic violations (violations of predictions regarding the target afforded by deviant sounds). We report a three-parameter model that captures all key features of the observed RTs. Overall, our results fit with the view that the brain builds forward models of the environment in order to optimize cognitive processing and that control of one’s attention and actions is called upon when predictions are violated.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {247-257},
  keywords = {Prediction,Distraction,AUDITORY attention,Deviant sounds,Oddball task},
  author = {Parmentier, Fabrice B. R. and Kefauver, Miriam},
  file = {D\:\\Sauve\\Zotero\\storage\\QGBM5S69\\Parmentier and Kefauver - 2015 - The semantic aftermath of distraction by deviant s.pdf;D\:\\Sauve\\Zotero\\storage\\27FMX7DR\\S0006899315000542.html}
}

@article{huettigFourCentralQuestions2015,
  title = {Four Central Questions about Prediction in Language Processing},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315001146},
  doi = {10.1016/j.brainres.2015.02.014},
  abstract = {The notion that prediction is a fundamental principle of human information processing has been en vogue over recent years. The investigation of language processing may be particularly illuminating for testing this claim. Linguists traditionally have argued prediction plays only a minor role during language understanding because of the vast possibilities available to the language user as each word is encountered. In the present review I consider four central questions of anticipatory language processing: Why (i.e. what is the function of prediction in language processing)? What (i.e. what are the cues used to predict up-coming linguistic information and what type of representations are predicted)? How (what mechanisms are involved in predictive language processing and what is the role of possible mediating factors such as working memory)? When (i.e. do individuals always predict up-coming input during language processing)? I propose that prediction occurs via a set of diverse PACS (production-, association-, combinatorial-, and simulation-based prediction) mechanisms which are minimally required for a comprehensive account of predictive language processing. Models of anticipatory language processing must be revised to take multiple mechanisms, mediating factors, and situational context into account. Finally, I conjecture that the evidence considered here is consistent with the notion that prediction is an important aspect but not a fundamental principle of language processing.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {118-135},
  keywords = {Prediction,Language processing},
  author = {Huettig, Falk},
  file = {D\:\\Sauve\\Zotero\\storage\\R8HH47FG\\Huettig - 2015 - Four central questions about prediction in languag.pdf;D\:\\Sauve\\Zotero\\storage\\G6RQHBHH\\S0006899315001146.html}
}

@article{stekelenburgPredictiveCodingVisual2015,
  title = {Predictive Coding of Visual–Auditory and Motor-Auditory Events: {{An}} Electrophysiological Study},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315000566},
  doi = {10.1016/j.brainres.2015.01.036},
  shorttitle = {Predictive Coding of Visual–Auditory and Motor-Auditory Events},
  abstract = {The amplitude of auditory components of the event-related potential (ERP) is attenuated when sounds are self-generated compared to externally generated sounds. This effect has been ascribed to internal forward modals predicting the sensory consequences of one’s own motor actions. Auditory potentials are also attenuated when a sound is accompanied by a video of anticipatory visual motion that reliably predicts the sound. Here, we investigated whether the neural underpinnings of prediction of upcoming auditory stimuli are similar for motor-auditory (MA) and visual–auditory (VA) events using a stimulus omission paradigm. In the MA condition, a finger tap triggered the sound of a handclap whereas in the VA condition the same sound was accompanied by a video showing the handclap. In both conditions, the auditory stimulus was omitted in either 50\% or 12\% of the trials. These auditory omissions induced early and mid-latency ERP components (oN1 and oN2, presumably reflecting prediction and prediction error), and subsequent higher-order error evaluation processes. The oN1 and oN2 of MA and VA were alike in amplitude, topography, and neural sources despite that the origin of the prediction stems from different brain areas (motor versus visual cortex). This suggests that MA and VA predictions activate a sensory template of the sound in auditory cortex.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {88-96},
  keywords = {predictive coding,event-related potentials,Motor–auditory,Stimulus omission,Visual–auditory},
  author = {Stekelenburg, Jeroen J. and Vroomen, Jean},
  file = {D\:\\Sauve\\Zotero\\storage\\X7WDS89T\\Stekelenburg and Vroomen - 2015 - Predictive coding of visual–auditory and motor-aud.pdf;D\:\\Sauve\\Zotero\\storage\\X8WWRKXE\\S0006899315000566.html}
}

@article{vargheseEvidenceAttentionalState2015,
  title = {Evidence against Attentional State Modulating Scalp-Recorded Auditory Brainstem Steady-State Responses},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S000689931500520X},
  doi = {10.1016/j.brainres.2015.06.038},
  abstract = {Auditory brainstem responses (ABRs) and their steady-state counterpart (subcortical steady-state responses, SSSRs) are generally thought to be insensitive to cognitive demands. However, a handful of studies report that SSSRs are modulated depending on the subject׳s focus of attention, either towards or away from an auditory stimulus. Here, we explored whether attentional focus affects the envelope-following response (EFR), which is a particular kind of SSSR, and if so, whether the effects are specific to which sound elements in a sound mixture a subject is attending (selective auditory attentional modulation), specific to attended sensory input (inter-modal attentional modulation), or insensitive to attentional focus. We compared the strength of EFR-stimulus phase locking in human listeners under various tasks: listening to a monaural stimulus, selectively attending to a particular ear during dichotic stimulus presentation, and attending to visual stimuli while ignoring dichotic auditory inputs. We observed no systematic changes in the EFR across experimental manipulations, even though cortical EEG revealed attention-related modulations of alpha activity during the task. We conclude that attentional effects, if any, on human subcortical representation of sounds cannot be observed robustly using EFRs.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {146-164},
  keywords = {Attention,auditory processing,Envelope-following response,FFR,Frequency-following response},
  author = {Varghese, Leonard and Bharadwaj, Hari M. and Shinn-Cunningham, Barbara G.},
  file = {D\:\\Sauve\\Zotero\\storage\\B2WNBJ8E\\Varghese et al. - 2015 - Evidence against attentional state modulating scal.pdf;D\:\\Sauve\\Zotero\\storage\\DX3CP6R5\\S000689931500520X.html}
}

@article{freyNotDifferentAll2015,
  title = {Not so Different after All: {{The}} Same Oscillatory Processes Support Different Types of Attention},
  volume = {1626},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899315001171},
  doi = {10.1016/j.brainres.2015.02.017},
  shorttitle = {Not so Different after All},
  abstract = {Scientific research from the last two decades has provided a vast amount of evidence that brain oscillations reflect physiological activity enabling diverse cognitive processes. The goal of this review is to give a broad empirical and conceptual overview of how ongoing oscillatory activity may support attention processes. Keeping in mind that definitions of cognitive constructs like attention are prone to being blurry and ambiguous, the present review focuses mainly on the neural correlates of ‘top-down’ attention deployment. In particular, we will discuss modulations of (ongoing) oscillatory activity during spatial, temporal, selective, and internal attention. Across these seemingly distinct attentional domains, we will summarize studies showing the involvement of two oscillatory processes observed during attention deployment: power modulations mainly in the alpha band, and phase modulations in lower frequency bands.

This article is part of a Special Issue entitled SI: Prediction and Attention.},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  series = {Predictive and {{Attentive Processing}} in {{Perception}} and {{Action}}},
  urldate = {2015-12-01},
  date = {2015-11-11},
  pages = {183-197},
  keywords = {Attention,Alpha,Entrainment,Phase reset,Power},
  author = {Frey, Julia Natascha and Ruhnau, Philipp and Weisz, Nathan},
  file = {D\:\\Sauve\\Zotero\\storage\\9AJGQKF2\\Frey et al. - 2015 - Not so different after all The same oscillatory p.pdf;D\:\\Sauve\\Zotero\\storage\\6TTJH7CM\\S0006899315001171.html}
}

@article{horvathSimultaneouslyActivePreattentive2001,
  langid = {english},
  title = {Simultaneously Active Pre-Attentive Representations of Local and Global Rules for Sound Sequences in the Human Brain},
  volume = {12},
  issn = {0926-6410},
  abstract = {Regular sequences of sounds (i.e., non-random) can usually be described by several, equally valid rules. Rules allowing extrapolation from one sound to the next are termed local rules, those that define relations between temporally non-adjacent sounds are termed global rules. The aim of the present study was to determine whether both local and global rules can be simultaneously extracted from a sound sequence even when attention is directed away from the auditory stimuli. The pre-attentive representation of a sequence of two alternating tones (differing only in frequency) was investigated using the mismatch negativity (MMN) auditory event-related potential. Both local- and global-rule violations of tone alternation elicited the MMN component while subjects ignored the auditory stimuli. This finding suggests that (a) pre-attentive auditory processes can extract both local and global rules from sound sequences, and (b) that several regularity representations of a sound sequence are simultaneously maintained during the pre-attentive phase of auditory stimulus processing.},
  number = {1},
  journaltitle = {Brain Research. Cognitive Brain Research},
  shortjournal = {Brain Res Cogn Brain Res},
  date = {2001-08},
  pages = {131-144},
  keywords = {Attention,brain,electroencephalography,Memory,Acoustic Stimulation,humans,Adult,Female,Male,Evoked Potentials; Auditory,Adolescent},
  author = {Horváth, J. and Czigler, I. and Sussman, E. and Winkler, I.},
  eprinttype = {pmid},
  eprint = {11489616}
}

@article{bregmanAuditoryStreamingCumulative1978,
  title = {Auditory {{Streaming}} Is {{Cumulative}}},
  volume = {4},
  number = {3},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {1978},
  pages = {380-387},
  author = {{Bregman}}
}

@inproceedings{sauves.EffectMusicalTraining2014,
  location = {{Seoul, Korea}},
  title = {The {{Effect}} of {{Musical Training}} on {{Auditory Grouping}}},
  eventtitle = {{{ICMPC}} 13},
  booktitle = {Proceedings of the {{ICMPC}}-{{APSCOM}} 2014 {{Joint Conference}}},
  date = {2014},
  pages = {293-296},
  author = {{Sauvé, S.} and {Stewart, L.} and {Pearce, M.T.}},
  editor = {Kyoung Song, M.}
}

@inproceedings{gillardj.ImprovingEfficacyAuditory2012,
  location = {{Atlanta, GA.}},
  title = {Improving the Efficacy of Auditory Alarms in Medical Devices by Exploring the Effect of Amplitude Envelope on Learning and Retention.},
  eventtitle = {International {{Conference}} on {{Auditory Display}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Auditory Display}}},
  date = {2012},
  pages = {240-241},
  author = {{Gillard, J.} and {Schutz, M.}}
}

@article{esberReconcilingInfluencePredictiveness2011,
  langid = {english},
  title = {Reconciling the Influence of Predictiveness and Uncertainty on Stimulus Salience: A Model of Attention in Associative Learning},
  volume = {278},
  issn = {0962-8452, 1471-2954},
  url = {http://rspb.royalsocietypublishing.org/content/278/1718/2553},
  doi = {10.1098/rspb.2011.0836},
  shorttitle = {Reconciling the Influence of Predictiveness and Uncertainty on Stimulus Salience},
  abstract = {Theories of selective attention in associative learning posit that the salience of a cue will be high if the cue is the best available predictor of reinforcement (high predictiveness). In contrast, a different class of attentional theory stipulates that the salience of a cue will be high if the cue is an inaccurate predictor of reinforcement (high uncertainty). Evidence in support of these seemingly contradictory propositions has led to: (i) the development of hybrid attentional models that assume the coexistence of separate, predictiveness-driven and uncertainty-driven mechanisms of changes in cue salience; and (ii) a surge of interest in identifying the neural circuits underpinning these mechanisms. Here, we put forward a formal attentional model of learning that reconciles the roles of predictiveness and uncertainty in salience modification. The issues discussed are relevant to psychologists, behavioural neuroscientists and neuroeconomists investigating the roles of predictiveness and uncertainty in behaviour.},
  number = {1718},
  journaltitle = {Proceedings of the Royal Society of London B: Biological Sciences},
  urldate = {2015-09-30},
  date = {2011-09-07},
  pages = {2553-2561},
  author = {Esber, Guillem R. and Haselgrove, Mark},
  file = {D\:\\Sauve\\Zotero\\storage\\N42A6EDP\\Esber and Haselgrove - 2011 - Reconciling the influence of predictiveness and un.pdf;D\:\\Sauve\\Zotero\\storage\\452NHRRZ\\2553.html},
  eprinttype = {pmid},
  eprint = {21653585}
}

@article{ModelingAttentionAssociative,
  title = {Modeling Attention in Associative Learning: {{Two}} Processes or One? - {{Springer}}},
  url = {http://link.springer.com/article/10.3758/s13420-012-0084-4/fulltext.html},
  doi = {10.3758/s13420-012-0084-4},
  shorttitle = {Modeling Attention in Associative Learning},
  urldate = {2015-09-30},
  file = {D\:\\Sauve\\Zotero\\storage\\53AQBE8U\\fulltext.html}
}

@article{larsonMusicalForcesMelodic2004,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2004.21.4.457},
  title = {Musical {{Forces}} and {{Melodic Expectations}}: {{Comparing Computer Models}} and {{Experimental Results}}},
  volume = {21},
  issn = {0730-7829},
  doi = {10.1525/mp.2004.21.4.457},
  shorttitle = {Musical {{Forces}} and {{Melodic Expectations}}},
  abstract = {Recent work on "musical forces" asserts that experienced listeners of tonal music not only talk about music in terms used to describe physical motion, but actually experience musical motion as if it were shaped by quantifiable analogues of physical gravity, magnetism, and inertia. This article presents a theory of melodic expectation based on that assertion, describes two computer models of aspects of that theory, and finds strong support for that theory in comparisons of the behavior of those models with the behavior of participants in several experiments. The following summary statement of the theory is explained and illustrated in the article: Experienced listeners of tonal music expect completions in which the musical forces of gravity, magnetism, and inertia control operations on alphabets in hierarchies of embellishment whose stepwise displacements of auralized traces create simple closed shapes. A "single-level" computer program models the operation of these musical forces on a single level of musical structure. Given a melodic beginning in a certain key, the model not only produces almost the same responses as experimental participants, but it also rates them in a similar way; the computer model gives higher ratings to responses that participants sing more often. In fact, the completions generated by this model match note-for-note the entire completions sung by participants in several psychological studies as often as the completions of any one of those participants matches those of the other participants. A "multilevel" computer program models the operation of these musical forces on multiple hierarchical levels. When the multilevel model is given a melodic beginning and a hierarchical description of its embellishment structure (i.e., a Schenkerian analysis of it), the model produces responses that reflect the operation of musical forces on all the levels of that hierarchical structure. Statistical analyses of the results of a number of experiments test hypotheses arising from the computer models' algorithm (S. Larson, 1993a) for the interaction of musical forces as well as from F. Lerdahl's similar (1996) algorithm. Further statistical analysis contrasts the explanatory power of the theory of musical forces with that of E. Narmour's (1990, 1992) implication-realization model. The striking agreement between computer-generated responses and experimental results suggests that the theory captures some important aspects of melodic expectation. Furthermore, the fact that these data can be modeled well by the interaction of constantly acting but contextually determined musical forces gives support to the idea that we experience musical motions metaphorically in terms of our experience of physical motions.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2004-06-01},
  pages = {457-498},
  author = {Larson, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\XCN898KE\\McAdams - 2004 - Musical Forces and Melodic Expectations Comparing.pdf}
}

@article{alluriExploringPerceptualAcoustical2010,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2010.27.3.223},
  title = {Exploring {{Perceptual}} and {{Acoustical Correlates}} of {{Polyphonic Timbre}}},
  volume = {27},
  issn = {0730-7829},
  doi = {10.1525/mp.2010.27.3.223},
  abstract = {POLYPHONIC TIMBRE HAS BEEN DEMONSTRATED TO BE an important element for computational categorization according to genre, style, mood, and emotions, but its perceptual constituents have received less attention. The work presented here comprises two experiments, Experiment 1, to devise a framework of subjective rating scales for quantifying the perceptual qualities of polyphonic timbre and Experiment 2, to rate short excerpts of Indian popular music and correlate them with computationally extracted acoustic features. A factor analysis of the ratings suggested three perceptual dimensions: Activity, Brightness, and Fullness. The present findings imply that there may be regularities and patterns in the way people perceive polyphonic timbre. Furthermore, the perceptual dimensions can be predicted relatively well by the regression models. Spectrotemporal modulations were found to be most relevant, while the well known polyphonic timbre descriptors, the Mel-Frequency Cepstral Coefficients, did not contribute significantly to any of the perceptual dimensions.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2010-02-01},
  pages = {223-242},
  keywords = {Auditory Perception,Music,timbre,acoustics,music perception,modeling,Perception,emotions,style,acoustic features,activity,bipolar scales,brightness,computational analysis,Emotional States,fullness,genre,mood,polyphonic timbre},
  author = {Alluri, Vinoo and Toiviainen, Petri},
  file = {D\:\\Sauve\\Zotero\\storage\\QNAHSCEV\\Alluri and Toiviainen - 2010 - Exploring Perceptual and Acoustical Correlates of .pdf}
}

@article{alluriEffectEnculturationSemantic2012,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2012.29.3.297},
  title = {Effect of {{Enculturation}} on the {{Semantic}} and {{Acoustic Correlates}} of {{Polyphonic Timbre}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2012.29.3.297},
  abstract = {polyphonic timbre perception was investigated in a cross-cultural context wherein Indian and Western nonmusicians rated short Indian and Western popular music excerpts (1.5 s, n = 200) on eight bipolar scales. Intrinsic dimensionality estimation revealed a higher number of perceptual dimensions in the timbre space for music from one's own culture. Factor analyses of Indian and Western participants' ratings resulted in highly similar factor solutions. The acoustic features that predicted the perceptual dimensions were similar across the two participant groups. Furthermore, both the perceptual dimensions and their acoustic correlates matched closely with the results of a previous study performed using Western musicians as participants. Regression analyses revealed relatively well performing models for the perceptual dimensions. The models displayed relatively high cross-validation performance. The findings suggest the presence of universal patterns in polyphonic timbre perception while demonstrating the increase of dimensionality of timbre space as a result of enculturation.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2012-02-01},
  pages = {297-310},
  keywords = {Music,timbre,acoustics,music perception,pitch perception,SEMANTICS,Cross Cultural Differences,enculturation,Indian & Western nonmusicians,Indian & Western popular music excerpts,polyphonic timbre perception,semantic & acoustic correlates,South Asian Cultural Groups},
  author = {Alluri, Vinoo and Toiviainen, Petri},
  file = {D\:\\Sauve\\Zotero\\storage\\R2XK7BQI\\Alluri and Toiviainen - 2012 - Effect of Enculturation on the Semantic and Acoust.pdf}
}

@article{eerolaTimbreAffectDimensions2012,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2012.30.1.49},
  title = {Timbre and {{Affect Dimensions}}: {{Evidence}} from {{Affect}} and {{Similarity Ratings}} and {{Acoustic Correlates}} of {{Isolated Instrument Sounds}}},
  volume = {30},
  issn = {0730-7829},
  doi = {10.1525/mp.2012.30.1.49},
  shorttitle = {Timbre and {{Affect Dimensions}}},
  abstract = {considerable effort has been made towards understanding how acoustic and structural features contribute to emotional expression in music, but relatively little attention has been paid to the role of timbre in this process. Our aim was to investigate the role of timbre in the perception of affect dimensions in isolated musical sounds, by way of three behavioral experiments. In Experiment 1, participants evaluated perceived affects of 110 instrument sounds that were equal in duration, pitch, and dynamics using a three-dimensional affect model (valence, energy arousal, and tension arousal) and preference and emotional intensity. In Experiment 2, an emotional dissimilarity task was applied to a subset of the instrument sounds used in Experiment 1 to better reveal the underlying affect structure. In Experiment 3, the perceived affect dimensions as well as preference and intensity of a new set of 105 instrument sounds were rated by participants. These sounds were also uniform in pitch, duration, and playback dynamics but contained systematic manipulations in the dynamics of sound production, articulation, and ratio of high-frequency to low-frequency energy. The affect dimensions for all the experiments were then explained in terms of the three kinds of acoustic features extracted: spectral (e.g., ratio of high-frequency to low-frequency energy), temporal (e.g., attack slope), and spectro-temporal (e.g., spectral flux). High agreement among the participants' ratings across the experiments suggested that even isolated instrument sounds contain cues that indicate affective expression, and these are recognized as such by the listeners. A dominant portion (50-57\%) of the two dimensions of affect (valence and energy arousal) could be predicted by linear combinations of few acoustic features such as ratio of high-frequency to low-frequency energy, attack slope, and spectral regularity. Links between these features and those observed in the vocal expression of affects and other sound phenomena are discussed.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2012-09-01},
  pages = {49-70},
  keywords = {emotion,Pitch (Frequency),Auditory Stimulation,timbre,acoustics,music perception,Musical Instruments,pitch,acoustic correlates,instrument sound},
  author = {Eerola, Tuomas and Ferrer, Rafael and Alluri, Vinoo},
  file = {D\:\\Sauve\\Zotero\\storage\\I884EH93\\Eerola et al. - 2012 - Timbre and Affect Dimensions Evidence from Affect.pdf}
}

@article{temperleyProbabilisticModelsMelodic2014,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2014.32.1.85},
  title = {Probabilistic {{Models}} of {{Melodic Interval}}},
  volume = {32},
  issn = {0730-7829},
  doi = {10.1525/mp.2014.32.1.85},
  abstract = {Two probabilistic models of melodic interval are compared. In the Markov model, the “interval probability” of a note is defined by the corpus frequency of its melodic interval (the interval to the previous note), conditioned on the previous one or two intervals; in the Gaussian model, the interval probability is a simple mathematical function of the size of the note’s melodic interval and its position in relation to the range of the melody. In both models, this interval probability is then multiplied by the probability of the note’s scale degree to yield its actual probability. The two models were tested on four corpora of tonal melodies using cross-entropy. The Markov model yielded a somewhat lower (better) cross-entropy than the Gaussian model, but is also much more complex, requiring far more parameters. The models were also tested on melodic expectation data, and on their ability to predict the distribution of intervals in a corpus. Possible ways of improving the models are discussed, as well as their broader implications for music cognition.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2014-09-01},
  pages = {85-99},
  keywords = {cognition,melody,model,expectation,music perception,Expectations,Models,corpus research,cross-entropy,Markov models,Probability},
  author = {Temperley, David},
  file = {D\:\\Sauve\\Zotero\\storage\\Q56HVU5H\\Temperley - 2014 - Probabilistic Models of Melodic Interval.pdf}
}

@article{witekEffectsPolyphonicContext2014,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2014.32.2.201},
  title = {Effects of {{Polyphonic Context}}, {{Instrumentation}}, and {{Metrical Location}} on {{Syncopation}} in {{Music}}},
  volume = {32},
  issn = {0730-7829},
  abstract = {In music, the rhythms of different instruments are often syncopated against each other to create tension. Existing perceptual theories of syncopation cannot adequately model such kinds of syncopation since they assume monophony. This study investigates the effects of polyphonic context, instrumentation and metrical location on the salience of syncopations. Musicians and nonmusicians were asked to tap along to rhythmic patterns of a drum kit and rate their stability; in these patterns, syncopations occurred among different numbers of streams, with different instrumentation and at different metrical locations. The results revealed that the stability of syncopations depends on all these factors and music training, in variously interacting ways. It is proposed that listeners’ experiences of syncopations are shaped by polyphonic and instrumental configuration, metrical structure, and individual music training, and a number of possible mechanisms are considered, including the rhythms’ acoustic properties, ecological associations, statistical learning, and timbral differentiation.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2014-12-01},
  pages = {201-217},
  keywords = {Auditory Perception,Music,rhythm,timbre,polyphony,Rhythm,meter,Instrumentality,instrumentation,syncopation},
  author = {Witek, Maria A. G. and Clarke, Eric F. and Kringelbach, Morten L. and Vuust, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\NKQCPWTW\\Witek et al. - 2014 - Effects of Polyphonic Context, Instrumentation, an.pdf}
}

@article{huronVirtuousVexatiousAge2013,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2013.31.1.4},
  title = {On the {{Virtuous}} and the {{Vexatious}} in an {{Age}} of {{Big Data}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2013.31.1.4},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2013-09-01},
  pages = {4-9},
  keywords = {Music,big data,computer use,Computers,Data Collection,Data Processing,Databases,Internet,large data sets,methodology,music research,Sample Size},
  author = {Huron, David},
  file = {D\:\\Sauve\\Zotero\\storage\\NUKSPIDC\\Huron - 2013 - On the Virtuous and the Vexatious in an Age of Big.pdf}
}

@article{princeTonalMetricHierarchyCorpus2014,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2014.31.3.254},
  title = {The {{Tonal}}-{{Metric Hierarchy}}: {{A Corpus Analysis}}},
  volume = {31},
  issn = {0730-7829},
  doi = {10.1525/mp.2014.31.3.254},
  shorttitle = {The {{Tonal}}-{{Metric Hierarchy}}},
  abstract = {Despite the plethora of research on the role of tonality and meter in music perception, there is little work on how these fundamental properties function together. The most basic question is whether the two hierarchical structures are correlated – that is, do metrically stable positions in the measure preferentially feature tonally stable pitches, and do tonally stable pitches occur more often than not at metrically stable locations? To answer this question, we analyzed a corpus of compositions by Bach, Mozart, Beethoven, and Chopin, tabulating the frequency of occurrence of each of the 12 pitch classes at all possible temporal positions in the bar. There was a reliable relation between the tonal and metric hierarchies, such that tonally stable pitch classes and metrically stable temporal positions co-occurred beyond their simple joint probability. Further, the pitch class distribution at stable metric temporal positions agreed more with the tonal hierarchy than at less metrically stable locations. This tonal-metric hierarchy was largely consistent across composers, time signatures, and modes. The existence, profile, and constancy of the tonal-metric hierarchy is relevant to several areas of music cognition research, including pitch-time integration, statistical learning, and global effects of tonality.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2014-02-01},
  pages = {254-270},
  keywords = {cognition,Music,Theories,Pitch (Frequency),Pitch Discrimination,melody,rhythm,corpus,music perception,Rhythm,tonality,music cognition,History,music meter,pitch time integration theory},
  author = {Prince, Jon B. and Schmuckler, Mark A.},
  file = {D\:\\Sauve\\Zotero\\storage\\NKB9MQSQ\\Prince and Schmuckler - 2014 - The Tonal-Metric Hierarchy A Corpus Analysis.pdf}
}

@article{cambouropoulosMusicalParallelismMelodic2006,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2006.23.3.249},
  title = {Musical {{Parallelism}} and {{Melodic Segmentation}}},
  volume = {23},
  issn = {0730-7829},
  doi = {10.1525/mp.2006.23.3.249},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2006-02-01},
  pages = {249-268},
  keywords = {Music,music perception,computational model,Algorithms,grouping processes,melodic segmentation,musical parallelism},
  author = {Cambouropoulos, Emilios},
  file = {D\:\\Sauve\\Zotero\\storage\\GAVVCNC3\\Cambouropoulos - 2006 - Musical Parallelism and Melodic Segmentation.pdf}
}

@article{londonExamplesComplexMeters1995,
  eprinttype = {jstor},
  eprint = {40285685},
  title = {Some {{Examples}} of {{Complex Meters}} and {{Their Implications}} for {{Models}} of {{Metric Perception}}},
  volume = {13},
  issn = {0730-7829},
  doi = {10.2307/40285685},
  abstract = {A music-theoretic discussion of metric structure. Describing a musical passage as "metric" usually implies that one can hear in it an isochronous series of beats and that these beats are hierarchically structured. In some cases, however, one cannot infer a wholly isochronous metric structure from the durations present on the musical surface. In particular, there may be some meters where the beat level of the metric hierarchy consists of a nonisochronous series of durations; these cases are referred to as complex meters, A number of these complex metric structures are presented and discussed. The implications of these structures for various models of metric perception are then considered, with particular reference to their implications for the entrainment model proposed by Jones and Boltz (1989). It is proposed that such meters must be accounted for under an additive rather than multiplicative formalism. The paper concludes with some considerations of how entrainment to complex meters might be tested, as well as the ways in which experiments that focus on complex meters might provide insights into other aspects of temporal perception.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1995-10-01},
  pages = {59-77},
  author = {London, Justin},
  file = {D\:\\Sauve\\Zotero\\storage\\STUG9EPX\\London - 1995 - Some Examples of Complex Meters and Their Implicat.pdf}
}

@article{radvanskyTimbreRelianceNonmusicians1995,
  eprinttype = {jstor},
  eprint = {40285691},
  title = {Timbre {{Reliance}} in {{Nonmusicians}}' and {{Musicians}}' {{Memory}} for {{Melodies}}},
  volume = {13},
  issn = {0730-7829},
  doi = {10.2307/40285691},
  abstract = {This paper reports two experiments that test WolpertV (1990) claim that musicians and nonmusicians differ in their memory for melodies because nonmusicians' memory performance reflects a greater use of the timbre dimension to make recognition decisions. In both experiments, listeners were asked to identify which of two test melodies, a target and a distractor, was heard previously. On one half of the trials, the target melody was in the same timbre as the original, and the distractor was in a different timbre. For the other half of the trials, the distractor melody was in the same timbre as the original, and the target melody was in a different timbre. In her earlier study, Wolpert found that nonmusicians' memory for melodies was affected by timbre changes, whereas musicians' memory was not. In the present experiments, we controlled for instruction clarity and brought listener performance down from near perfect. As a result, it was found that timbre changes differentially affected neither musicians' nor nonmusicians' memory for melodies.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1995-12-01},
  pages = {127-140},
  author = {Radvansky, Gabriel A. and Fleming, Kevin J. and Simmons, Julie A.},
  file = {D\:\\Sauve\\Zotero\\storage\\CKH9FRG7\\Radvansky et al. - 1995 - Timbre Reliance in Nonmusicians' and Musicians' Me.pdf}
}

@article{vosParallelProcessingKeyFindingModel1996,
  eprinttype = {jstor},
  eprint = {40285717},
  title = {A {{Parallel}}-{{Processing Key}}-{{Finding Model}}},
  volume = {14},
  issn = {0730-7829},
  doi = {10.2307/40285717},
  abstract = {A model of key finding is presented for single-voiced pieces of tonal music. Each tone is input as a pitch class and a duration. The model makes a parallel search for the key in the scalar and chordal domains, taking into account primacy and memory constraints. The model has been tested for a range of tonal music including the fugue subjects of J. S. Bach's Wohltemperierte Klavier (WTK). The notated key was usually found after a few processing steps and from then on remained stable— but was still sensitive to modulation. The performance of the parallel-processing model was compared with the performance of key-finding models previously proposed by Krumhansl and Schmuckler and by Longuet-Higgins and Steedman. The comparison showed that the new model's most distinctive features, implementation of parallel key search in the scalar and chordal domains, as well as the implementation of search-restricting factors, primacy and memory, make the new model a powerful and plausible alternative to the other models. Subsequently, the parallel-processing model's perceptual plausibility has been tested in two experiments, in which 20 musically well-trained subjects had to produce the key(s) of eight WTK fugue themes (Experiment 1) and to rate the key transparency for seven contrapuntal variations of the A minor subject of J. S. Bach's Kunst der Fuge (Experiment 2). A substantial concordance between listeners' judgments and the key inferences produced by the model was found in both experiments. Conceptual limitations, such as the model's disregard for the potential impact of recency on key finding and for expectations from functional implications of tone order, are discussed. Potential extensions of the model are suggested, as well as ideas for further perceptual studies in which the model might be tested in a more advanced manner than in the present study.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1996-12-01},
  pages = {185-223},
  author = {Vos, Piet G. and Geenen, Erwin W. Van},
  file = {D\:\\Sauve\\Zotero\\storage\\AGRDKFZZ\\Vos and Geenen - 1996 - A Parallel-Processing Key-Finding Model.pdf}
}

@article{deliegeMusicalSchemataRealTime1996,
  eprinttype = {jstor},
  eprint = {40285715},
  title = {Musical {{Schemata}} in {{Real}}-{{Time Listening}} to a {{Piece}} of {{Music}}},
  volume = {14},
  issn = {0730-7829},
  doi = {10.2307/40285715},
  abstract = {A series of experiments investigated cognitive processes involved in listening to a piece of music, focusing in particular on the abstraction of surface features (here referred to as cues). Subjects listened to an unfamiliar piece in a familiar musical idiom, and their sensitivities to aspects of the just-heard piece were used to elucidate the nature of their representations of the piece in recent memory. The study also sought to assess the capacities of subjects to use any declarative knowledge of aspects of tonal structure that they possessed in organizing musical material. Three experiments made use of different procedures to address these issues, using either a single short tonal piece—Schubert's Valse sentimentale, D. 779, op. 50, no. 6—or a variant of this. The first two experiments used nonmusician subjects and examined (1) the cues abstracted in listening to the piece and (2) subjects' ability to identify the temporal location of segments of the piece after listening. The third experiment explored the constructional abilities of musician and nonmusician subjects, requiring them to create a coherent piece by ordering the segments that made up the original piece. The results of these experiments indicated that although the abilities of musicians differed from those of nonmusicians, both groups of subjects exhibited a weaker sensitivity to features of musical structure than to cues abstracted from the musical surface.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1996-12-01},
  pages = {117-159},
  author = {Deliège, Iréne and Mélen, Marc and Stammers, Diana and Cross, Ian},
  file = {D\:\\Sauve\\Zotero\\storage\\TKUUAN5X\\Deliège et al. - 1996 - Musical Schemata in Real-Time Listening to a Piece.pdf}
}

@article{tekmanInteractionsPerceivedIntensity1997,
  eprinttype = {jstor},
  eprint = {40285722},
  title = {Interactions of {{Perceived Intensity}}, {{Duration}}, and {{Pitch}} in {{Pure Tone Sequences}}},
  volume = {14},
  issn = {0730-7829},
  doi = {10.2307/40285722},
  abstract = {If one dimension of sound is manipulated in a way that suggests a particular rhythmic organization, does perception of other dimensions change in ways that are consistent with the same rhythmic organization? When subjects were asked to judge or adjust intensities of tones, rhythmic manipulations of pitch structure changed the perception of intensity. When subjects were asked to judge timing, rhythmic manipulations of intensity had a similar effect. Timing manipulations did not have an effect on judgments of pitch. The results indicate that temporal structure as a whole is more accessible than the individual physical manipulations that give rise to that structure. It may be concluded that the temporal structure itself, rather than pitches, intensities, and durations in isolation, is a perceptual object.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1997-04-01},
  pages = {281-294},
  author = {Tekman, Hasan Gürkan},
  file = {D\:\\Sauve\\Zotero\\storage\\NGA3VFMC\\Tekman - 1997 - Interactions of Perceived Intensity, Duration, and.pdf}
}

@article{schellenbergSimplifyingImplicationRealizationModel1997,
  eprinttype = {jstor},
  eprint = {40285723},
  title = {Simplifying the {{Implication}}-{{Realization Model}} of {{Melodic Expectancy}}},
  volume = {14},
  issn = {0730-7829},
  doi = {10.2307/40285723},
  abstract = {Results from previous investigations indicate that the implication-realization (I-R) model (Narmour, 1990) of expectancy in melody may be overspecified and more complex than necessary. Indeed, Schellenberg's (1996) revised model, with two fewer predictor variables, improved predictive accuracy compared with the original model. A reanalysis of data reported by Cuddy and Lunney (1995) provided similar results. When the principles of the I-R model were submitted to a principal- components analysis, a solution containing three orthogonal (uncorrelated) factors retained the accuracy of the model but was inferior to the revised model. A separate principal-components analysis of the predictors of the revised model yielded a two-factor solution that did not compromise the revised model's predictive power. Consequently, an even simpler model of melodic expectancy was derived. These results provide further evidence that redundancy in the I-R model can be eliminated without loss of predictive accuracy.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1997-04-01},
  pages = {295-318},
  author = {Schellenberg, E. Glenn},
  file = {D\:\\Sauve\\Zotero\\storage\\3KDUQNAG\\Schellenberg - 1997 - Simplifying the Implication-Realization Model of M.pdf}
}

@article{hantzNeuralResponsesMelodic1997,
  eprinttype = {jstor},
  eprint = {40285739},
  title = {Neural {{Responses}} to {{Melodic}} and {{Harmonic Closure}}: {{An Event}}-{{Related}}-{{Potential Study}}},
  volume = {15},
  issn = {0730-7829},
  doi = {10.2307/40285739},
  shorttitle = {Neural {{Responses}} to {{Melodic}} and {{Harmonic Closure}}},
  abstract = {The event-related evoked potential (ERP) responses to sentence endings that either confirm or violate syntactic/semantic constraints have been extensively studied. Very little is known, however, about the corresponding situation with respect to music. The current study investigates the brain- wave (ERP) responses to perceived phrase closure. ERPs are a potentially valid measure of how language-like or uniquely musical the perception of phrase closure is. In our study, highly trained musicians (N= 16) judged whether or not novel musical phrases were closed (melodically or harmonically). Three stimulus series consisted of seven- note tunes with four possible endings: closed (tonic note or tonic chord), open/ diatonic (dominant chord or a member thereof), open/ chromatic (a chromatic note or chord outside the key of the melody), or open/white noise (a nonmusical control). One series included melodies alone, a second series included melodies harmonized, and a third series included melodies in which the melodic contexts were disrupted rather than the endings. In the recorded ERPs, a statistically significant negative drift in the waveforms occurred over the course of the context series, indicating anticipation of closure. The drift-corrected poststimulus waveforms for all series were subjected to a principal components analysis/analysis of variance. Two subject variables were also considered: sex and absolute pitch. All four stimulus types elicited identifiable responses. The waveform peaks for the four stimulus types are clearly differentiated by principal component analysis scores to two components: one with a maximum value at 273 ms and one with a maximum value at 471 ms. Taking the closed endings as the expected "standard," the waveforms for the two types of musical deviant endings were significantly below the standard at 273 ms and above the standard at 471 ms. The amount of negativity was proportional to the amount of deviance of the ending. The positive peak in the closed condition and the reduced peak in the open/diatonic condition are contrary to the normal inverse relationship between peak size and stimulus probability; the former agrees with peaks found in response to syntactic closure in language. Significant, though isolated, interactions involving both sex and absolute pitch also emerged.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1997-10-01},
  pages = {69-98},
  author = {Hantz, Edwin C. and Kreilick, Kelley G. and Kananen, William and Swartz, Kenneth P.},
  file = {D\:\\Sauve\\Zotero\\storage\\6MBMUZ36\\Hantz et al. - 1997 - Neural Responses to Melodic and Harmonic Closure .pdf}
}

@article{tillmannEffectsGlobalLocal1998,
  eprinttype = {jstor},
  eprint = {40285780},
  title = {Effects of {{Global}} and {{Local Contexts}} on {{Harmonic Expectancy}}},
  volume = {16},
  issn = {0730-7829},
  doi = {10.2307/40285780},
  abstract = {Several psycholinguistic studies have investigated the influence of local and global semantic contexts on word processing. The first aim of the present study was to examine local and global level contributions to harmonic priming. The second was to test a spreading-activation account of harmonic context effects (Bharucha, 1987). The expectations for the last chord (the target) of eight-chord sequences were varied by simultaneously manipulating the harmonic relationship of the target to the first six chords (global context) and to the seventh chord (local context). Human performances demonstrated that harmonic expectancies are derived from both the global and local levels of musical structure. Bharucha's connectionist model provides a possible account of local and global context effects. In isochronous chord sequences, harmonic priming seems to result from activation spreading via a schematic knowledge of tonal hierarchies.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1998-10-01},
  pages = {99-117},
  author = {Tillmann, B. and Bigand, E. and Pineau, M.},
  file = {D\:\\Sauve\\Zotero\\storage\\PQN95TQG\\Tillmann et al. - 1998 - Effects of Global and Local Contexts on Harmonic E.pdf}
}

@book{lemanMusicGestaltComputing1997,
  location = {{Berlin ; New York}},
  title = {Music, Gestalt, and Computing: Studies in Cognitive and Systematic Musicology},
  isbn = {0-387-55034-8},
  shorttitle = {Music, Gestalt, and Computing},
  series = {({{Lecture}} Notes in Computer Science ; 1317 {{Lecture}} Notes in Artifical Intelligence)},
  publisher = {{Springer-Verlag}},
  date = {1997},
  editora = {Leman, Marc},
  editoratype = {collaborator},
  note = {Physical descrip: ix 521 p. + 1 CD-ROM ; cm
General Note: Includes indexes
LC subject term: Logic, Symbolic and mathematical
LC subject term: Logic programming
LC subject term: Artificial intelligence}
}

@article{toiviainenTimbreSimilarityConvergence1998,
  eprinttype = {jstor},
  eprint = {40285788},
  title = {Timbre {{Similarity}}: {{Convergence}} of {{Neural}}, {{Behavioral}}, and {{Computational Approaches}}},
  volume = {16},
  issn = {0730-7829},
  doi = {10.2307/40285788},
  shorttitle = {Timbre {{Similarity}}},
  abstract = {The present study compared the degree of similarity of timbre representations as observed with brain recordings, behavioral studies, and computer simulations. To this end, the electrical brain activity of subjects was recorded while they were repetitively presented with five sounds differing in timbre. Subjects read simultaneously so that their attention was not focused on the sounds. The brain activity was quantified in terms of a change-specific mismatch negativity component. Thereafter, the subjects were asked to judge the similarity of all pairs along a five-step scale. A computer simulation was made by first training a Kohonen self-organizing map with a large set of instrumental sounds. The map was then tested with the experimental stimuli, and the distance between the most active artificial neurons was measured. The results of these methods were highly similar, suggesting that timbre representations reflected in behavioral measures correspond to neural activity, both as measured directly and as simulated in self-organizing neural network models.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1998-12-01},
  pages = {223-241},
  author = {Toiviainen, Petri and Tervaniemi, Mari and Louhivuori, Jukka and Saher, Marieke and Huotilainen, Minna and Näätänen, Risto},
  file = {D\:\\Sauve\\Zotero\\storage\\FFI5TDEZ\\Toiviainen et al. - 1998 - Timbre Similarity Convergence of Neural, Behavior.pdf}
}

@article{kendallPerceptualAcousticalFeatures1999,
  eprinttype = {jstor},
  eprint = {40285796},
  title = {Perceptual and {{Acoustical Features}} of {{Natural}} and {{Synthetic Orchestral Instrument Tones}}},
  volume = {16},
  issn = {0730-7829},
  doi = {10.2307/40285796},
  abstract = {Four experiments were conducted to explore the timbres of natural, continuant orchestral instruments with emulation based on sampling, frequency modulation ( FM) synthesis, and a hybrid consisting of sampling and synthesis techniques combined. Identification of instruments using verbal labels was significantly better for the natural and sampling- based signals than for either FM synthesis or the hybrid technique, a result also found for aural categorization. Perceptual scaling of timbral similarities demonstrated great consistency across a series of independent variables, including musical training, monophonic and stereo presentation, and long versus short signal durations. The first dimension of the classical multidimensional scaling (CMDS) solutions mapped onto long- time- average spectral centroid. The second dimension mapped onto a measure of spectral variability. Little evidence was found for the mapping of attack time or signal duration onto either dimension. A third dimension separated most natural instruments from their emulated counterparts. Experiments using verbal attribute ratings confirmed the correlation of spectral centroid, the first dimension of the perceptual space, and ratings of nasality; the second dimension correlated with spectral variability and modestly correlated with ratings of rich, brilliant, and tremulous. Mismatches of spectral distribution and variability resulted in poor emulations of the natural instruments. Results suggest that further study of centroid and time-variant psychophysical properties is warranted.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1999-04-01},
  pages = {327-363},
  author = {Kendall, Roger A. and Carterette, Edward C. and Hajda, John M.},
  file = {D\:\\Sauve\\Zotero\\storage\\JARPH72E\\Kendall et al. - 1999 - Perceptual and Acoustical Features of Natural and .pdf}
}

@article{temperleyWhatKeyKey1999,
  eprinttype = {jstor},
  eprint = {40285812},
  title = {What's {{Key}} for {{Key}}? {{The Krumhansl}}-{{Schmuckler Key}}-{{Finding Algorithm Reconsidered}}},
  volume = {17},
  issn = {0730-7829},
  doi = {10.2307/40285812},
  shorttitle = {What's {{Key}} for {{Key}}?},
  abstract = {This study examines the Krumhansl-Schmuckler key-finding model, in which the distribution of pitch classes in a piece is compared with an ideal distribution or "key profile" for each key. Several changes are proposed. First, the formula used for the matching process is somewhat simplified. Second, alternative values are proposed for the key profiles themselves. Third, rather than summing the durations of all events of each pitch class, the revised model divides the piece into short segments and labels each pitch class as present or absent in each segment. Fourth, a mechanism for modulation is proposed; a penalty is imposed for changing key from one segment to the next. An implementation of this model was subjected to two tests. First, the model was tested on the fugue subjects from Bach's Well-Tempered Clavier; the model's performance on this corpus is compared with the performances of other models. Second, the model was tested on a corpus of excerpts from the Kostka and Payne harmony textbook (as analyzed by Kostka). Several problems with the modified algorithm are discussed, concerning the rate of modulation, the role of harmony in key finding, and the role of pitch "spellings." The model is also compared with Huron and Parncutt's exponential decay model. The tests presented here suggest that the key-profile model, with the modifications proposed, can provide a highly successful approach to key finding.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1999-10-01},
  pages = {65-100},
  author = {Temperley, David},
  file = {D\:\\Sauve\\Zotero\\storage\\QK37TED8\\Temperley - 1999 - What's Key for Key The Krumhansl-Schmuckler Key-F.pdf}
}

@article{krumhanslMelodicExpectationFinnish1999,
  eprinttype = {jstor},
  eprint = {40285890},
  title = {Melodic {{Expectation}} in {{Finnish Spiritual Folk Hymns}}: {{Convergence}} of {{Statistical}}, {{Behavioral}}, and {{Computational Approaches}}},
  volume = {17},
  issn = {0730-7829},
  doi = {10.2307/40285890},
  shorttitle = {Melodic {{Expectation}} in {{Finnish Spiritual Folk Hymns}}},
  abstract = {This study of Finnish spiritual folk hymns combined three approaches to understanding melodic expectation. The first approach was a statistical style analysis of a representative corpus of 18 hymns, which determined the relative frequencies of tone onsets and two- and three-tone transitions. The second approach was a behavioral experiment in which listeners, either familiar (experts) or unfamiliar (nonexperts) with the hymns, made judgments about melodic continuations. The third approach simulated melodic expectation with neural network models of the self-organizing map (SOM) type (Kohonen, 1997). One model was trained on a corpus of Finnish folk songs and Lutheran hymns (Finnish SOM), while another was trained with the hymn contexts used in the experiment with the correct continuation tone (Hymn SOM). The three approaches converged on the following conclusions: (1) Listeners appear to be sensitive to the distributions of tones and tone transitions in music, (2) The nonexperts' responses more strongly reflected the general distribution of tones, whereas the experts' responses more strongly reflected the tone transitions and the correct continuations, (3) The SOMs produced results similar to listeners and also appeared sensitive to the distributions of tones and tone transitions, (4) The Hymn SOM correlated more strongly with the experts' judgments than the Finnish SOM, and (5) the principles of the implication-realization model (Narmour, 1990) were weighted similarly by the behavioral data and the Hymn SOM. /// Tässä suomalaisia hengellisiä kansansävelmiä käsittelevässä tutkimuksessa pyrittiin selvittämään melodisia odotuksia kolmen tutkimusmenetelmän avulla. Ensimmäinen menetelmä oli kyseistä tyyliä edustavien 18 sävelmän tilastollinen analyysi, jossa määritelteltiin sävelkorkeuksien sekä kahden ja kolmen sävelen siirtymien tilastolliset jakaumat. Toinen menetelmä oli behavioraalinen koe, jossa kuulijat arvioivat sävelmien jatkoja. Kuulijat jakaantuivat kahteen ryhmään: sävelmät tunteviin (asiantuntijoihin) ja sävelmiä tuntemattomiin (ei-asiantuntijoihin). Kolmannessa menetelmässä simuloitiin melodisia odotuksia itsejärjestäytyvään karttaan (Kohonen, 1997) perustuvalla keinotekoisella hermoverkkomallilla. Ensimmäiselle mallille opetettiin joukko suomalaisia kansanlauluja ja luterilaisia virsiä (suomalainen verkko), toiselle kokeessa käytettyjä hengellisiä kansansävelmiä (hengellinen verkko). Käytetyt menetelmät tuottivat yhteneviä tuloksia ja antoivat aihetta seuraaviin johtopäätöksiin: (1) kuulijat näyttävät olevan vastaanottavaisia musiikin säveljakaumille ja sävelsiirtymille, (2) ei-asiantuntijoiden vastaukset noudattivat enemmän sävelten yleistä jakaumaa, kun taas asiantuntijoiden vastaukset heijastivat enemmän sävelsiirtymiä ja sävelmien oikeita jatkoja, (3) hermoverkot tuottivat tuloksia, jotka olivat samankaltaisia kuulijoiden arvioiden kanssa ja jotka noudattivat sävelten ja sävelsiirtymien jakaumia, (4) hengellisen verkon tulokset korreloivat suomalaisen verkon tuloksia voimakkaammin asiantuntijoiden arvioiden kanssa, ja (5) behavioraaliset tulokset ja hengellinen verkko painottavat implikaatio-realisaatio-mallin (Narmour, 1990) periaatteita samalla tavalla.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {1999-12-01},
  pages = {151-195},
  author = {Krumhansl, Carol L. and Louhivuori, Jukka and Toiviainen, Petri and Järvinen, Topi and Eerola, Tuomas},
  file = {D\:\\Sauve\\Zotero\\storage\\GV8P5MBB\\Krumhansl et al. - 1999 - Melodic Expectation in Finnish Spiritual Folk Hymn.pdf}
}

@article{narmourMusicExpectationCognitive2000,
  eprinttype = {jstor},
  eprint = {40285821},
  title = {Music {{Expectation}} by {{Cognitive Rule}}-{{Mapping}}},
  volume = {17},
  issn = {0730-7829},
  doi = {10.2307/40285821},
  abstract = {Iterative rules appear everywhere in music cognition, creating strong expectations. Consequently, denial of rule projection becomes an important compositional strategy, generating numerous possibilities for musical affect. Other rules enter the musical aesthetic through reflexive game playing. Still other kinds are completely constructivist in nature and may be uncongenial to cognition, requiring much training to be recognized, if at all. Cognitive rules are frequently found in contexts of varied repetition (AA), but they are not necessarily bounded by stylistic similarity. Indeed, rules may be especially relevant in the processing of unfamiliar contexts (AB), where only abstract coding is available. There are many kinds of deduction in music cognition. Typical examples include melodic sequence, partial melodic sequence, and alternating melodic sequence (which produces streaming). These types may coexist in the musical fabric, involving the invocation of both simultaneous and nested rules. Intervallic expansion and reduction in melody also involve higherorder abstractions. Various mirrored forms in music entail rule-mapping as well, although these may be more difficult to perceive than their analogous visual symmetries. Listeners can likewise deduce additivity and subtractivity at work in harmony, tempo, texture, pace, and dynamics. Rhythmic augmentation and diminution, by contrast, rely on multiplication and division. The examples suggest numerous hypotheses for experimental research.},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2000-04-01},
  pages = {329-398},
  author = {Narmour, Eugene},
  file = {D\:\\Sauve\\Zotero\\storage\\GBZTWKGS\\Narmour - 2000 - Music Expectation by Cognitive Rule-Mapping.pdf}
}

@article{jonesTemporalAspectsStimulusDriven2002,
  langid = {english},
  title = {Temporal {{Aspects}} of {{Stimulus}}-{{Driven Attending}} in {{Dynamic Arrays}}},
  volume = {13},
  issn = {0956-7976, 1467-9280},
  url = {http://pss.sagepub.com/content/13/4/313},
  doi = {10.1111/1467-9280.00458},
  abstract = {Auditory sequences of tones were used to examine a form of stimulus-driven attending that involves temporal expectancies and is influenced by stimulus rhythm. Three experiments examined the influence of sequence timing on comparative pitch judgments of two tones (standard, comparison) separated by interpolated pitches. In two of the experiments, interpolated tones were regularly timed, with onset times of comparison tones varied relative to this rhythm. Listeners were most accurate judging the pitch of rhythmically expected tones and least accurate with very unexpected ones. This effect persisted over time, but disappeared when the rhythm of interpolated tones was either missing or irregular.},
  number = {4},
  journaltitle = {Psychological Science},
  shortjournal = {Psychological Science},
  urldate = {2016-06-13},
  date = {2002-07-01},
  pages = {313-319},
  author = {Jones, Mari Riess and Moynihan, Heather and MacKenzie, Noah and Puente, Jennifer},
  file = {D\:\\Sauve\\Zotero\\storage\\WGZ6W39S\\313.html},
  eprinttype = {pmid},
  eprint = {12137133}
}

@article{largeDynamicsAttendingHow1999,
  title = {The Dynamics of Attending: {{How}} People Track Time-Varying Events},
  volume = {106},
  issn = {1939-1471(Electronic);0033-295X(Print)},
  doi = {10.1037/0033-295X.106.1.119},
  shorttitle = {The Dynamics of Attending},
  abstract = {A theory of attentional dynamics is proposed and aimed at explaining how listeners respond to systematic change in everyday events while retaining a general sense of their rhythmic structure. The approach describes attending as the behavior of internal oscillations, called attending rhythms, that are capable of entraining to external events and targeting attentional energy to expected points in time. A mathematical formulation of the theory describes internal oscillations that focus pulses of attending energy and interact in various ways to enable attentional tracking of events with complex rhythms. This approach provides reliable predictions about the role of attending to event time structure in rhythmical events that modulate in rate, as demonstrated in 3 listening experiments.},
  number = {1},
  journaltitle = {Psychological Review},
  date = {1999},
  pages = {119-159},
  keywords = {Theories,*Attention,*Human Biological Rhythms,*Mathematical Modeling,*Pattern Discrimination,*Time Perception},
  author = {Large, Edward W. and Jones, Mari Riess}
}

@article{jonesDynamicAttendingResponses1989,
  title = {Dynamic Attending and Responses to Time},
  volume = {96},
  issn = {1939-1471(Electronic);0033-295X(Print)},
  doi = {10.1037/0033-295X.96.3.459},
  abstract = {A temporally based theory of attending is proposed that assumes that the structure of world events affords different attending modes. Future-oriented attending supports anticipatory behaviors and occurs with highly coherent temporal events. Time judgments, given this attending mode, are influenced by the way an event's ending confirms or violates temporal expectancies. Analytic attending supports other activities (e.g., grouping, counting), and if it occurs with events of low temporal coherence, then time judgments depend on the attending levels involved. A weighted contrast model describes over- and underestimations of event durations. The model applies to comparative duration judgments of equal and unequal time intervals; its rationale extends to temporal productions/extrapolations. Two experiments compare predictions of the contrast model with those derived from other traditional approaches.},
  number = {3},
  journaltitle = {Psychological Review},
  date = {1989},
  pages = {459-491},
  keywords = {Models,*Attention,*Time Estimation},
  author = {Jones, Mari R. and Boltz, Marilyn}
}

@article{trainorEffectsMusicalTraining2003,
  langid = {english},
  title = {Effects of {{Musical Training}} on the {{Auditory Cortex}} in {{Children}}},
  volume = {999},
  issn = {1749-6632},
  url = {http://onlinelibrary.wiley.com/doi/10.1196/annals.1284.061/abstract},
  doi = {10.1196/annals.1284.061},
  abstract = {Abstract:  Several studies of the effects of musical experience on sound representations in the auditory cortex are reviewed. Auditory evoked potentials are compared in response to pure tones, violin tones, and piano tones in adult musicians versus nonmusicians as well as in 4- to 5-year-old children who have either had or not had extensive musical experience. In addition, the effects of auditory frequency discrimination training in adult nonmusicians on auditory evoked potentials are examined. It was found that the P2-evoked response is larger in both adult and child musicians than in nonmusicians and that auditory training enhances this component in nonmusician adults. The results suggest that the P2 is particularly neuroplastic and that the effects of musical experience can be seen early in development. They also suggest that although the effects of musical training on cortical representations may be greater if training begins in childhood, the adult brain is also open to change. These results are discussed with respect to potential benefits of early musical training as well as potential benefits of musical experience in aging.},
  number = {1},
  journaltitle = {Annals of the New York Academy of Sciences},
  urldate = {2016-06-12},
  date = {2003-11-01},
  pages = {506-513},
  keywords = {Musicians,plasticity,Development,musical training,Auditory Cortex,Learning,Event-related potential (ERP),children},
  author = {Trainor, Laurel J. and Shahin, Antoine and Roberts, Larry E.},
  file = {D\:\\Sauve\\Zotero\\storage\\ST3Z279S\\abstract.html}
}

@article{trainorAreThereCritical2005,
  langid = {english},
  title = {Are There Critical Periods for Musical Development?},
  volume = {46},
  issn = {1098-2302},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/dev.20059/abstract},
  doi = {10.1002/dev.20059},
  abstract = {A critical period can be defined as a developmental window during which specific experience has a greater effect than at other times. Musical behavior involves many skills, including the basic encoding of pitch and time information, understanding scale and harmonic structure, performance, interpretation, and composition. We review studies of genetics, behavior, and brain structure and function in conjunction with the experiences of auditory deprivation and musical enrichment, and conclude that there is more supporting evidence for critical periods for basic than for more complex aspects of musical pitch acquisition. Much remains unknown about the mechanisms of interaction between genetic and experiential factors that create critical periods, but it is clear that there are multiple pathways for achieving musical expertise. © 2005 Wiley Periodicals, Inc. Dev Psychobiol 46: 262–278, 2005.},
  number = {3},
  journaltitle = {Developmental Psychobiology},
  shortjournal = {Dev. Psychobiol.},
  urldate = {2016-06-12},
  date = {2005-04-01},
  pages = {262-278},
  keywords = {musical expertise,auditory deprivation,critical period,musical enrichment,pitch acquisition,spectral structure,tonotopic map},
  author = {Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\9S9XF6RJ\\abstract\;jsessionid=510E80CEDDCD7669D95817BF8CAD0221.html}
}

@article{bigandPerceptionMusicalTension1996,
  langid = {english},
  title = {Perception of Musical Tension in Short Chord Sequences: {{The}} Influence of Harmonic Function, Sensory Dissonance, Horizontal Motion, and Musical Training},
  volume = {58},
  issn = {0031-5117, 1532-5962},
  url = {http://link.springer.com/article/10.3758/BF03205482},
  doi = {10.3758/BF03205482},
  shorttitle = {Perception of Musical Tension in Short Chord Sequences},
  abstract = {This study investigates the effect of four variables (tonal hierarchies, sensory chordal consonance, horizontal motion, and musical training) on perceived musical tension. Participants were asked to evaluate the tension created by a chord X in sequences of three chords \{C major → X → C major\} in a C major context key. The X chords could be major or minor triads major-minor seventh, or minor seventh chords built on the 12 notes of the chromatic scale. The data were compared with Krumhansl’s (1990) harmonic hierarchy and with predictions of Lerdahl’s (1988) cognitive theory, Hutchinson and Knopoff’s (1978) and Parncutt’s (1989) sensory-psychoacoustical theories, and the model of horizontal motion defined in the paper. As a main outcome, it appears that judgments of tension arose from a convergence of several cognitive and psychoacoustics influences, whose relative importance varies, depending on musical training.},
  number = {1},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  urldate = {2016-06-12},
  date = {1996-01},
  pages = {125-141},
  keywords = {cognitive psychology},
  author = {Bigand, Emmanuel and Parncutt, Richard and Lerdahl, Fred},
  file = {D\:\\Sauve\\Zotero\\storage\\M7NPBD5N\\Bigand et al. - 1996 - Perception of musical tension in short chord seque.pdf;D\:\\Sauve\\Zotero\\storage\\GRTM9GNR\\BF03205482.html}
}

@article{straitBiologicalImpactAuditory2014,
  langid = {english},
  title = {Biological Impact of Auditory Expertise across the Life Span: Musicians as a Model of Auditory Learning},
  volume = {308},
  issn = {1878-5891},
  doi = {10.1016/j.heares.2013.08.004},
  shorttitle = {Biological Impact of Auditory Expertise across the Life Span},
  abstract = {Experience-dependent characteristics of auditory function, especially with regard to speech-evoked auditory neurophysiology, have garnered increasing attention in recent years. This interest stems from both pragmatic and theoretical concerns as it bears implications for the prevention and remediation of language-based learning impairment in addition to providing insight into mechanisms engendering experience-dependent changes in human sensory function. Musicians provide an attractive model for studying the experience-dependency of auditory processing in humans due to their distinctive neural enhancements compared to nonmusicians. We have only recently begun to address whether these enhancements are observable early in life, during the initial years of music training when the auditory system is under rapid development, as well as later in life, after the onset of the aging process. Here we review neural enhancements in musically trained individuals across the life span in the context of cellular mechanisms that underlie learning, identified in animal models. Musicians' subcortical physiologic enhancements are interpreted according to a cognitive framework for auditory learning, providing a model in which to study mechanisms of experience-dependent changes in human auditory function.},
  journaltitle = {Hearing Research},
  shortjournal = {Hear. Res.},
  date = {2014-02},
  pages = {109-121},
  keywords = {Attention,brain,cognition,Auditory Perception,Music,pitch perception,Acoustic Stimulation,animals,humans,HEARING,Learning,Time Factors,Models; Neurological,Evoked Potentials; Auditory,Age Factors,Gene Expression Regulation,Models; Animal,Synaptic Transmission},
  author = {Strait, Dana L and Kraus, Nina},
  eprinttype = {pmid},
  eprint = {23988583},
  pmcid = {PMC3947192}
}

@article{krausMusicTrainingDevelopment2010,
  langid = {english},
  title = {Music Training for the Development of Auditory Skills},
  volume = {11},
  issn = {1471-003X},
  url = {http://www.nature.com/nrn/journal/v11/n8/full/nrn2882.html},
  doi = {10.1038/nrn2882},
  abstract = {The effects of music training in relation to brain plasticity have caused excitement, evident from the popularity of books on this topic among scientists and the general public. Neuroscience research has shown that music training leads to changes throughout the auditory system that prime musicians for listening challenges beyond music processing. This effect of music training suggests that, akin to physical exercise and its impact on body fitness, music is a resource that tones the brain for auditory fitness. Therefore, the role of music in shaping individual development deserves consideration.},
  number = {8},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  urldate = {2016-06-12},
  date = {2010-08},
  pages = {599-605},
  author = {Kraus, Nina and Chandrasekaran, Bharath},
  file = {D\:\\Sauve\\Zotero\\storage\\9BJ28X8V\\nrn2882.html}
}

@article{krausCognitiveFactorsShape2012,
  langid = {english},
  title = {Cognitive Factors Shape Brain Networks for Auditory Skills: Spotlight on Auditory Working Memory},
  volume = {1252},
  issn = {1749-6632},
  doi = {10.1111/j.1749-6632.2012.06463.x},
  shorttitle = {Cognitive Factors Shape Brain Networks for Auditory Skills},
  abstract = {Musicians benefit from real-life advantages, such as a greater ability to hear speech in noise and to remember sounds, although the biological mechanisms driving such advantages remain undetermined. Furthermore, the extent to which these advantages are a consequence of musical training or innate characteristics that predispose a given individual to pursue music training is often debated. Here, we examine biological underpinnings of musicians' auditory advantages and the mediating role of auditory working memory. Results from our laboratory are presented within a framework that emphasizes auditory working memory as a major factor in the neural processing of sound. Within this framework, we provide evidence for music training as a contributing source of these abilities.},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann. N. Y. Acad. Sci.},
  date = {2012-04},
  pages = {100-107},
  keywords = {brain,cognition,Auditory Perception,Music,Acoustic Stimulation,Auditory Pathways,humans,Adult,Middle Aged,Speech Perception,Memory; Short-Term,Young Adult,Neurosciences,Adolescent,Aged,Child,Nerve Net,Noise,Perceptual Masking},
  author = {Kraus, Nina and Strait, Dana L. and Parbery-Clark, Alexandra},
  eprinttype = {pmid},
  eprint = {22524346},
  pmcid = {PMC3338202}
}

@article{wongMusicalExperienceShapes2007,
  langid = {english},
  title = {Musical Experience Shapes Human Brainstem Encoding of Linguistic Pitch Patterns},
  volume = {10},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v10/n4/abs/nn1872.html},
  doi = {10.1038/nn1872},
  abstract = {Music and speech are very cognitively demanding auditory phenomena generally attributed to cortical rather than subcortical circuitry. We examined brainstem encoding of linguistic pitch and found that musicians show more robust and faithful encoding compared with nonmusicians. These results not only implicate a common subcortical manifestation for two presumed cortical functions, but also a possible reciprocity of corticofugal speech and music tuning, providing neurophysiological explanations for musicians' higher language-learning ability.},
  number = {4},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-06-12},
  date = {2007-04},
  pages = {420-422},
  author = {Wong, Patrick C. M. and Skoe, Erika and Russo, Nicole M. and Dees, Tasha and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\MN7VKDZ9\\nn1872.html}
}

@article{thompsonFastLoudBackground2012,
  langid = {english},
  title = {Fast and Loud Background Music Disrupts Reading Comprehension},
  volume = {40},
  issn = {0305-7356, 1741-3087},
  url = {http://pom.sagepub.com/content/40/6/700},
  doi = {10.1177/0305735611400173},
  abstract = {We examined the effect of background music on reading comprehension. Because the emotional consequences of music listening are affected by changes in tempo and intensity, we manipulated these variables to create four repeated-measures conditions: slow/low, slow/high, fast/low, fast/high. Tempo and intensity manipulations were selected to be psychologically equivalent in magnitude (pilot study 1). In each condition, 25 participants were given four minutes to read a passage, followed by three minutes to answer six multiple-choice questions. Baseline performance was established by having control participants complete the reading task in silence (pilot study 2). A significant tempo by intensity interaction was observed, with comprehension in the fast/high condition falling significantly below baseline. These findings reveal that listening to background instrumental music is most likely to disrupt reading comprehension when the music is fast and loud.},
  number = {6},
  journaltitle = {Psychology of Music},
  shortjournal = {Psychology of Music},
  urldate = {2016-06-11},
  date = {2012-11-01},
  pages = {700-708},
  keywords = {background music,music and cognition,music and reading,reading comprehension},
  author = {Thompson, William Forde and Schellenberg, E. Glenn and Letnic, Adriana Katharine},
  file = {D\:\\Sauve\\Zotero\\storage\\3TCP2DH7\\700.html}
}

@article{kigerEffectsMusicInformation1989,
  title = {Effects of Music Information Load on a Reading Comprehension Task},
  volume = {69},
  issn = {1558-688X(Electronic);0031-5125(Print)},
  doi = {10.2466/pms.1989.69.2.531},
  abstract = {Examined the effects of high or low music information-load (MIL) on 54 high school sophomores' reading comprehension while controlling for prior background music/study habits via a pretest questionnaire. Ss who read passages in the presence of low MIL performed significantly better than those who read in silence or with high MIL. The effects of music on comprehension may be explained in terms of an early selection filter model of attention, which contends that information is actively blocked early in processing when it might interfere with higher-order processing. Another explanation of the results may involve the arousal effects of music.},
  number = {2},
  journaltitle = {Perceptual and Motor Skills},
  date = {1989},
  pages = {531-534},
  keywords = {*Auditory Stimulation,*Music,*Reading Comprehension,High School Students},
  author = {Kiger, Derrick M.},
  file = {D\:\\Sauve\\Zotero\\storage\\CVGIXGSF\\1990-13323-001.html}
}

@article{haimsonMathematiciansHaveAverage2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.203},
  title = {Do {{Mathematicians Have Above Average Musical Skill}}?},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.203},
  abstract = {Accompanying the view that music training leads to improved mathematical performance is the view that that there is an overlap between the kinds of skills needed for music and mathematics. We examined the popular conception that mathematicians have better music abilities than nonmathematicians. We administered a self-report questionnaire via the internet to assess musicality (music perception and music memory) and musicianship (music performance and music creation). Respondents were doctoral-level members of the American Mathematical Association or the Modern Language Association (i.e., literature and language scholars). The mathematics group did not exhibit higher levels of either musicality or musicianship. Among those reporting high music-performance ability (facility in playing an instrument and/or sight-reading ability), mathematicians did not report significantly greater musicality than did the literature/language scholars. These findings do not lend support to the hypothesis that mathematicians are more musical than people with nonquantitative backgrounds.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {203-213},
  keywords = {musical training},
  author = {Haimson, Jennifer and Swain, Deanna and Winner, Ellen},
  file = {D\:\\Sauve\\Zotero\\storage\\E2QZ2UCV\\Haimson et al. - 2011 - Do Mathematicians Have Above Average Musical Skill.pdf}
}

@article{degeMusicLessonsIntelligence2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.195},
  title = {Music {{Lessons}} and {{Intelligence}}: {{A Relation Mediated}} by {{Executive Functions}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.195},
  shorttitle = {Music {{Lessons}} and {{Intelligence}}},
  abstract = {The present study investigated whether the association between music lessons and intelligence is mediated by executive functions. Intelligence and five different executive functions (set shifting, selective attention, planning, inhibition, and fluency) were assessed in 9- to 12-year-old children with varying amounts of music lessons. Significant associations emerged between music lessons and all of the measures of executive function. Executive functions mediated the association between music lessons and intelligence, with the measures of selective attention and inhibition being the strongest contributors to the mediation effect. Our results suggest that at least part of the association between music lessons and intelligence is explained by the positive influence music lessons have on executive functions, which in turn improve performance on intelligence tests.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {195-201},
  keywords = {musical training},
  author = {Degé, Franziska and Kubicek, Claudia and Schwarzer, Gudrun},
  file = {D\:\\Sauve\\Zotero\\storage\\66QAE2XV\\Degé et al. - 2011 - Music Lessons and Intelligence A Relation Mediate.pdf}
}

@article{steeleListeningMozartDoes1997,
  langid = {english},
  title = {Listening to {{Mozart}} Does Not Enhance Backwards Digit Span Performance},
  volume = {84},
  issn = {0031-5125},
  doi = {10.2466/pms.1997.84.3c.1179},
  abstract = {Rauscher, Shaw, and Ky recently reported that exposure to brief periods of music by Mozart produced a temporary increase in performance on tasks taken from the Stanford-Binet Intelligence Scale-IV. The present study examined whether this effect occurred in performance on a backwards digit span task. In a within-subjects design 36 undergraduates were exposed to 10-min. periods of Mozart music, a recording of rain, or silence. After each stimulus period, undergraduates had three attempts to hear and recall different 9-digit strings in reverse order. No significant differences among treatment conditions were found. There was a significant effect of practice. Results are discussed in terms of the need to isolate the conditions responsible for production of the Mozart effect.},
  issue = {3 Pt 2},
  journaltitle = {Perceptual and Motor Skills},
  shortjournal = {Percept Mot Skills},
  date = {1997-06},
  pages = {1179-1184},
  keywords = {Auditory Perception,Music,Acoustic Stimulation,humans,Female,Male,Memory; Short-Term,Practice (Psychology),SOUND,Intelligence Tests,Mental Recall},
  author = {Steele, K. M. and Ball, T. N. and Runk, R.},
  eprinttype = {pmid},
  eprint = {9229433}
}

@article{chanMusicTrainingImproves1998,
  langid = {english},
  title = {Music Training Improves Verbal Memory},
  volume = {396},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v396/n6707/abs/396128a0.html},
  doi = {10.1038/24075},
  abstract = {Magnetic resonance imaging has shown that the left planum temporale region of the brain is larger in musicians than in non-musicians. If this results from a change in cortical organization,, the left temporal area in musicians might have a better developed cognitive function than the right temporal lobe. Because verbal memory is mediated mainly by the left temporal lobe, and visual memory by the right,, adults with music training should have better verbal, but not visual, memory than adults without such training. Here we show that adults who received music training before the age of 12 have a better memory for spoken words than those who did not. Music training in childhood may therefore have long-term positive effects on verbal memory.},
  number = {6707},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2016-06-10},
  date = {1998-11-12},
  pages = {128-128},
  author = {Chan, Agnes S. and Ho, Yim-Chi and Cheung, Mei-Chun},
  file = {D\:\\Sauve\\Zotero\\storage\\35T7WEUF\\396128a0.html}
}

@article{franklinEffectsMusicalTraining2008,
  langid = {english},
  title = {The Effects of Musical Training on Verbal Memory},
  issn = {0305-7356, 1741-3087},
  url = {http://pom.sagepub.com/content/early/2008/04/16/0305735607086044},
  doi = {10.1177/0305735607086044},
  abstract = {A number of studies suggest a link between musical training and general cognitive abilities. Despite some positive results, there is disagreement about which abilities are improved. One line of research leads to the hypothesis that verbal abilities in general, and verbal memory in particular, are related to musical training. In the present article, we review this line of research and present newly collected data comparing trained musicians to non-musicians on a number of tasks that recruit verbal memory. The results showed an advantage for musicians' long-term verbal memory that disappeared when articulatory suppression was introduced. In addition, we found evidence for a greater verbal working memory span in musicians. Together, these results show that musical training may influence verbal working memory and long-term memory, and they suggest that these improved abilities are due to enhanced verbal rehearsal mechanisms in musicians.},
  journaltitle = {Psychology of Music},
  shortjournal = {Psychology of Music},
  urldate = {2016-06-10},
  date = {2008-04-01},
  keywords = {Working memory,articulatory suppression,memory span},
  author = {Franklin, Michael S. and Moore, Katherine Sledge and Yip, Chun-Yu and Jonides, John and Rattray, Katie and Moher, Jeff},
  file = {D\:\\Sauve\\Zotero\\storage\\9ZKUS9V2\\0305735607086044.html}
}

@article{anvariRelationsMusicalSkills2002,
  title = {Relations among Musical Skills, Phonological Processing, and Early Reading Ability in Preschool Children},
  volume = {83},
  issn = {0022-0965},
  url = {http://www.sciencedirect.com/science/article/pii/S0022096502001248},
  doi = {10.1016/S0022-0965(02)00124-8},
  abstract = {We examined the relations among phonological awareness, music perception skills, and early reading skills in a population of 100 4- and 5-year-old children. Music skills were found to correlate significantly with both phonological awareness and reading development. Regression analyses indicated that music perception skills contributed unique variance in predicting reading ability, even when variance due to phonological awareness and other cognitive abilities (math, digit span, and vocabulary) had been accounted for. Thus, music perception appears to tap auditory mechanisms related to reading that only partially overlap with those related to phonological awareness, suggesting that both linguistic and nonlinguistic general auditory mechanisms are involved in reading.},
  number = {2},
  journaltitle = {Journal of Experimental Child Psychology},
  shortjournal = {Journal of Experimental Child Psychology},
  urldate = {2016-06-10},
  date = {2002-10},
  pages = {111-130},
  keywords = {Development,Music,Auditory processes,Reading},
  author = {Anvari, Sima H and Trainor, Laurel J and Woodside, Jennifer and Levy, Betty Ann},
  file = {D\:\\Sauve\\Zotero\\storage\\7R6DJ3EZ\\S0022096502001248.html}
}

@article{fiedlerValidationGoldMSIQuestionnaire2015,
  title = {Validation of the {{Gold}}-{{MSI}} Questionnaire to Measure Musical Sophistication of {{German}} Students at Secondary Education Schools.},
  volume = {36},
  url = {http://research.gold.ac.uk/17193/},
  journaltitle = {Musikpädagogische Forschung / Research in Music Education},
  urldate = {2016-06-10},
  date = {2015},
  pages = {199-219},
  author = {Fiedler, D. and Müllensiefen, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\79HE8NVF\\17193.html}
}

@article{schaalGoldMSIReplikationUnd2014,
  langid = {english},
  title = {Der {{Gold}}-{{MSI}}: {{Replikation}} Und {{Validierung}} Eines {{Fragebogeninstrumentes}} Zur {{Messung Musikalischer Erfahrenheit}} Anhand Einer Deutschen {{Stichprobe}}},
  volume = {18},
  issn = {1029-8649,},
  url = {http://msx.sagepub.com/content/18/4/423},
  doi = {10.1177/1029864914541851},
  shorttitle = {Der {{Gold}}-{{MSI}}},
  abstract = {The present study introduces the German version of the Gold-MSI inventory, a tool for evaluating self-reported musical abilities and musical expertise. The Gold-MSI is based around the multidimensional construct of Musical Sophistication and builds on the idea that musical expertise cannot only be developed through musical training on an instrument but also through active engagement with music in its many facets. The questionnaire was developed with a very large English sample (Müllensiefen et al., 2014) and comprises musical expertise with five factors as well as the general factor Musical Sophistication. The English Gold-MSI questionnaire was translated into German and evaluated with a German sample (N = 641). Using confirmative factor analysis the underlying factor structure was confirmed. Furthermore, the results show high reliabilities of the five sub-factors as well as the general factor Musical Sophistication (Cronbach’s alpha between .72 and .91.). Additionally, relationships between variables of the socio-economic status and the sub-factors of the Gold-MSI of the German sample are investigated using a structural equation model. The statistical model reveals positive relationships between income and professional status on the one hand and musical training, perceptual abilities and emotional engagement with music on the other hand. The inventory is freely available and is designed to contribute to the refined investigation of musical sophistication and expertise in German speaking countries.},
  number = {4},
  journaltitle = {Musicae Scientiae},
  shortjournal = {Musicae Scientiae},
  urldate = {2016-06-10},
  date = {2014-12-01},
  pages = {423-447},
  keywords = {musical expertise,Gold-MSI questionnaire,musical sophistication,socio-economic status},
  author = {Schaal, Nora K. and Bauer, Anna-Katharina R. and Müllensiefen, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\39GDQZQB\\423.html}
}

@article{mcdonaldUsesFunctionsMusic2008,
  langid = {english},
  title = {Uses and {{Functions}} of {{Music}} in {{Congenital Amusia}}},
  volume = {25},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/25/4/345},
  doi = {10.1525/mp.2008.25.4.345},
  abstract = {THE GOAL OF THIS STUDY WAS TO ASCERTAIN whether deficits in music perception impact upon music appreciation. Likert ratings were gathered from congenital amusics and matched controls concerning the degree to which individuals incorporate music in their everyday lives, are able to achieve certain psychological states through music, and feel positively about music imposed upon them. Those with amusia reported incorporating music into everyday activities to a lesser degree than controls. They also reported experiencing fewer changes in psychological states when listening to music and felt more negatively about imposed music compared to controls. However, the scores of some amusic individuals fell within the control range on these questionnaires, providing some evidence for a developmental dissociation between music perception (impaired) and music appreciation (normal). Potential reasons for this dissociation are discussed.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-06-10},
  date = {2008-04-01},
  pages = {345-355},
  keywords = {Music,Perception,appreciation,congenital amusia,everyday listening},
  author = {Mcdonald, Claire and Stewart, Lauren},
  file = {D\:\\Sauve\\Zotero\\storage\\ZFXQP98G\\345.html}
}

@article{cuddyMusicalDifficultiesAre2005,
  langid = {english},
  title = {Musical Difficulties Are Rare: A Study of "Tone Deafness" among University Students},
  volume = {1060},
  issn = {0077-8923},
  doi = {10.1196/annals.1360.026},
  shorttitle = {Musical Difficulties Are Rare},
  abstract = {This study was concerned with self-reported "tone deafness" and its possible relationship to congenital amusia. Nearly 17\% of over 2,000 first-year psychology students at Queen's University self-reported tone deafness. Two hundred students were recruited from this pool of students, comprising 100 who reported tone deafness and 100 who reported that they were not tone-deaf (NTD). The study contained two parts. In part 1, participants completed the six tests of the Montreal Battery of Evaluation of Amusia (MBEA) developed by Peretz and collaborators. In part 2, participants completed an extensive questionnaire designed to elicit details about musical experiences, abilities, training, and interests. Twenty-eight questionnaire items allowing a quantitative response were subjected to factor analysis. Four orthogonal components emerged from the analysis. The components reflected self-report of (1) vocal production, (2) music instruction, (3) listening attitudes, and (4) childhood memories of musical environment. Results for each of the MBEA tests and composite scores for all tests were regressed on participants' factor scores. The best and significant predictors of the MBEA scores were factor I and factor II, followed by factor III. Factor scores accounted for a higher percentage of the variance in MBEA composite test results (27\%) than the self-report of tone deafness alone (7\%). The musical difficulties revealed by the MBEA test results for some participants warrant further attention and study. However, an encouraging conclusion from the MBEA results is that many individuals who consider themselves "tone-deaf" may not, in fact, have perceptual difficulties, and these individuals should be supported in any of their efforts to proceed with music enjoyment and instruction.},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann. N. Y. Acad. Sci.},
  date = {2005-12},
  pages = {311-324},
  keywords = {Music,Pitch Discrimination,pitch perception,Auditory Pathways,humans,Adult,Middle Aged,Female,Male,Models; Neurological,Adolescent,Aged,Perceptual Disorders,Surveys and Questionnaires},
  author = {Cuddy, Lola L. and Balkwill, Laura-Lee and Peretz, Isabelle and Holden, Ronald R.},
  eprinttype = {pmid},
  eprint = {16597781}
}

@article{peretzVarietiesMusicalDisorders2003,
  langid = {english},
  title = {Varieties of {{Musical Disorders}}},
  volume = {999},
  issn = {1749-6632},
  url = {http://onlinelibrary.wiley.com/doi/10.1196/annals.1284.006/abstract},
  doi = {10.1196/annals.1284.006},
  abstract = {Abstract:  Multiple disorders of musical abilities can occur after brain damage. Conversely, early brain anomalies or vast brain injuries may sometimes spare ordinary musical skills in individuals who experience severe cognitive losses. To document these incidences, comprehensive behavioral testing is required. We propose to use the Montreal Battery of Evaluation of Amusia (MBEA) because it is arguably the best tool currently available. Over the last decade, this battery was developed and validated in populations with brain damage of various etiologies. Furthermore, the MBEA is theoretically motivated and satisfies important psychometric properties. It is sensitive, normally distributed, reliable on test-retest, and correlates with Gordon's Musical Aptitude Profile, another more widely used battery of tests. To promote its wide usage, the MBEA is now available upon request. In addition, individual MBEA data of 160 normal participants of variable age and education have been made available to all via the internet.},
  number = {1},
  journaltitle = {Annals of the New York Academy of Sciences},
  urldate = {2016-06-10},
  date = {2003-11-01},
  pages = {58-75},
  keywords = {amusia evaluation,disorders of music perception and memory,learning deficits,musical disorders,tests of musical abilities},
  author = {Peretz, Isabelle and Champod, Anne Sophie and Hyde, Krista},
  file = {D\:\\Sauve\\Zotero\\storage\\SHGDT95C\\abstract\;jsessionid=185AA3E92A1A138F74973EF823A20BB0.html}
}

@misc{bentleyBentleyMeasuresMusical1966,
  title = {Bentley Measures of Musical Abilities},
  publisher = {{London: Harrap}},
  date = {1966},
  author = {Bentley, A}
}

@misc{gordonAdvancedMeasuresMusic1989,
  title = {Advanced Measures of Music Audiation},
  publisher = {{Chicago: Riverside Publishing Company}},
  date = {1989},
  author = {Gordon, EE}
}

@misc{seashoreSeashoreMeasuresMusical1960,
  title = {Seashore Measures of Musical Talen},
  publisher = {{New York: The Psychological Corporation}},
  date = {1960},
  author = {Seashore, CE and Lewis, D and Saetveit, JG}
}

@article{bilhartzEffectEarlyMusic1999,
  title = {The {{Effect}} of {{Early Music Training}} on {{Child Cognitive Development}}},
  volume = {20},
  issn = {0193-3973},
  url = {http://www.sciencedirect.com/science/article/pii/S0193397399000337},
  doi = {10.1016/S0193-3973(99)00033-7},
  abstract = {The relationship between participation in a structured music curriculum and cognitive development was studied with 71 4- through 6-year olds. Children were pre- and posttested with six subtests of the Stanford-Binet Intelligence Scale, fourth edition (SB) and the Young Child Music Skills Assessment (MSA). Approximately one half of the sample participated in a 30-week, 75-minute weekly, parent-involved music curriculum. Statistical analysis showed significant gains for participants receiving music instruction on the MSA and on the SB Bead Memory subtest. Four-order partial correlations analysis found musical treatment influence on Bead Memory scores when the participants were controlled for sex, ethnicity, parental education, and economic class. Treatment also produced higher scores on other SB measurements for select populations. This study suggests a significant correspondence between early music instruction and spatial–temporal reasoning abilities.},
  number = {4},
  journaltitle = {Journal of Applied Developmental Psychology},
  shortjournal = {Journal of Applied Developmental Psychology},
  urldate = {2016-06-09},
  date = {1999-12},
  pages = {615-636},
  author = {Bilhartz, Terry D and Bruhn, Rick A and Olson, Judith E},
  file = {D\:\\Sauve\\Zotero\\storage\\6MK85PVX\\S0193397399000337.html}
}

@article{rauscherClassroomKeyboardInstruction2000,
  title = {Classroom Keyboard Instruction Improves Kindergarten Children’s Spatial-Temporal Performance: {{A}} Field Experiment},
  volume = {15},
  issn = {0885-2006},
  url = {http://www.sciencedirect.com/science/article/pii/S0885200600000508},
  doi = {10.1016/S0885-2006(00)00050-8},
  shorttitle = {Classroom Keyboard Instruction Improves Kindergarten Children’s Spatial-Temporal Performance},
  abstract = {The purpose of this study was to determine the effects of classroom music instruction featuring the keyboard on the spatial-temporal reasoning of kindergarten children. Sixty-two kindergartners were assigned to one of two conditions, keyboard or no music. All children were pretested with two spatial-temporal tasks and one pictorial memory task. The keyboard group was provided with 20-min lessons two times per week in groups of approximately 10 children. Children were then retested at two 4-month intervals. The keyboard group scored significantly higher than the no music group on both spatial-temporal tasks after 4 months of lessons, a difference that was greater in magnitude after 8 months of lessons. Pictorial memory did not differ for the two groups after the lessons. These data support studies that found similar skills enhancements in preschool children, despite vast differences in the setting in which the instruction occurred. The results have strong implications for school administrators and educators.},
  number = {2},
  journaltitle = {Early Childhood Research Quarterly},
  shortjournal = {Early Childhood Research Quarterly},
  urldate = {2016-06-09},
  date = {2000},
  pages = {215-228},
  author = {Rauscher, Frances H and Zupan, Mary Anne},
  file = {D\:\\Sauve\\Zotero\\storage\\ADIQ2M9E\\S0885200600000508.html}
}

@article{hurwitzNonmusicolEffectsKodaly1975,
  langid = {english},
  title = {Nonmusicol {{Effects}} of the {{Kodaly Music Curriculum}} in {{Primary Grade Children}}},
  volume = {8},
  issn = {0022-2194,},
  url = {http://ldx.sagepub.com/content/8/3/167},
  doi = {10.1177/002221947500800310},
  abstract = {This study compares the performances of two matched groups of primary grade children on tasks of temporal and spatial abilities. One group received an intensive exposure to the Kodaly Music Training Program, while the other group did not. The results indicated that the music group performed more effectively on both temporal and spatial tasks than the non-music control group. Sex differences are also reported in which experimental group boys demonstrated a significantly better level of performance than control boys. Comparison with a second control group indicated that children receiving the Kodaly Music Program performed more effectively on reading tests than comparable groups of first graders not receiving this music instruction. This facilitative effect on reading performance was observed beyond the first grade level when the music program was continued.},
  number = {3},
  journaltitle = {Journal of Learning Disabilities},
  shortjournal = {J Learn Disabil},
  urldate = {2016-06-09},
  date = {1975-01-03},
  pages = {167-174},
  author = {Hurwitz, Irving and Wolff, Peter H. and Bortnick, Barrie D. and Kokas, Klara},
  file = {D\:\\Sauve\\Zotero\\storage\\MKDJ25XJ\\167.html}
}

@article{barwickRelationsReadingMusical1989,
  langid = {english},
  title = {Relations {{Between Reading}} and {{Musical Abilities}}},
  volume = {59},
  issn = {2044-8279},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.2044-8279.1989.tb03097.x/abstract},
  doi = {10.1111/j.2044-8279.1989.tb03097.x},
  abstract = {Summary.  In two studies, groups of children with a wide range of reading ability and including a high proportion of poor readers were given Bentley's musical ability battery. Scores on tonal memory and chord analysis were related significantly to reading age with chronological age and IQ partialled out. The implications of these findings for views about component skills involved in reading and listening to music are discussed.},
  number = {2},
  journaltitle = {British Journal of Educational Psychology},
  urldate = {2016-06-09},
  date = {1989-06-01},
  pages = {253-257},
  author = {Barwick, Julia and Valentine, Elizabeth and West, Robert and Wilding, John},
  file = {D\:\\Sauve\\Zotero\\storage\\23ZMDTBG\\abstract\;jsessionid=83A48EE11F581CE5F6D92BD97621AB43.html}
}

@article{phillipsInvestigationRelationshipMusicality1976,
  title = {An Investigation of the Relationship between Musicality and Intelligence},
  volume = {4},
  issn = {1741-3087(Electronic);0305-7356(Print)},
  doi = {10.1177/030573567642003},
  abstract = {Investigated the association of musicality, musical background, and intelligence with social status. Ss were 194 3rd-yr students from 4 junior schools, ranging in status from lower class to upper-middle class. Musical aptitude was assessed by the Wing Standardized Tests of Musical Intelligence (TMI) and the Tests of Rhythmic Ability for Children; intelligence was assessed by the Cognitive Abilities Test (CAT); and a questionnaire was administered to each S in order to determine his/her degree of musical training. Correlations were .61 between TMI and CAT, and .69 between TRA and CAT. Scores on each of the tests decreased progressively from School A to School D, and most of the differences between schools on each test were significant. Division of the Ss into 5 groups of increasing TMI mean scores yielded proportional increases in CAT mean scores. Results show a close relationship between musicality and intelligence which, along with musical background, is associated with social status. (23 ref)},
  number = {2},
  journaltitle = {Psychology of Music},
  date = {1976},
  pages = {16-31},
  keywords = {*Intelligence,*Junior High School Students,*Music Education,*Musical Ability,Socioeconomic Status},
  author = {Phillips, Douglas},
  file = {D\:\\Sauve\\Zotero\\storage\\KZS4J3UQ\\1980-04131-001.html}
}

@article{gromkoEffectMusicTraining1998,
  langid = {english},
  title = {The {{Effect}} of {{Music Training}} on {{Preschoolers}}' {{Spatial}}-{{Temporal Task Performance}}},
  volume = {46},
  issn = {0022-4294, 1945-0095},
  url = {http://jrm.sagepub.com/content/46/2/173},
  doi = {10.2307/3345621},
  abstract = {The purpose of this study was to investigate the effect of music training on preschoolers' Performance IQ (Wechsler Preschool and Primary Intelligence Scale-Revised, 1989). Preschoolers in the treatment group (N = 15) met weekly from October 1996 through April 1997. A Mann-Whitney test on Performance IQ (scaled) gain scores by group yielded U = 67, p =.059; a Mann-Whitney test on Performance IQ (raw) gain scores by group yielded U = 65, p =.049. Regressions of IQ gain scores on age showed significantly less gain for older children in the control group (N = 15). A regression analysis showed that the relationship of Performance IQ to age was not significant for the treatment group. Slopes intersected at age 3. For 3-year-olds in this study, an intellectually stimulating environment, per se, results in a gain in the ability to perform spatial-temporal tasks.},
  number = {2},
  journaltitle = {Journal of Research in Music Education},
  shortjournal = {Journal of Research in Music Education},
  urldate = {2016-06-09},
  date = {1998-01-07},
  pages = {173-181},
  author = {Gromko, Joyce Eastlund and Poorman, Allison Smith},
  file = {D\:\\Sauve\\Zotero\\storage\\XFNTZ8CZ\\173.html}
}

@article{hutchinsonCerebellarVolumeMusicians2003,
  langid = {english},
  title = {Cerebellar Volume of Musicians},
  volume = {13},
  issn = {1047-3211},
  abstract = {There is evidence that the cerebellum is involved in motor learning and cognitive function in humans. Animal experiments have found structural changes in the cerebellum in response to long-term motor skill activity. We investigated whether professional keyboard players, who learn specialized motor skills early in life and practice them intensely throughout life, have larger cerebellar volumes than matched non-musicians by analyzing high-resolution T(1)-weighted MR images from a large prospectively acquired database (n = 120). Significantly greater absolute (P = 0.018) and relative (P = 0.006) cerebellar volume but not total brain volume was found in male musicians compared to male non-musicians. Lifelong intensity of practice correlated with relative cerebellar volume in the male musician group (r = 0.595, P = 0.001). In the female group, there was no significant difference noted in volume measurements between musicians and non-musicians. The significant main effect for gender on relative cerebellar volume (F = 10.41, P {$<$} 0.01), with females having a larger relative cerebellar volume, may mask the effect of musicianship in the female group. We propose that the significantly greater cerebellar volume in male musicians and the positive correlation between relative cerebellar volume and lifelong intensity of practice represents structural adaptation to long-term motor and cognitive functional demands in the human cerebellum.},
  number = {9},
  journaltitle = {Cerebral Cortex (New York, N.Y.: 1991)},
  shortjournal = {Cereb. Cortex},
  date = {2003-09},
  pages = {943-949},
  keywords = {Music,Magnetic Resonance Imaging,humans,Learning,Adult,Female,Male,Cerebellum,Motor Skills,Neuronal Plasticity,Prospective Studies,Sex Characteristics},
  author = {Hutchinson, Siobhan and Lee, Leslie Hui-Lin and Gaab, Nadine and Schlaug, Gottfried},
  eprinttype = {pmid},
  eprint = {12902393}
}

@article{habibEffectsHandednessSex1991,
  langid = {english},
  title = {Effects of Handedness and Sex on the Morphology of the Corpus Callosum: A Study with Brain Magnetic Resonance Imaging},
  volume = {16},
  issn = {0278-2626},
  shorttitle = {Effects of Handedness and Sex on the Morphology of the Corpus Callosum},
  abstract = {In view of conflicting data in the existing literature, we examined 53 normal subjects using a handedness questionnaire and callosal area measurements obtained from midsagittal MRI images. The callosum was found to be significantly larger in nonconsistent right-handers (NCRH), especially in its anterior half and especially for males. A significant hand x sex interaction, favoring consistent right-handed (CRH) females, was also found for the posterior midbody, a region known to house interhemispheric fibers connecting the right and left posterior association cortices. These results (1) confirm Witelson's (1985) first findings on postmortem specimens; (2) validate a dichotomy between CRH and NCRH rather than simply considering the writing hand, as was the case in most other similar studies; and (3) suggest that at least two different sex-related--probably hormonal--factors may be acting during the callosal development, one explaining the larger anterior half in NCRH males and the other the larger posterior midbody in CRH females.},
  number = {1},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain Cogn},
  date = {1991-05},
  pages = {41-61},
  keywords = {Magnetic Resonance Imaging,humans,Adult,Middle Aged,Female,Male,Analysis of Variance,Adolescent,Surveys and Questionnaires,Sex Characteristics,Corpus Callosum,Functional Laterality},
  author = {Habib, M. and Gayraud, D. and Oliva, A. and Regis, J. and Salamon, G. and Khalil, R.},
  eprinttype = {pmid},
  eprint = {1854469}
}

@article{maguireNavigationrelatedStructuralChange2000,
  langid = {english},
  title = {Navigation-Related Structural Change in the Hippocampi of Taxi Drivers},
  volume = {97},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/97/8/4398},
  doi = {10.1073/pnas.070039597},
  abstract = {Structural MRIs of the brains of humans with extensive navigation experience, licensed London taxi drivers, were analyzed and compared with those of control subjects who did not drive taxis. The posterior hippocampi of taxi drivers were significantly larger relative to those of control subjects. A more anterior hippocampal region was larger in control subjects than in taxi drivers. Hippocampal volume correlated with the amount of time spent as a taxi driver (positively in the posterior and negatively in the anterior hippocampus). These data are in accordance with the idea that the posterior hippocampus stores a spatial representation of the environment and can expand regionally to accommodate elaboration of this representation in people with a high dependence on navigational skills. It seems that there is a capacity for local plastic change in the structure of the healthy adult human brain in response to environmental demands.},
  number = {8},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2016-06-07},
  date = {2000-11-04},
  pages = {4398-4403},
  author = {Maguire, Eleanor A. and Gadian, David G. and Johnsrude, Ingrid S. and Good, Catriona D. and Ashburner, John and Frackowiak, Richard S. J. and Frith, Christopher D.},
  file = {D\:\\Sauve\\Zotero\\storage\\6RRTFB4C\\Maguire et al. - 2000 - Navigation-related structural change in the hippoc.pdf;D\:\\Sauve\\Zotero\\storage\\ENBV524R\\4398.html},
  eprinttype = {pmid},
  eprint = {10716738}
}

@article{hansenPredictiveUncertaintyAuditory2014,
  title = {Predictive Uncertainty in Auditory Sequence Processing},
  volume = {5},
  issn = {1664-1078},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.01052/abstract},
  doi = {10.3389/fpsyg.2014.01052},
  number = {1052},
  journaltitle = {Frontiers in Psychology},
  urldate = {2016-06-07},
  date = {2014-09-23},
  author = {Hansen, Niels Chr. and Pearce, Marcus T.}
}

@article{schlaugIncreasedCorpusCallosum1995,
  langid = {english},
  title = {Increased Corpus Callosum Size in Musicians},
  volume = {33},
  issn = {0028-3932},
  abstract = {Using in-vivo magnetic resonance morphometry it was investigated whether the midsagittal area of the corpus callosum (CC) would differ between 30 professional musicians and 30 age-, sex- and handedness-matched controls. Our analyses revealed that the anterior half of the CC was significantly larger in musicians. This difference was due to the larger anterior CC in the subgroup of musicians who had begun musical training before the age of 7. Since anatomic studies have provided evidence for a positive correlation between midsagittal callosal size and the number of fibers crossing through the CC, these data indicate a difference in interhemispheric communication and possibly in hemispheric (a)symmetry of sensorimotor areas. Our results are also compatible with plastic changes of components of the CC during a maturation period within the first decade of human life, similar to those observed in animal studies.},
  number = {8},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  date = {1995-08},
  pages = {1047-1055},
  keywords = {Music,Magnetic Resonance Imaging,humans,Adult,Female,Male,Reference Values,Dominance; Cerebral,Motor Skills,Neuronal Plasticity,Corpus Callosum,Functional Laterality,Motor Cortex},
  author = {Schlaug, G. and Jäncke, L. and Huang, Y. and Staiger, J. F. and Steinmetz, H.},
  eprinttype = {pmid},
  eprint = {8524453}
}

@article{trainorOriginsMusicAuditory2015,
  langid = {english},
  title = {The Origins of Music in Auditory Scene Analysis and the Roles of Evolution and Culture in Musical Creation},
  volume = {370},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/content/370/1664/20140089},
  doi = {10.1098/rstb.2014.0089},
  abstract = {Whether music was an evolutionary adaptation that conferred survival advantages or a cultural creation has generated much debate. Consistent with an evolutionary hypothesis, music is unique to humans, emerges early in development and is universal across societies. However, the adaptive benefit of music is far from obvious. Music is highly flexible, generative and changes rapidly over time, consistent with a cultural creation hypothesis. In this paper, it is proposed that much of musical pitch and timing structure adapted to preexisting features of auditory processing that evolved for auditory scene analysis (ASA). Thus, music may have emerged initially as a cultural creation made possible by preexisting adaptations for ASA. However, some aspects of music, such as its emotional and social power, may have subsequently proved beneficial for survival and led to adaptations that enhanced musical behaviour. Ontogenetic and phylogenetic evidence is considered in this regard. In particular, enhanced auditory–motor pathways in humans that enable movement entrainment to music and consequent increases in social cohesion, and pathways enabling music to affect reward centres in the brain should be investigated as possible musical adaptations. It is concluded that the origins of music are complex and probably involved exaptation, cultural creation and evolutionary adaptation.},
  number = {1664},
  journaltitle = {Phil. Trans. R. Soc. B},
  shortjournal = {Phil. Trans. R. Soc. B},
  urldate = {2016-06-06},
  date = {2015-03-19},
  pages = {20140089},
  author = {Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\GD72XF6M\\Trainor - 2015 - The origins of music in auditory scene analysis an.pdf;D\:\\Sauve\\Zotero\\storage\\AGAUHDM7\\20140089.html},
  eprinttype = {pmid},
  eprint = {25646512}
}

@article{snyderGammabandActivityReflects2005,
  title = {Gamma-Band Activity Reflects the Metric Structure of Rhythmic Tone Sequences},
  volume = {24},
  issn = {0926-6410},
  url = {http://www.sciencedirect.com/science/article/pii/S0926641005000054},
  doi = {10.1016/j.cogbrainres.2004.12.014},
  abstract = {Relatively little is known about the dynamics of auditory cortical rhythm processing using non-invasive methods, partly because resolving responses to events in patterns is difficult using long-latency auditory neuroelectric responses. We studied the relationship between short-latency gamma-band (20–60 Hz) activity (GBA) and the structure of rhythmic tone sequences. We show that induced (non-phase-locked) GBA predicts tone onsets and persists when expected tones are omitted. Evoked (phase-locked) GBA occurs in response to tone onsets with ∼50 ms latency, and is strongly diminished during tone omissions. These properties of auditory GBA correspond with perception of meter in acoustic sequences and provide evidence for the dynamic allocation of attention to temporally structured auditory sequences.},
  number = {1},
  journaltitle = {Cognitive Brain Research},
  shortjournal = {Cognitive Brain Research},
  urldate = {2016-06-06},
  date = {2005-06},
  pages = {117-126},
  keywords = {Music,electroencephalography,speech,rhythm perception,Gamma-band activity},
  author = {Snyder, Joel S. and Large, Edward W.},
  file = {D\:\\Sauve\\Zotero\\storage\\CGA8GFST\\S0926641005000054.html}
}

@article{bhattacharyaPhaseSynchronyAnalysis2005,
  title = {Phase Synchrony Analysis of {{EEG}} during Music Perception Reveals Changes in Functional Connectivity Due to Musical Expertise},
  volume = {85},
  issn = {0165-1684},
  url = {http://www.sciencedirect.com/science/article/pii/S0165168405002070},
  doi = {10.1016/j.sigpro.2005.07.007},
  abstract = {Differences in functional and topographical connectivity patterns between two groups, musicians and non-musicians, during attentively listening to three different pieces of music and to a text of neutral content, were presented by means of EEG phase synchrony analysis in five standard frequency bands: delta (\&lt;4 Hz), theta (4–8 Hz), alpha (8–13 Hz), beta (13–30 Hz), and gamma (\&gt;30 Hz). The degree of phase synchrony or phase coherence between EEG signals was measured by a recently developed index, which is more suitable than classical indices, like correlation or coherence, when dealing with nonlinear and nonstationary signals like EEG. Comparing the music listening task to rest or control condition, musicians showed increase in phase synchrony over distributed cortical areas, both near and distant, in delta, and most conspicuously in gamma frequency band, whereas non-musicians showed enhancement only in delta band. Further, the degree of phase synchrony in musicians was reduced during listening to text as compared to listening to music. Comparing the two groups during the listening tasks, the clear-cut difference was found in gamma band phase synchrony, which was significantly stronger in musicians while listening to every chosen piece of music, yet no large difference between these two groups was found while listening to the chosen text. Musicians also showed stronger higher order inter-frequency phase synchrony between delta band oscillations in anterior regions and gamma band oscillations in posterior regions. In addition, consistent left hemispheric dominance, in terms of the strength of phase synchrony, was observed in musicians while listening to music, whereas right hemispheric dominance was observed in non-musicians. These results suggest that professional training in music is able to elicit context-sensitive functional connectivity between multiple cortical regions resulting in different listening strategies to music.},
  number = {11},
  journaltitle = {Signal Processing},
  shortjournal = {Signal Processing},
  series = {Neuronal {{Coordination}} in the {{Brain}}: {{A Signal Processing Perspective}}},
  urldate = {2016-06-06},
  date = {2005-11},
  pages = {2161-2177},
  keywords = {Music,EEG,Oscillations,Cortical Network,Functional Connectivity,Phase Synchrony},
  author = {Bhattacharya, Joydeep and Petsche, Hellmuth},
  file = {D\:\\Sauve\\Zotero\\storage\\DH52MV4D\\S0165168405002070.html}
}

@article{bidelmanNeuralCorrelatesConsonance2009,
  langid = {english},
  title = {Neural Correlates of Consonance, Dissonance, and the Hierarchy of Musical Pitch in the Human Brainstem},
  volume = {29},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.3900-09.2009},
  abstract = {Consonant and dissonant pitch relationships in music provide the foundation of melody and harmony, the building blocks of Western tonal music. We hypothesized that phase-locked neural activity within the brainstem may preserve information relevant to these important perceptual attributes of music. To this end, we measured brainstem frequency-following responses (FFRs) from nonmusicians in response to the dichotic presentation of nine musical intervals that varied in their degree of consonance and dissonance. Neural pitch salience was computed for each response using temporally based autocorrelation and harmonic pitch sieve analyses. Brainstem responses to consonant intervals were more robust and yielded stronger pitch salience than those to dissonant intervals. In addition, the ordering of neural pitch salience across musical intervals followed the hierarchical arrangement of pitch stipulated by Western music theory. Finally, pitch salience derived from neural data showed high correspondence with behavioral consonance judgments (r = 0.81). These results suggest that brainstem neural mechanisms mediating pitch processing show preferential encoding of consonant musical relationships and, furthermore, preserve the hierarchical pitch relationships found in music, even for individuals without formal musical training. We infer that the basic pitch relationships governing music may be rooted in low-level sensory processing and that an encoding scheme that favors consonant pitch relationships may be one reason why such intervals are preferred behaviorally.},
  number = {42},
  journaltitle = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  shortjournal = {J. Neurosci.},
  date = {2009-10-21},
  pages = {13165-13171},
  keywords = {Music,pitch perception,electroencephalography,Acoustic Stimulation,Auditory Pathways,humans,Adult,Female,Male,Psycholinguistics,Time Factors,Analysis of Variance,Young Adult,SOUND,REACTION time,Brain Mapping,Evoked Potentials; Auditory,Brain Stem,Statistics as Topic},
  author = {Bidelman, Gavin M. and Krishnan, Ananthanarayan},
  eprinttype = {pmid},
  eprint = {19846704},
  pmcid = {PMC2804402}
}

@article{straitMusicalTrainingEarly2012,
  langid = {english},
  title = {Musical Training during Early Childhood Enhances the Neural Encoding of Speech in Noise},
  volume = {123},
  issn = {1090-2155},
  doi = {10.1016/j.bandl.2012.09.001},
  abstract = {For children, learning often occurs in the presence of background noise. As such, there is growing desire to improve a child's access to a target signal in noise. Given adult musicians' perceptual and neural speech-in-noise enhancements, we asked whether similar effects are present in musically-trained children. We assessed the perception and subcortical processing of speech in noise and related cognitive abilities in musician and nonmusician children that were matched for a variety of overarching factors. Outcomes reveal that musicians' advantages for processing speech in noise are present during pivotal developmental years. Supported by correlations between auditory working memory and attention and auditory brainstem response properties, we propose that musicians' perceptual and neural enhancements are driven in a top-down manner by strengthened cognitive abilities with training. Our results may be considered by professionals involved in the remediation of language-based learning deficits, which are often characterized by poor speech perception in noise.},
  number = {3},
  journaltitle = {Brain and Language},
  shortjournal = {Brain Lang},
  date = {2012-12},
  pages = {191-201},
  keywords = {Attention,Music,humans,Learning,Female,Male,Speech Perception,Memory; Short-Term,Adolescent,Child,Noise,Brain Stem,Evoked Potentials; Auditory; Brain Stem},
  author = {Strait, Dana L. and Parbery-Clark, Alexandra and Hittner, Emily and Kraus, Nina},
  eprinttype = {pmid},
  eprint = {23102977},
  pmcid = {PMC3502676}
}

@article{skoeMusicalTrainingHeightens2013,
  title = {Musical Training Heightens Auditory Brainstem Function during Sensitive Periods in Development},
  volume = {4},
  issn = {1664-1078},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3777166/},
  doi = {10.3389/fpsyg.2013.00622},
  abstract = {Experience has a profound influence on how sound is processed in the brain. Yet little is known about how enriched experiences interact with developmental processes to shape neural processing of sound. We examine this question as part of a large cross-sectional study of auditory brainstem development involving more than 700 participants, 213 of whom were classified as musicians. We hypothesized that experience-dependent processes piggyback on developmental processes, resulting in a waxing-and-waning effect of experience that tracks with the undulating developmental baseline. This hypothesis led to the prediction that experience-dependent plasticity would be amplified during periods when developmental changes are underway (i.e., early and later in life) and that the peak in experience-dependent plasticity would coincide with the developmental apex for each subcomponent of the auditory brainstem response (ABR). Consistent with our predictions, we reveal that musicians have heightened response features at distinctive times in the life span that coincide with periods of developmental change. The effect of musicianship is also quite specific: we find that only select components of auditory brainstem activity are affected, with musicians having heightened function for onset latency, high-frequency phase-locking, and response consistency, and with little effect observed for other measures, including lower-frequency phase-locking and non-stimulus-related activity. By showing that musicianship imparts a neural signature that is especially evident during childhood and old age, our findings reinforce the idea that the nervous system's response to sound is “chiseled” by how a person interacts with his specific auditory environment, with the effect of the environment wielding its greatest influence during certain privileged windows of development.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  urldate = {2016-06-06},
  date = {2013-09-19},
  author = {Skoe, Erika and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\U66BQ756\\Skoe and Kraus - 2013 - Musical training heightens auditory brainstem func.pdf},
  eprinttype = {pmid},
  eprint = {24065935},
  pmcid = {PMC3777166}
}

@article{straitPlayingMusicSmarter2011,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2011.29.2.133},
  title = {Playing {{Music}} for a {{Smarter Ear}}: {{Cognitive}}, {{Perceptual}} and {{Neurobiological Evidence}}},
  volume = {29},
  issn = {0730-7829},
  doi = {10.1525/mp.2011.29.2.133},
  shorttitle = {Playing {{Music}} for a {{Smarter Ear}}},
  abstract = {Human hearing depends on a combination of cognitive and sensory processes that function by means of an interactive circuitry of bottom-up and top-down neural pathways, extending from the cochlea to the cortex and back again. Given that similar neural pathways are recruited to process sounds related to both music and language, it is not surprising that the auditory expertise gained over years of consistent music practice fine-tunes the human auditory system in a comprehensive fashion, strengthening neurobiological and cognitive underpinnings of both music and speech processing. In this review we argue not only that common neural mechanisms for speech and music exist, but that experience in music leads to enhancements in sensory and cognitive contributors to speech processing. Of specific interest is the potential for music training to bolster neural mechanisms that undergird language-related skills, such as reading and hearing speech in background noise, which are critical to academic progress, emotional health, and vocational success.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2011-12-01},
  pages = {133-146},
  keywords = {musical training},
  author = {Strait, Dana and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\RUC87CBP\\Strait and Kraus - 2011 - Playing Music for a Smarter Ear Cognitive, Percep.pdf}
}

@article{chabrisPreludeRequiemMozart1999,
  langid = {english},
  title = {Prelude or Requiem for the ‘{{Mozart}} Effect’?},
  volume = {400},
  issn = {0028-0836},
  url = {http://www.nature.com/nature/journal/v400/n6747/full/400826a0.html},
  doi = {10.1038/23608},
  abstract = {Rauscher et al. reported that listening to ten minutes of Mozart's music increased the abstract reasoning ability of college students, as measured by IQ scores, by 8 or 9 points compared with listening to relaxation instructions or silence, respectively. This startling finding became known as the ‘Mozart effect’, and has since been explored by several research groups. Here I use a meta-analysis to demonstrate that any cognitive enhancement is small and does not reflect any change in IQ or reasoning ability in general, but instead derives entirely from performance on one specific type of cognitive task and has a simple neuropsychological explanation.},
  number = {6747},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2016-06-06},
  date = {1999-08-26},
  pages = {826-827},
  author = {Chabris, Christopher F.},
  file = {D\:\\Sauve\\Zotero\\storage\\RGHTGE3C\\400826a0.html}
}

@article{pietschnigMozartEffectShmozart2010,
  title = {Mozart Effect–{{Shmozart}} Effect: {{A}} Meta-Analysis},
  volume = {38},
  issn = {0160-2896},
  url = {http://www.sciencedirect.com/science/article/pii/S0160289610000267},
  doi = {10.1016/j.intell.2010.03.001},
  shorttitle = {Mozart Effect–{{Shmozart}} Effect},
  abstract = {The transient enhancement of performance on spatial tasks in standardized tests after exposure to the first movement “allegro con spirito” of the Mozart sonata for two pianos in D major (KV 448) is referred to as the Mozart effect since its first observation by Rauscher, Shaw, and Ky (1993). These findings turned out to be amazingly hard to replicate, thus leading to an abundance of conflicting results. Sixteen years after initial publication we conduct the so far largest, most comprehensive, and up-to-date meta-analysis (nearly 40 studies, over 3000 subjects), including a diversity of unpublished research papers to finally clarify the scientific record about whether or not a specific Mozart effect exists. We could show that the overall estimated effect is small in size (d = 0.37, 95\% CI [0.23, 0.52]) for samples exposed to the Mozart sonata KV 448 and samples that had been exposed to a non-musical stimulus or no stimulus at all preceding spatial task performance. Additionally, calculation of effect sizes for samples exposed to any other musical stimulus and samples exposed to a non-musical stimulus or no stimulus at all yielded effects similar in strength (d = 0.38, 95\% CI [0.13, 0.63]), whereas there was a negligible effect between the two music conditions (d = 0.15, 95\% CI [0.02, 0.28]). Furthermore, formal tests yielded evidence for confounding publication bias, requiring downward correction of effects. The central finding of the present paper however, is certainly the noticeably higher overall effect in studies performed by Rauscher and colleagues than in studies performed by other researchers, indicating systematically moderating effects of lab affiliation. On the whole, there is little evidence left for a specific, performance-enhancing Mozart effect.},
  number = {3},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  urldate = {2016-06-06},
  date = {2010-05},
  pages = {314-323},
  keywords = {meta-analysis,Mozart effect,Publication bias,Spatial ability},
  author = {Pietschnig, Jakob and Voracek, Martin and Formann, Anton K.},
  file = {D\:\\Sauve\\Zotero\\storage\\JK5UPTFJ\\S0160289610000267.html}
}

@article{rauscherListeningMozartEnhances1995,
  title = {Listening to {{Mozart}} Enhances Spatial-Temporal Reasoning: Towards a Neurophysiological Basis},
  volume = {185},
  issn = {0304-3940},
  url = {http://www.sciencedirect.com/science/article/pii/0304394094112214},
  doi = {10.1016/0304-3940(94)11221-4},
  shorttitle = {Listening to {{Mozart}} Enhances Spatial-Temporal Reasoning},
  abstract = {Motivated by predictions of a structured neuronal model of the cortex, we performed a behavioral experiment which showed that listening to a Mozart piano sonata produced significant short-term enhancement of spatial-temporal reasoning in college students. Here we present results from an experiment which replicates these findings, and shows that (i) ‘repetitive’ music does not enhance reasoning; (ii) a taped short story does not enhance reasoning; and (iii) short-term memory is not enhanced. We propose experiments designed to explore the neurophysiological bases of this causal enhancement of spatial-temporal reasoning by music, and begin to search for quantitative measures of further higher ognitive effects of music.},
  number = {1},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  urldate = {2016-06-06},
  date = {1995-02-06},
  pages = {44-47},
  keywords = {Music,EEG,Analytic reasoning,Creative reasoning,Spatial-temporal neuronal pattern development,Trion model of cortex},
  author = {Rauscher, Frances H. and Shaw, Gordon L. and Ky, Katherine N.},
  file = {D\:\\Sauve\\Zotero\\storage\\IDKC6H7U\\0304394094112214.html}
}

@article{zacksEventPerceptionMindbrain2007,
  title = {Event Perception: A Mind-Brain Perspective.},
  volume = {133},
  url = {http://psycnet.apa.org/psycinfo/2007-02367-005},
  shorttitle = {Event Perception},
  number = {2},
  journaltitle = {Psychological bulletin},
  urldate = {2016-04-27},
  date = {2007},
  pages = {273},
  author = {Zacks, Jeffrey M. and Speer, Nicole K. and Swallow, Khena M. and Braver, Todd S. and Reynolds, Jeremy R.},
  file = {D\:\\Sauve\\Zotero\\storage\\KRT2PT7T\\PMC2852534.html}
}

@article{hoshi-shibaNeuralCorrelatesExpectation2014,
  langid = {english},
  title = {Neural Correlates of Expectation of Musical Termination Structure or Cadence:},
  volume = {25},
  issn = {0959-4965},
  url = {http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage&an=00001756-201407090-00003},
  doi = {10.1097/WNR.0000000000000160},
  shorttitle = {Neural Correlates of Expectation of Musical Termination Structure or Cadence},
  number = {10},
  journaltitle = {NeuroReport},
  urldate = {2016-04-27},
  date = {2014-07},
  pages = {743-748},
  author = {Hoshi-Shiba, Reiko and Furukawa, Kiyoshi and Okanoya, Kazuo}
}

@article{bogelsProsodicBreaksSentence2011,
  langid = {english},
  title = {Prosodic {{Breaks}} in {{Sentence Processing Investigated}} by {{Event}}-{{Related Potentials}}},
  volume = {5},
  issn = {1749-818X},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1749-818X.2011.00291.x/abstract},
  doi = {10.1111/j.1749-818X.2011.00291.x},
  abstract = {Prosodic breaks (PBs) can indicate a sentence’s syntactic structure. Event-related brain potentials (ERPs) are an excellent way to study auditory sentence processing, since they provide an on-line measure across a complete sentence, in contrast to other on- and off-line methods. ERPs for the first time allowed investigating the processing of a PB itself. PBs reliably elicit a closure positive shift (CPS). We first review several studies on the CPS, leading to the conclusion that it is elicited by abstract structuring or phrasing of the input. Then we review ERP findings concerning the role of PBs in sentence processing as indicated by ERP components like the N400, P600 and LAN. We focus on whether and how PBs can (help to) disambiguate locally ambiguous sentences. Differences in results between different studies can be related to differences in items, initial parsing preferences and tasks. Finally, directions for future research are discussed.},
  number = {7},
  journaltitle = {Language and Linguistics Compass},
  urldate = {2016-04-25},
  date = {2011-07-01},
  pages = {424-440},
  author = {Bögels, Sara and Schriefers, Herbert and Vonk, Wietske and Chwilla, Dorothee J.},
  file = {D\:\\Sauve\\Zotero\\storage\\8MVNESZQ\\abstract\;jsessionid=B92A82046B858B8F17F72E7CA027891C.html}
}

@article{margulisSilencesMusicAre2007,
  langid = {english},
  title = {Silences in {{Music}} Are {{Musical Not Silent}}: {{An Exploratory Study}} of {{Context Effects}} on the {{Experience}} of {{Musical Pauses}}},
  volume = {24},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/24/5/485},
  doi = {10.1525/mp.2007.24.5.485},
  shorttitle = {Silences in {{Music}} Are {{Musical Not Silent}}},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-04-25},
  date = {2007-06-01},
  pages = {485-506},
  author = {Margulis, Elizabeth Hellmuth},
  file = {D\:\\Sauve\\Zotero\\storage\\JFBJARVZ\\485.html}
}

@article{nanPerceptionMusicalPhrase2006,
  title = {The Perception of Musical Phrase Structure: {{A}} Cross-Cultural {{ERP}} Study},
  volume = {1094},
  issn = {0006-8993},
  url = {http://www.sciencedirect.com/science/article/pii/S0006899306009097},
  doi = {10.1016/j.brainres.2006.03.115},
  shorttitle = {The Perception of Musical Phrase Structure},
  abstract = {Electroencephalography (EEG) was used in a cross-cultural music study investigating phrase boundary perception. Chinese and German musicians performed a cultural categorization task under Chinese and Western music listening conditions. Western music was the major subject for both groups of musicians, while Chinese music was familiar to Chinese subjects only. By manipulating the presence of pauses between two phrases in the biphrasal melodies, EEG correlates for the perception of phrase boundaries were found in both groups under both music listening conditions. Between 450 and 600~ms, the music CPS (closure positive shift), which had been found in earlier studies with a false tone detection task, was replicated for the more global categorization task and for all combinations of subject group and musical style. At short latencies (100 and 450~ms post phrase boundary offset), EEG correlates varied as a function of musical styles and subject group. Both bottom–up (style properties of the music) and top–down (acculturation of the subjects) information interacted during this early processing stage.},
  number = {1},
  journaltitle = {Brain Research},
  shortjournal = {Brain Research},
  urldate = {2016-04-25},
  date = {2006-06-13},
  pages = {179-191},
  keywords = {music perception,Event-related potential,Cultural difference,Phrase structure},
  author = {Nan, Yun and Knösche, Thomas R. and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\B9I2NV8E\\S0006899306009097.html}
}

@article{nanCrossculturalMusicPhrase2008,
  langid = {english},
  title = {Cross-Cultural Music Phrase Processing: {{An fMRI}} Study},
  volume = {29},
  doi = {10.1002/hbm.20390},
  shorttitle = {Cross-Cultural Music Phrase Processing},
  number = {3},
  journaltitle = {Human Brain Mapping},
  date = {2008},
  pages = {312--328},
  author = {Nan, Yun and Knösche, Thomas R. and Zysset, Stefan and Friederici, Angela D.}
}

@article{sridharanNeuralDynamicsEvent2007,
  title = {Neural {{Dynamics}} of {{Event Segmentation}} in {{Music}}: {{Converging Evidence}} for {{Dissociable Ventral}} and {{Dorsal Networks}}},
  volume = {55},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627307005004},
  doi = {10.1016/j.neuron.2007.07.003},
  shorttitle = {Neural {{Dynamics}} of {{Event Segmentation}} in {{Music}}},
  abstract = {Summary
The real world presents our sensory systems with a continuous stream of undifferentiated information. Segmentation of this stream at event boundaries is necessary for object identification~and feature extraction. Here, we investigate the neural dynamics of event segmentation in entire musical symphonies under natural listening conditions. We isolated time-dependent sequences of brain responses in a 10 s window surrounding transitions between movements of~symphonic works. A strikingly right-lateralized network of brain regions showed peak response during the movement transitions~when, paradoxically, there was no physical stimulus. Model-dependent and model-free analysis techniques provided converging evidence for activity in two distinct functional networks at the movement transition: a ventral fronto-temporal network associated with detecting salient events, followed in time by a dorsal fronto-parietal network associated with maintaining attention and updating working memory. Our study provides direct experimental evidence for dissociable and causally linked ventral and dorsal networks during event segmentation of ecologically valid auditory stimuli.},
  number = {3},
  journaltitle = {Neuron},
  shortjournal = {Neuron},
  urldate = {2016-04-25},
  date = {2007-08-02},
  pages = {521-532},
  keywords = {SYSNEURO},
  author = {Sridharan, Devarajan and Levitin, Daniel J. and Chafe, Chris H. and Berger, Jonathan and Menon, Vinod},
  file = {D\:\\Sauve\\Zotero\\storage\\RJ5JQARJ\\Sridharan et al. - 2007 - Neural Dynamics of Event Segmentation in Music Co.pdf;D\:\\Sauve\\Zotero\\storage\\J4B5HBCZ\\S0896627307005004.html}
}

@article{mcdermottRelativePitchSpecific2008,
  langid = {english},
  title = {Is {{Relative Pitch Specific}} to {{Pitch}}?},
  volume = {19},
  issn = {0956-7976, 1467-9280},
  url = {http://pss.sagepub.com/content/19/12/1263},
  doi = {10.1111/j.1467-9280.2008.02235.x},
  abstract = {Melodies, speech, and other stimuli that vary in pitch are processed largely in terms of the relative pitch differences between sounds. Relative representations permit recognition of pitch patterns despite variations in overall pitch level between instruments or speakers. A key component of relative pitch is the sequence of pitch increases and decreases from note to note, known as the melodic contour. Here we report that contour representations are also produced by patterns in loudness and brightness (an aspect of timbre). The representations of contours in different dimensions evidently have much in common, as contours in one dimension can be readily recognized in other dimensions. Moreover, contours in loudness and brightness are nearly as useful as pitch contours for recognizing familiar melodies that are normally conveyed via pitch. Our results indicate that relative representations via contour extraction are a general feature of the auditory system, and may have a common central locus.},
  number = {12},
  journaltitle = {Psychological Science},
  shortjournal = {Psychological Science},
  urldate = {2016-04-25},
  date = {2008-12-01},
  pages = {1263-1271},
  author = {McDermott, Josh H. and Lehr, Andriana J. and Oxenham, Andrew J.},
  file = {D\:\\Sauve\\Zotero\\storage\\JWSKK2HV\\PMC2841133.html;D\:\\Sauve\\Zotero\\storage\\RNMN98NH\\j.1467-9280.2008.02235.html;D\:\\Sauve\\Zotero\\storage\\ZNF7PNCC\\1263.html},
  eprinttype = {pmid},
  eprint = {19121136}
}

@article{wertheimerUntersuchungenZurLehre1923,
  title = {Untersuchungen Zur {{Lehre}} von Der {{Gestalt}}, {{II}} [{{Investigations}} in {{Gestalt Theory}}: {{II}}. {{Laws}} of Organization in Perceptual Forms]},
  volume = {4},
  journaltitle = {Psychologische Forschung},
  date = {1923},
  pages = {301-350},
  author = {Wertheimer, M}
}

@article{huronMelodicArchWestern1996,
  title = {The Melodic Arch in {{Western}} Folksongs},
  volume = {10},
  journaltitle = {Computing in Musicology},
  date = {1996},
  pages = {3-23},
  author = {Huron, David}
}

@article{kalmanNewApproachLinear1960,
  title = {A New Approach to Linear Filtering and Prediction Problems},
  volume = {82},
  number = {1},
  journaltitle = {Journal of Basic Engineering},
  date = {1960},
  pages = {35-45},
  author = {Kalman, R. E.}
}

@article{vuustNewCenterMusic2015,
  title = {The New {{Center}} for {{Music}} in the {{Brain}} ({{MIB}}) in {{Denmark}}},
  volume = {25},
  issn = {2162-1535(Electronic);0275-3987(Print)},
  doi = {10.1037/pmu0000125},
  abstract = {We are delighted to announce a new Center for Music in the Brain (MIB), directed by Professor Peter Vuust, which has been funded by the Danish National Research Foundation (DNRF) for 6 years initially and a possible 4 years extension. The MIB Center aims to elucidate how music moves us physically and emotionally and shapes brain structure and function. In turn, music can also provide a model for empirically testing influential theories that hold prediction to be a general principle of brain function. Embedded within both an educational and clinical environment, the establishment of MIB will provide a unique international hotspot for music and neuroscience research, with high potential for translational impact.},
  number = {4},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  date = {2015},
  pages = {457-458},
  keywords = {Neurosciences,*Music,*Brain},
  author = {Vuust, Peter}
}

@article{douglasAuditorySceneAnalysis2016,
  langid = {english},
  title = {Auditory {{Scene Analysis}} and the {{Perception}} of {{Sound Mass}} in {{Ligeti}}’s {{Continuum}}},
  volume = {33},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/33/3/287},
  doi = {10.1525/mp.2016.33.3.287},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-03-10},
  date = {2016-02-01},
  pages = {287-305},
  keywords = {auditory scene analysis,Continuum,fusion,Ligeti,sound mass},
  author = {Douglas, Chelsea and Noble, Jason and McAdams, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\4PUJS8ZS\\Douglas et al. - 2016 - Auditory Scene Analysis and the Perception of Soun.pdf;D\:\\Sauve\\Zotero\\storage\\W5A8GZGH\\Douglas et al. - 2016 - Auditory Scene Analysis and the Perception of Soun.pdf;D\:\\Sauve\\Zotero\\storage\\AX4CBDDD\\287.html;D\:\\Sauve\\Zotero\\storage\\XR9XMP8X\\287.html}
}

@article{margulisModelMelodicExpectation2005,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2005.22.4.663},
  title = {A {{Model}} of {{Melodic Expectation}}},
  volume = {22},
  issn = {0730-7829},
  doi = {10.1525/mp.2005.22.4.663},
  abstract = {A model of melodic expectation is proposed. The model assigns ratings to the expectedness of melodic events. The ratings depend on the hierarchic implementation of three primary factors: stability, proximity, and direction; and one secondary factor: mobility. The model explicitly links expectancy ratings to aspects of listeners' experiences of tension in melody. An approach to temporal expectations is discussed but not quantified. The model is situated within a framework for thinking about a type of schematic melodic expectations. This article assesses the position of these expectations within the broader cognitive processes invoked in listening to music. It suggests methods for investigating the expectations empirically. Additionally, it outlines connections between the theorized expectations and the dynamic, affective contours of musical experience.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2005-06-01},
  pages = {663-714},
  author = {Margulis, Elizabeth Hellmuth},
  file = {D\:\\Sauve\\Zotero\\storage\\A6UV74SI\\Margulis - 2005 - A Model of Melodic Expectation.pdf}
}

@article{burunatActionPerceptionProminent2015,
  title = {Action in Perception: Prominent Visuo-Motor Functional Symmetry in Musicians during Music Listening},
  volume = {10},
  url = {http://dx.doi.org/10.1371/journal.pone.0138238},
  doi = {10.1371/journal.pone.0138238},
  shorttitle = {Action in {{Perception}}},
  abstract = {Musical training leads to sensory and motor neuroplastic changes in the human brain. Motivated by findings on enlarged corpus callosum in musicians and asymmetric somatomotor representation in string players, we investigated the relationship between musical training, callosal anatomy, and interhemispheric functional symmetry during music listening. Functional symmetry was increased in musicians compared to nonmusicians, and in keyboardists compared to string players. This increased functional symmetry was prominent in visual and motor brain networks. Callosal size did not significantly differ between groups except for the posterior callosum in musicians compared to nonmusicians. We conclude that the distinctive postural and kinematic symmetry in instrument playing cross-modally shapes information processing in sensory-motor cortical areas during music listening. This cross-modal plasticity suggests that motor training affects music perception.},
  number = {9},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  urldate = {2015-12-17},
  date = {2015-09-30},
  pages = {e0138238},
  author = {Burunat, Iballa and Brattico, Elvira and Puoliväli, Tuomas and Ristaniemi, Tapani and Sams, Mikko and Toiviainen, Petri},
  file = {D\:\\Sauve\\Zotero\\storage\\4RXNFTCX\\Burunat et al. - 2015 - Action in Perception Prominent Visuo-Motor Functi.pdf}
}

@article{bratticoContextEffectsPitch2001,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2001.19.2.199},
  title = {Context Effects on Pitch Perception in Musicians and Nonmusicians: Evidence from Event-Related-Potential Recordings},
  volume = {19},
  issn = {0730-7829},
  doi = {10.1525/mp.2001.19.2.199},
  shorttitle = {Context {{Effects}} on {{Pitch Perception}} in {{Musicians}} and {{Nonmusicians}}},
  abstract = {Behavioral evidence indicates that musical context facilitates pitch discrimination. In the present study, we sought to determine whether pitch context and its familiarity might affect brain responses to pitch change even at the preattentive level. Ten musicians and 10 nonmusicians, while concentrating on reading a book, were presented with sound stimuli that had an infrequent (p = 15\%) pitch shift of 144 Hz. In the familiar condition, the infrequent third-position deviant changed the mode (major vs. minor) of the five-tone pattern. In the unfamiliar condition, patterns were formed from five arithmetically determined tone frequencies, the deviant not causing any change of mode. The no-context condition included only third-position tones. All deviants elicited the change-specific mismatch negativity component of the event-related potentials in both groups of subjects. In both musicians and nonmusicians, pitch change in the familiar condition evoked larger mismatch negativity amplitude than the change in the unfamiliar condition and, correspondingly, larger mismatch negativity in the unfamiliar condition than in the no-context condition. This suggests that preattentive pitch-change processing is generally enhanced in a familiar context. Moreover, the latency of the mismatch negativity was shorter for musicians than for nonmusicians in both the familiar and unfamiliar conditions, whereas no difference between groups was observed in the no-context condition. This finding indicates that, in response to sequential structured sound events, the auditory system reacts faster in musicians than in nonmusicians.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2001-12-01},
  pages = {199-222},
  author = {Brattico, Elvira and Näätänen, Risto and Tervaniemi, Mari},
  file = {D\:\\Sauve\\Zotero\\storage\\R727P27B\\Brattico et al. - 2001 - Context Effects on Pitch Perception in Musicians a.pdf}
}

@article{wanCausalInferenceCortical2014,
  title = {The Causal Inference of Cortical Neural Networks during Music Improvisations},
  volume = {9},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0112776},
  doi = {10.1371/journal.pone.0112776},
  abstract = {We present an EEG study of two music improvisation experiments. Professional musicians with high level of improvisation skills were asked to perform music either according to notes (composed music) or in improvisation. Each piece of music was performed in two different modes: strict mode and “let-go” mode. Synchronized EEG data was measured from both musicians and listeners. We used one of the most reliable causality measures: conditional Mutual Information from Mixed Embedding (MIME), to analyze directed correlations between different EEG channels, which was combined with network theory to construct both intra-brain and cross-brain networks. Differences were identified in intra-brain neural networks between composed music and improvisation and between strict mode and “let-go” mode. Particular brain regions such as frontal, parietal and temporal regions were found to play a key role in differentiating the brain activities between different playing conditions. By comparing the level of degree centralities in intra-brain neural networks, we found a difference between the response of musicians and the listeners when comparing the different playing conditions.},
  number = {12},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2016-03-18},
  date = {2014-12-09},
  pages = {e112776},
  keywords = {Attention,electroencephalography,Functional Magnetic Resonance Imaging,Neural Networks,music cognition,Bioacoustics,Centrality,Prefrontal cortex},
  author = {Wan, Xiaogeng and Crüts, Björn and Jensen, Henrik Jeldtoft},
  file = {D\:\\Sauve\\Zotero\\storage\\II3D7CP3\\Wan et al. - 2014 - The Causal Inference of Cortical Neural Networks d.pdf;D\:\\Sauve\\Zotero\\storage\\XEAFZ72S\\article.html}
}

@inproceedings{yangFeatureExtractionBased2016,
  location = {{London, UK}},
  title = {Feature Extraction Based on Auditory Image Model for Noise-Robust Automatic Speech Recognition},
  eventtitle = {Workshop on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  booktitle = {Poster at {{Workshop}} on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  date = {2016},
  author = {Yang, X. and Karbasi, M. and Bleeck, S. and Kolossa, D.}
}

@article{zuijenGroupingSequentialSounds2004,
  title = {Grouping of Sequential Sounds—an Event-Related Potential Study Comparing Musicians and Nonmusicians},
  volume = {16},
  issn = {0898-929X},
  url = {http://dx.doi.org/10.1162/089892904322984607},
  doi = {10.1162/089892904322984607},
  abstract = {It is believed that auditory processes governing grouping and segmentation of sounds are automatic and represent universal aspects of music perception (e.g., they are independent of the listener's musical skill). The present study challenges this view by showing that musicians and nonmusicians differ in their ability to preattentively group consecutive sounds. We measured event-related potentials (ERPs) from professional musicians and nonmusicians who were presented with isochronous tone sequences that they ignored. Four consecutive tones in a sequence could be grouped according to either pitch similarity or good continuation of pitch. Occasionally, the tone-group length was violated by a deviant tone. The mismatch negativity (MMN) was elicited to the deviants in both subject groups when the sounds could be grouped based on pitch similarity. In contrast, MMN was only elicited in musicians when the sounds could be grouped according to good continuation of pitch. These results suggest that some forms of auditory grouping depend on musical skill and that not all aspects of auditory grouping are universal.},
  number = {2},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  urldate = {2016-03-14},
  date = {2004-03-01},
  pages = {331-338},
  author = {van Zuijen, Titia L. and Sussman, Elyse and Winkler, István and Näätänen, Risto and Tervaniemi, Mari},
  file = {D\:\\Sauve\\Zotero\\storage\\ERCHTXMK\\089892904322984607.html}
}

@book{jammalamadakaTopicsCircularStatistics2001,
  langid = {english},
  title = {Topics in {{Circular Statistics}}},
  isbn = {978-981-277-926-7},
  abstract = {This research monograph on circular data analysis covers some recent advances in the field, besides providing a brief introduction to, and a review of, existing methods and models. The primary focus is on recent research into topics such as change-point problems, predictive distributions, circular correlation and regression, etc. An important feature of this work is the S-plus subroutines provided for analyzing actual data sets. Coupled with the discussion of new theoretical research, the book should benefit both the researcher and the practitioner. Contents: Circular Probability Distributions; Some Sampling Distributions; Estimation of Parameters; Tests for Mean Direction and Concentration; Tests for Uniformity; Nonparametric Testing Procedures; Circular Correlation and Regression; Predictive Inference for Directional Data; Outliers and Related Problems; Change-Point Problems; Miscellaneous Topics; Some Facts on Bessel Functions; How to Use the CircStats Package. Readership: Researchers and practitioners dealing with circular data.},
  pagetotal = {348},
  publisher = {{World Scientific}},
  date = {2001},
  author = {Jammalamadaka, S. Rao and Sengupta, Ambar}
}

@article{deprettoPrinciplesParsimonyFMRI2015,
  title = {Principles of Parsimony: {{fMRI}} Correlates of Beat-Based versus Duration-Based Sensorimotor Synchronization},
  volume = {25},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2015-58237-003&site=ehost-live},
  doi = {10.1037/pmu0000122},
  shorttitle = {Principles of Parsimony},
  abstract = {This study investigated functional MRI (fMRI) cerebral correlates of beat- and duration-based sensorimotor synchronization (SMS). We developed an original paradigm to compare SMS in beat-based versus duration-based contexts. In the beat-based conditions, participants synchronized finger taps with a regular beat. The condition had either metrical or nonmetrical subdivisions (2:1 vs. 2.4:1 ratio). In the duration-based conditions, participants synchronized by referring to a cue tone appearing 300 ms ahead of an irregularly occurring target tone. The behavioral results suggest the use of different strategies for beat-based and duration-based conditions. Synchronization accuracy was similar in both types of tasks. However, participants reported higher attentional demands in duration-based conditions. ICA analysis of the fMRI data isolated 2 underlying cerebral networks for all tasks, both more strongly involved in duration-based conditions. The first brain network involved the bilateral superior temporal gyrus, supplementary motor area, and inferior frontal gyrus; the left dorsal premotor cortex and primary motor cortex; and the right posterior cerebellum. The second brain network involved the bilateral basal ganglia, thalamus, inferior parietal lobules, and cerebellum. We suggest that the first network managed temporal information processing and execution of motor commands, and that the second controlled error correction processing. The involvement of the same pool of cerebral resources with different strengths according to the level of regularity of the input may represent a principle of parsimony: as beat-based SMS allows better anticipation, it requires less cerebral resources than duration-based SMS. (PsycINFO Database Record (c) 2016 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  shortjournal = {Psychomusicology: Music, Mind, and Brain},
  series = {Neurosciences and {{Music}}: {{Part}} 2},
  urldate = {2016-03-31},
  date = {2015-12},
  pages = {380-391},
  keywords = {Attention,Rhythm,Time,Functional Magnetic Resonance Imaging,timing,rhythm production,beat processing,finger tapping,Perceptual Motor Processes},
  author = {De Pretto, Michael and James, Clara E.},
  file = {D\:\\Sauve\\Zotero\\storage\\2NFWVGW8\\De Pretto and James - 2015 - Principles of parsimony fMRI correlates of beat-b.pdf}
}

@article{ellisonAffectiveCognitiveResponses2015,
  title = {Affective versus Cognitive Responses to Musical Chords: {{An ERP}} and Behavioral Study},
  volume = {25},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2015-58237-007&site=ehost-live},
  doi = {10.1037/pmu0000127},
  shorttitle = {Affective versus Cognitive Responses to Musical Chords},
  abstract = {Via electrophysiological methods, early neural responses to musical chords have been, on one hand, associated with feature encoding, feature change discrimination, rule violation processing, and conscious updating of musical expectations. On the other hand, late neural responses have been related to affective evaluation of sounds. The chronometric succession of these neural processes and their underlying psychological mechanisms related to cognitive and affective aspects of music listening have thus far remained unexplored. Here, the neural correlates of affective and cognitive processing of musical chords were contrasted by means of the event-related potential (ERP) technique and behavioral ratings. Adult subjects (N = 24) performed an emotion judgment (affective) task and a correctness judgment (cognitive) task while listening to chord sequences ending in various major and minor final chords, which were either correctly tuned or mistuned. Enhanced negative ERPs during cadence listening preceding the affective ratings, relative to the cognitive ratings, suggest different neural preparation for these tasks. Furthermore, negatively rated (sad or incorrect) cadence endings in both tasks elicited early negative ERPs and later positive ERPs. These positivities, peaking at 500 ms, differed in scalp distribution between sad and incorrect stimuli. The present findings suggest a neural chronometry of music listening in which feature encoding and sensory memory processes are followed at a medium latency by affective classification, after which an evaluative stage takes place. This study provides a first look at the chronometric succession of electrophysiological brain responses in relation to emotion judgments of musical pitch as opposed to nonaffective correctness judgments. (PsycINFO Database Record (c) 2016 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  shortjournal = {Psychomusicology: Music, Mind, and Brain},
  series = {Neurosciences and {{Music}}: {{Part}} 2},
  urldate = {2016-03-31},
  date = {2015-12},
  pages = {423-434},
  keywords = {Evoked Potentials,Music,music perception,mismatch negativity,Mismatch negativity (MMN),Human Information Storage,Cognitive Processes,Harmony,tonality,Event-related potential (ERP),early right anterior negativity (ERAN)},
  author = {Ellison, David and Moisseinen, Nella and Fachner, Jörg and Brattico, Elvira},
  file = {D\:\\Sauve\\Zotero\\storage\\46FB2HJX\\Ellison et al. - 2015 - Affective versus cognitive responses to musical ch.pdf}
}

@article{schaeferProbingNeuralMechanisms2012,
  title = {Probing Neural Mechanisms of Music Perception, Cognition, and Performance Using Multivariate Decoding},
  volume = {22},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2013-01453-009&site=ehost-live},
  doi = {10.1037/a0031014},
  abstract = {Recent neuroscience research has shown increasing use of multivariate decoding methods and machine learning. These methods, by uncovering the source and nature of informative variance in large data sets, invert the classical direction of inference that attempts to explain brain activity from mental state variables or stimulus features. However, these techniques are not yet commonly used among music researchers. In this position article, we introduce some key features of machine learning methods and review their use in the field of cognitive and behavioral neuroscience of music. We argue for the great potential of these methods in decoding multiple data types, specifically audio waveforms, electroencephalography, functional MRI, and motion capture data. By finding the most informative aspects of stimulus and performance data, hypotheses can be generated pertaining to how the brain processes incoming musical information and generates behavioral output, respectively. Importantly, these methods are also applicable to different neural and physiological data types such as magnetoencephalography, near-infrared spectroscopy, positron emission tomography, and electromyography. (PsycINFO Database Record (c) 2013 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  shortjournal = {Psychomusicology: Music, Mind, and Brain},
  series = {Neurosciences and {{Music}}},
  urldate = {2016-03-31},
  date = {2012-12},
  pages = {168-174},
  keywords = {cognition,Music,music perception,neural mechanisms,Performance,Cognitive neuroscience,Behavioral Neuroscience,classification,machine learning,music neuroscience},
  author = {Schaefer, Rebecca S. and Furuya, Shinichi and Smith, Leigh M. and Kaneshiro, Blair Bohannan and Toiviainen, Petri},
  file = {D\:\\Sauve\\Zotero\\storage\\MJ3GW5VQ\\Schaefer et al. - 2012 - Probing neural mechanisms of music perception, cog.pdf}
}

@article{sturmExtractingNeuralRepresentation2015,
  title = {Extracting the Neural Representation of Tone Onsets for Separate Voices of Ensemble Music Using Multivariate {{EEG}} Analysis},
  volume = {25},
  issn = {0275-3987},
  url = {http://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2015-50149-001&site=ehost-live},
  doi = {10.1037/pmu0000104},
  abstract = {When listening to ensemble music, even nonmusicians can follow single instruments effortlessly. Electrophysiological indices for neural sensory encoding of separate streams have been described using oddball paradigms that utilize brain reactions to sound events that deviate from a repeating standard pattern. Obviously, these paradigms put constraints on the compositional complexity of the musical stimulus. Here, we apply a regression-based method of multivariate electroencephalogram (EEG) analysis in order to reveal the neural encoding of separate voices of naturalistic ensemble music that is based on cortical responses to tone onsets, such as N1/P2 event-related potential components. Music clips (resembling minimalistic electro-pop) were presented to 11 subjects, either in an ensemble version (drums, bass, keyboard) or in the corresponding 3 solo versions. For each instrument, we train a spatiotemporal regression filter that optimizes the correlation between EEG and a target function that represents the sequence of note onsets in the audio signal of the respective solo voice. This filter extracts an EEG projection that reflects the brain’s reaction to note onsets with enhanced sensitivity. We apply these instrument-specific filters to 61-channel EEG recorded during the presentations of the ensemble version and assess by means of correlation measures how strongly the voice of each solo instrument is reflected in the EEG. Our results show that the reflection of the melody instrument keyboard in the EEG exceeds that of the other instruments by far, suggesting a high-voice superiority effect in the neural representation of note onsets. Moreover, the results indicated that focusing attention on a particular instrument can enhance this reflection. We conclude that the voice-discriminating neural representation of tone onsets at the level of early auditory event-related potentials parallels the perceptual segregation of multivoiced music. (PsycINFO Database Record (c) 2016 APA, all rights reserved). (journal abstract)},
  number = {4},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  shortjournal = {Psychomusicology: Music, Mind, and Brain},
  series = {Neurosciences and {{Music}}: {{Part}} 2},
  urldate = {2016-03-31},
  date = {2015-12},
  pages = {366-379},
  keywords = {Auditory Perception,auditory stream segregation,Pitch (Frequency),music perception,Auditory Evoked Potentials,electroencephalography,EEG,event-related potentials,naturalistic music},
  author = {Sturm, Irene and Treder, Matthias and Miklody, Daniel and Purwins, Hendrik and Dähne, Sven and Blankertz, Benjamin and Curio, Gabriel},
  file = {D\:\\Sauve\\Zotero\\storage\\N9GMX5I3\\Sturm et al. - 2015 - Extracting the neural representation of tone onset.pdf}
}

@article{aucouturierCovertDigitalManipulation2016,
  title = {Covert Digital Manipulation of Vocal Emotion Alter Speakers' Emotional States in a Congruent Direction},
  volume = {113},
  abstract = {Research has shown that people often exert control over their emotions. By modulating expressions, reappraising feelings, and redirecting attention, they can regulate their emotional experience. These findings have contributed to a blurring of the traditional boundaries between cognitive and emotional processes, and it has been suggested that emotional signals are produced in a goal-directed way and monitored for errors like other intentional actions. However, this interesting possibility has never been experimentally tested. To this end, we created a digital audio platform to covertly modify the emotional tone of participants' voices while they talked in the direction of happiness, sadness, or fear. The result showed that the audio transformations were being perceived as natural examples of the intended emotions, but the great majority of the participants, nevertheless, remained unaware that their own voices were being manipulated. This finding indicates that people are not continuously monitoring their own voice to make sure that it meets a predetermined emotional target. Instead, as a consequence of listening to their altered voices, the emotional state of the participants changed in congruence with the emotion portrayed, which was measured by both self-report and skin conductance level. This change is the first evidence, to our knowledge, of peripheral feedback effects on emotional experience in the auditory domain. As such, our result reinforces the wider framework of self-perception theory: that we often use the same inferential strategies to understand ourselves as those that we use to understand others.},
  number = {4},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2016},
  pages = {948-53},
  author = {Aucouturier, JJ and Johansson, P and Hall, L and Mercadié, L and Watanabe, K}
}

@book{wangComputationalAuditoryScene2006,
  title = {Computational Auditory Scene Analysis: {{Principles}}, Algorithms, and Applications},
  publisher = {{Wiley-IEEE Press}},
  date = {2006},
  author = {Wang, D. and Brown, G. J.}
}

@article{stefanlattnerPseudoSupervisedTrainingImproves2015,
  title = {Pseudo-{{Supervised Training Improves Unsupervised Melody Segmentation}}},
  date = {2015},
  author = {Stefan Lattner, Carlos Eduardo Cancino},
  file = {D\:\\Sauve\\Zotero\\storage\\KA9NN3V7\\275830951_Pseudo-Supervised_Training_Improves_Unsupervised_Melody_Segmentation.html}
}

@article{hutchisonMindingGapExperimental2015,
  langid = {english},
  title = {Minding the Gap: {{An}} Experimental Assessment of Musical Segmentation Models.},
  volume = {25},
  issn = {2162-1535, 0275-3987},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/pmu0000085},
  doi = {10.1037/pmu0000085},
  shorttitle = {Minding the Gap},
  number = {2},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  urldate = {2016-03-22},
  date = {2015},
  pages = {103-115},
  author = {Hutchison, Joanna L. and Hubbard, Timothy L. and Hubbard, Nicholas A. and Brigante, Ryan and Rypma, Bart}
}

@article{deanGenerativeStructuresImprovisation2014,
  title = {Generative {{Structures}} in {{Improvisation}}: {{Computational Segmentation}} of {{Keyboard Performances}}},
  volume = {43},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298215.2013.859710},
  doi = {10.1080/09298215.2013.859710},
  shorttitle = {Generative {{Structures}} in {{Improvisation}}},
  abstract = {We assessed the hypothesis elaborated by Pressing and by the first author that improvisation is normally based on the generation of segmental contrasts in musical features. Nine experienced improvisers performed a series of free- and 3-section referent-based pieces, each of about three minutes. Each player undertook eight improvisations with ‘referent’ instructions, preceded and followed by a free improvisation. A MIDI-equipped grand piano was used, and audio and MIDI were recorded. Computational analyses of the MIDI data assessed whether performers realized the referents successfully, and then determined the large-scale segmentation of the free improvisations. The performers almost always fulfilled the referents (68/72 improvisations, p {$<$} 0.0001), and their free improvisations were also susceptible to large-scale segmentation (p {$<$} 0.005). Since performers were only told of the referent structures as the session proceeded, the similar segmentation of both the first and second free improvisation suggests that such segmentation is common in solo free improvisation, in accord with the hypothesis.},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2016-03-22},
  date = {2014-04-03},
  pages = {224-236},
  author = {Dean, Roger T. and Bailes, Freya and Drummond, Jon},
  file = {D\:\\Sauve\\Zotero\\storage\\8FV8TD5B\\Dean et al. - 2014 - Generative Structures in Improvisation Computatio.pdf;D\:\\Sauve\\Zotero\\storage\\P5CVHXJS\\09298215.2013.html}
}

@incollection{lattnerProbabilisticSegmentationMusical2015,
  langid = {english},
  title = {Probabilistic {{Segmentation}} of {{Musical Sequences Using Restricted Boltzmann Machines}}},
  isbn = {978-3-319-20602-8 978-3-319-20603-5},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-20603-5_33},
  abstract = {A salient characteristic of human perception of music is that musical events are perceived as being grouped temporally into structural units such as phrases or motifs. Segmentation of musical sequences into structural units is a topic of ongoing research, both in cognitive psychology and music information retrieval. Computational models of music segmentation are typically based either on explicit knowledge of music theory or human perception, or on statistical and information-theoretic properties of musical data. The former, rule-based approach has been found to better account for (human annotated) segment boundaries in music than probabilistic approaches [14], although the statistical model proposed in [14] performs almost as well as state-of-the-art rule-based approaches. In this paper, we propose a new probabilistic segmentation method, based on Restricted Boltzmann Machines (RBM). By sampling, we determine a probability distribution over a subset of visible units in the model, conditioned on a configuration of the remaining visible units. We apply this approach to an n-gram representation of melodies, where the RBM generates the conditional probability of a note given its n−1n−1n-1 predecessors. We use this quantity in combination with a threshold to determine the location of segment boundaries. A comparative evaluation shows that this model slightly improves segmentation performance over the model proposed in [14], and as such is closer to the state-of-the-art rule-based models.},
  number = {9110},
  booktitle = {Mathematics and {{Computation}} in {{Music}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  urldate = {2016-03-22},
  date = {2015-06-22},
  pages = {323-334},
  keywords = {Computer Appl. in Arts and Humanities},
  author = {Lattner, Stefan and Grachten, Maarten and Agres, Kat and Chacón, Carlos Eduardo Cancino},
  editor = {Collins, Tom and Meredith, David and Volk, Anja},
  file = {D\:\\Sauve\\Zotero\\storage\\U6HWVSA7\\10.html},
  doi = {10.1007/978-3-319-20603-5_33}
}

@inproceedings{marceloMelodicSegmentationUsing2012,
  title = {Melodic {{Segmentation Using}} the {{Jensen}}-{{Shannon Divergence}}},
  volume = {2},
  doi = {10.1109/ICMLA.2012.204},
  abstract = {This paper introduces an unsupervised model for melodic segmentation that extends a method initially proposed in computational biology. In the model segments are identified as sections of maximal contrast within a musical piece, using for this the Jensen-Shannon divergence. The model is extended upon its original formulation, and experiments to test its performance are carried out for a small set of selected folk song melodies. Generalization of the model is tested on 100 folk songs. Our results show a significant improvement upon the model's original formulation. In addition, we situate our model in the context of a cognition-based ensemble learning framework and justify its use within it. The need for such a cognition-based ensemble approach is also discussed.},
  eventtitle = {2012 11th {{International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  booktitle = {2012 11th {{International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  date = {2012-12},
  pages = {351-356},
  keywords = {cognition,Music,Computational Modeling,entropy,melodic segmentation,Biological system modeling,cognition-based ensemble learning framework,computational biology,Context,Context modeling,ensemble methods,folk song melodies,information theory,Jensen-Shannon divergence,Markov processes,music information retrieval,musical piece,Predictive models,unsupervised learning,unsupervised model},
  author = {{Marcelo} and {Enrique} and {Rodríguez} and {López} and Volk, A.},
  file = {D\:\\Sauve\\Zotero\\storage\\FDVSJ6IK\\Marcelo et al. - 2012 - Melodic Segmentation Using the Jensen-Shannon Dive.pdf;D\:\\Sauve\\Zotero\\storage\\BCEJMREM\\cookiedetectresponse.html}
}

@incollection{rodriguez-lopezSymbolicSegmentationCorpusbased2013,
  title = {Symbolic Segmentation: A Corpus-Based Analysis of Melodic Phrases},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-12976-1_33},
  shorttitle = {Symbolic Segmentation},
  booktitle = {Sound, {{Music}}, and {{Motion}}},
  publisher = {{Springer International Publishing}},
  urldate = {2016-03-22},
  date = {2013},
  pages = {548--557},
  author = {Rodríguez-López, Marcelo and Volk, Anja},
  file = {D\:\\Sauve\\Zotero\\storage\\D7B45ASB\\Rodríguez-López and Volk - 2013 - Symbolic segmentation a corpus-based analysis of .pdf;D\:\\Sauve\\Zotero\\storage\\AKZ6X9C4\\978-3-319-12976-1_33.html}
}

@article{silvaYouKnowWhen2014,
  title = {You Know When: {{Event}}-Related Potentials and Theta/Beta Power Indicate Boundary Prediction in Music},
  volume = {13},
  issn = {0219-6352},
  url = {http://www.worldscientific.com/doi/abs/10.1142/S0219635214500022},
  doi = {10.1142/S0219635214500022},
  shorttitle = {You Know When},
  abstract = {Neuroscientific and musicological approaches to music cognition indicate that listeners familiarized in the Western tonal tradition expect a musical phrase boundary at predictable time intervals. However, phrase boundary prediction processes in music remain untested. We analyzed event-related potentials (ERPs) and event-related induced power changes at the onset and offset of a boundary pause. We made comparisons with modified melodies, where the pause was omitted and filled by tones. The offset of the pause elicited a closure positive shift (CPS), indexing phrase boundary detection. The onset of the filling tones elicited significant increases in theta and beta powers. In addition, the P2 component was larger when the filling tones started than when they ended. The responses to boundary omission suggest that listeners expected to hear a boundary pause. Therefore, boundary prediction seems to coexist with boundary detection in music segmentation.},
  number = {01},
  journaltitle = {Journal of Integrative Neuroscience},
  shortjournal = {J. Integr. Neurosci.},
  urldate = {2016-03-22},
  date = {2014-02-25},
  pages = {19-34},
  author = {Silva, Susana and Barbosa, Fernando and Marques-Teixeira, João and Petersson, Karl Magnus and Castro, São Luís},
  file = {D\:\\Sauve\\Zotero\\storage\\8AV64PRJ\\S0219635214500022.html}
}

@article{nanNonmusiciansPerceptionPhrase2009,
  title = {Non-Musicians’ Perception of Phrase Boundaries in Music: {{A}} Cross-Cultural {{ERP}} Study},
  volume = {82},
  issn = {0301-0511},
  url = {http://www.sciencedirect.com/science/article/pii/S0301051109001240},
  doi = {10.1016/j.biopsycho.2009.06.002},
  shorttitle = {Non-Musicians’ Perception of Phrase Boundaries in Music},
  abstract = {The present study investigates neural responses to musical phrase boundaries in subjects without formal musical training, with special emphasis on the issue of cultural familiarity (i.e., the relation between the enculturation of the subjects and the cultural style of the presented music). German and Chinese non-musicians listened to Western and Chinese melodies which contained manipulated phrase boundaries while event-related potentials (ERP) were measured. The behavioral data clearly showed that melodic phrase boundary perception is influenced by cultural familiarity. The ERP revealed a series of positive and negative peaks with latencies between 40 ms and 550 ms relative to the phrase boundary offset, all of which were influenced by the phrase melodic structure type. In contrast, cultural familiarity only influenced phrase boundary processing at longer latencies, reflected by a P3-like component peaking at 280 ms.

At about 450–600 ms post phrase boundary offset, we observed a slightly right-lateralized music closure positive shift (CPS), which has been reported as a marker for phrase boundary processing in musicians in earlier studies. This study demonstrates for the first time that the music CPS can be elicited in non-musicians, suggesting that the underlying phrase boundary processing does not require formal musical training.},
  number = {1},
  journaltitle = {Biological Psychology},
  shortjournal = {Biological Psychology},
  urldate = {2016-03-22},
  date = {2009-09},
  pages = {70-81},
  keywords = {music perception,event-related potentials,Phrase structure,Cultural differences,Non-musicians},
  author = {Nan, Yun and Knösche, Thomas R. and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\HP9V4C3G\\Nan et al. - 2009 - Non-musicians’ perception of phrase boundaries in .pdf;D\:\\Sauve\\Zotero\\storage\\88QB2XUB\\S0301051109001240.html}
}

@article{istokExpressiveTimingFacilitates2013,
  title = {Expressive {{Timing Facilitates}} the {{Neural Processing}} of {{Phrase Boundaries}} in {{Music}}: {{Evidence}} from {{Event}}-{{Related Potentials}}},
  volume = {8},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0055150},
  doi = {10.1371/journal.pone.0055150},
  shorttitle = {Expressive {{Timing Facilitates}} the {{Neural Processing}} of {{Phrase Boundaries}} in {{Music}}},
  abstract = {The organization of sound into meaningful units is fundamental to the processing of auditory information such as speech and music. In expressive music performance, structural units or phrases may become particularly distinguishable through subtle timing variations highlighting musical phrase boundaries. As such, expressive timing may support the successful parsing of otherwise continuous musical material. By means of the event-related potential technique (ERP), we investigated whether expressive timing modulates the neural processing of musical phrases. Musicians and laymen listened to short atonal scale-like melodies that were presented either isochronously (deadpan) or with expressive timing cues emphasizing the melodies’ two-phrase structure. Melodies were presented in an active and a passive condition. Expressive timing facilitated the processing of phrase boundaries as indicated by decreased N2b amplitude and enhanced P3a amplitude for target phrase boundaries and larger P2 amplitude for non-target boundaries. When timing cues were lacking, task demands increased especially for laymen as reflected by reduced P3a amplitude. In line, the N2b occurred earlier for musicians in both conditions indicating general faster target detection compared to laymen. Importantly, the elicitation of a P3a-like response to phrase boundaries marked by a pitch leap during passive exposure suggests that expressive timing information is automatically encoded and may lead to an involuntary allocation of attention towards significant events within a melody. We conclude that subtle timing variations in music performance prepare the listener for musical key events by directing and guiding attention towards their occurrences. That is, expressive timing facilitates the structuring and parsing of continuous musical material even when the auditory input is unattended.},
  number = {1},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2016-03-22},
  date = {2013-01-30},
  pages = {e55150},
  keywords = {Attention,cognition,acoustics,music perception,pitch perception,music cognition,event-related potentials,Bioacoustics},
  author = {Istók, Eva and Friberg, Anders and Huotilainen, Minna and Tervaniemi, Mari},
  file = {D\:\\Sauve\\Zotero\\storage\\B6VKKNWC\\Istók et al. - 2013 - Expressive Timing Facilitates the Neural Processin.pdf;D\:\\Sauve\\Zotero\\storage\\TW6FQDAI\\article.html}
}

@article{neuhausEffectsMusicalExpertise2006,
  title = {Effects of {{Musical Expertise}} and {{Boundary Markers}} on {{Phrase Perception}} in {{Music}}},
  volume = {18},
  issn = {0898-929X},
  url = {http://dx.doi.org/10.1162/jocn.2006.18.3.472},
  doi = {10.1162/jocn.2006.18.3.472},
  abstract = {A neural correlate for phrase boundary perception in music has recently been identified in musicians. It is called music closure positive shift (“music CPS”) and has an equivalent in the perception of speech (“language CPS”). The aim of the present study was to investigate the influence of musical expertise and different phrase boundary markers on the music CPS, using event-related brain potentials (ERPs) and event-related magnetic fields (ERFs). Musicians and nonmusicians were tested while listening to binary phrased melodies. ERPs and ERFs of both subject groups differed considerably from each other. Phrased melody versions evoked an electric CPS and a magnetic CPSm in musicians, but an early negativity and a less pronounced CPSm in nonmusicians, suggesting different perceptual strategies for both subject groups. Musicians seem to process musical phrases in a structured manner similar to language. Nonmusicians, in contrast, are thought to detect primarily discontinuity in the melodic input. Variations of acoustic cues in the vicinity of the phrase boundary reveal that the CPS is influenced by a number of parameters that are considered to indicate phrasing in melodies: pause length, length of the last tone preceding the pause, and harmonic function of this last tone. This is taken as evidence that the CPS mainly reflects higher cognitive processing of phrasing, rather than mere perception of pauses. Furthermore, results suggest that the ERP and MEG methods are sensitive to different aspects within phrase perception. For both subject groups, qualitatively different ERP components (CPS and early negativity) seem to reflect a top-down activation of general but different phrasing schemata, whereas quantitatively differing MEG signals appear to reflect gradual differences in the bottom-up processing of acoustic boundary markers.},
  number = {3},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  urldate = {2016-03-22},
  date = {2006-03-01},
  pages = {472-493},
  author = {Neuhaus, Christiane and Knösche, Thomas R. and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\VN5JE2CS\\jocn.2006.18.3.html}
}

@article{ramirezMusicalNeurofeedbackTreating2015,
  title = {Musical Neurofeedback for Treating Depression in Elderly People},
  volume = {9},
  issn = {1662-4548},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4591427/},
  doi = {10.3389/fnins.2015.00354},
  abstract = {We introduce a new neurofeedback approach, which allows users to manipulate expressive parameters in music performances using their emotional state, and we present the results of a pilot clinical experiment applying the approach to alleviate depression in elderly people. Ten adults (9 female and 1 male, mean = 84, SD = 5.8) with normal hearing participated in the neurofeedback study consisting of 10 sessions (2 sessions per week) of 15 min each. EEG data was acquired using the Emotiv EPOC EEG device. In all sessions, subjects were asked to sit in a comfortable chair facing two loudspeakers, to close their eyes, and to avoid moving during the experiment. Participants listened to music pieces preselected according to their music preferences, and were encouraged to increase the loudness and tempo of the pieces, based on their arousal and valence levels. The neurofeedback system was tuned so that increased arousal, computed as beta to alpha activity ratio in the frontal cortex corresponded to increased loudness, and increased valence, computed as relative frontal alpha activity in the right lobe compared to the left lobe, corresponded to increased tempo. Pre and post evaluation of six participants was performed using the BDI depression test, showing an average improvement of 17.2\% (1.3) in their BDI scores at the end of the study. In addition, an analysis of the collected EEG data of the participants showed a significant decrease of relative alpha activity in their left frontal lobe (p = 0.00008), which may be interpreted as an improvement of their depression condition.},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  urldate = {2016-03-18},
  date = {2015-10-02},
  author = {Ramirez, Rafael and Palencia-Lefler, Manel and Giraldo, Sergio and Vamvakousis, Zacharias},
  file = {D\:\\Sauve\\Zotero\\storage\\RXC6FVDV\\Ramirez et al. - 2015 - Musical neurofeedback for treating depression in e.pdf},
  eprinttype = {pmid},
  eprint = {26483628},
  pmcid = {PMC4591427}
}

@inproceedings{hardyUsingAuditoryBrainstem2016,
  location = {{London, UK}},
  title = {Using Auditory Brainstem Responses ({{ABRs}}) to Measure Hearing Loss-Induced Increases in Neural Gain and Its Implications with Tinnitus},
  eventtitle = {Workshop on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  booktitle = {Poster at {{Workshop}} on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  date = {2016},
  author = {Hardy, A. J. and de Boer, J. and Krumbholz, K.},
  options = {useprefix=true}
}

@inproceedings{prokopiouFunctionalNeuralModelling2016,
  location = {{London, UK}},
  title = {Functional Neural Modelling of Just Noticeable Difference in Interaural Time Detection for Normal Hearing and Bilateral Cochlear Implant Users},
  eventtitle = {Workshop on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  booktitle = {Poster at {{Workshop}} on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  date = {2016},
  author = {Prokopiou, Andreas N. and Wouters, Jan and Francart, Tom}
}

@inproceedings{vanheusdenAnalysisEnvelopeFollowing2016,
  location = {{London, UK}},
  title = {Analysis of Envelope Following Responses to Natural Vowels Using a {{Fourier}} Analyzer},
  eventtitle = {Workshop on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  booktitle = {Poster at {{Workshop}} on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  date = {2016},
  author = {Vanheusden, Frederique and Bell, Steven L and Simpson, David M}
}

@inproceedings{katsiavalosAutomaticIdentificationMusical2016,
  location = {{London, UK}},
  title = {Automatic Identification of Musical Schemata via Symbolic Fingerprinting and Temporal Filters},
  eventtitle = {Workshop on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  booktitle = {Poster at {{Workshop}} on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  date = {2016},
  author = {Katsiavalos, Andreas and Collins, Tom and Battey, Brett}
}

@article{brownPsychologicalRepresentationMusical2016,
  langid = {english},
  title = {The {{Psychological Representation}} of {{Musical Intervals}} in a {{Twelve}}-{{Tone Context}}},
  volume = {33},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/33/3/274},
  doi = {10.1525/mp.2016.33.3.274},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-03-16},
  date = {2016-02-01},
  pages = {274-286},
  keywords = {statistical learning,derived row,interval,perception of non-tonal music,twelve-tone},
  author = {Brown, Jenine L.},
  file = {D\:\\Sauve\\Zotero\\storage\\82SPJHDN\\Brown - 2016 - The Psychological Representation of Musical Interv.pdf;D\:\\Sauve\\Zotero\\storage\\8UFX8B3E\\274.html}
}

@article{ramanRealTimeProbingModulations2016,
  langid = {english},
  title = {Real-{{Time Probing}} of {{Modulations}} in {{South Indian Classical}} ({{Carnātic}}) {{Music}} by {{Indian}} and {{Western Musicians}}},
  volume = {33},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/33/3/367},
  doi = {10.1525/mp.2016.33.3.367},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-03-16},
  date = {2016-02-01},
  pages = {367-393},
  keywords = {concurrent probe-tone technique,cross-cultural,culture-specific cues,modulation,tone-distribution cues},
  author = {Raman, Rachna and Dowling, W. Jay},
  file = {D\:\\Sauve\\Zotero\\storage\\8EFNRX7X\\Raman and Dowling - 2016 - Real-Time Probing of Modulations in South Indian C.pdf;D\:\\Sauve\\Zotero\\storage\\KQB2RE8H\\367.html}
}

@article{chewPlayingEdge2016,
  langid = {english},
  title = {Playing with the {{Edge}}},
  volume = {33},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/33/3/344},
  doi = {10.1525/mp.2016.33.3.344},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-03-16},
  date = {2016-02-01},
  pages = {344-366},
  keywords = {tonality,music performance,expressive timing,thresholds,visualization},
  author = {Chew, Elaine},
  file = {D\:\\Sauve\\Zotero\\storage\\KF99TTJH\\Chew - 2016 - Playing with the Edge.pdf;D\:\\Sauve\\Zotero\\storage\\XZAPFIJR\\344.html}
}

@article{cratonRollBeethovenInitial2016,
  langid = {english},
  title = {Roll {{Over Beethoven}}? {{An Initial Investigation}} of {{Listeners}}’ {{Perception}} of {{Chords Used}} in {{Rock Music}}},
  volume = {33},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/33/3/332},
  doi = {10.1525/mp.2016.33.3.332},
  shorttitle = {Roll {{Over Beethoven}}?},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-03-16},
  date = {2016-02-01},
  pages = {332-343},
  keywords = {harmonic expectancies,harmonic hierarchy,harmony perception,rock harmony,tonal hierarchy},
  author = {Craton, Lincoln G. and Juergens, Daniel S. and Michalak, Hannah R. and Poirier, Christopher R.},
  file = {D\:\\Sauve\\Zotero\\storage\\7AJFSVKV\\Craton et al. - 2016 - Roll Over Beethoven An Initial Investigation of L.pdf;D\:\\Sauve\\Zotero\\storage\\MTTBJMJM\\332.html}
}

@article{taherEffectsRepetitionAttention2016,
  langid = {english},
  title = {Effects of {{Repetition}} on {{Attention}} in {{Two}}-{{Part Counterpoint}}},
  volume = {33},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/33/3/306},
  doi = {10.1525/mp.2016.33.3.306},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-03-16},
  date = {2016-02-01},
  pages = {306-318},
  keywords = {Attention,stream segregation,repetition,texture,voice prominence},
  author = {Taher, Cecilia and Rusch, René and McAdams, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\GFV95UN4\\Taher et al. - 2016 - Effects of Repetition on Attention in Two-Part Cou.pdf;D\:\\Sauve\\Zotero\\storage\\AGWR6FF8\\306.html}
}

@article{largeNeurodynamicAccountMusical2016,
  langid = {english},
  title = {A {{Neurodynamic Account}} of {{Musical Tonality}}},
  volume = {33},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/33/3/319},
  doi = {10.1525/mp.2016.33.3.319},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-03-16},
  date = {2016-02-01},
  pages = {319-331},
  keywords = {plasticity,tonality,mode-locking,neurodynamics,oscillation},
  author = {Large, Edward W. and Kim, Ji Chul and Flaig, Nicole Kristine and Bharucha, Jamshed J. and Krumhansl, Carol Lynne},
  file = {D\:\\Sauve\\Zotero\\storage\\N9WIMVCZ\\Large et al. - 2016 - A Neurodynamic Account of Musical Tonality.pdf;D\:\\Sauve\\Zotero\\storage\\FS93FQWQ\\319.html}
}

@article{sethInteroceptiveInferenceEmotion2013,
  title = {Interoceptive Inference, Emotion, and the Embodied Self},
  volume = {17},
  issn = {1364-6613},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661313002118},
  doi = {10.1016/j.tics.2013.09.007},
  abstract = {The concept of the brain as a prediction machine has enjoyed a resurgence in the context of the Bayesian brain and predictive coding approaches within cognitive science. To date, this perspective has been applied primarily to exteroceptive perception (e.g., vision, audition), and action. Here, I describe a predictive, inferential perspective on interoception: ‘interoceptive inference’ conceives of subjective feeling states (emotions) as arising from actively-inferred generative (predictive) models of the causes of interoceptive afferents. The model generalizes ‘appraisal’ theories that view emotions as emerging from cognitive evaluations of physiological changes, and it sheds new light on the neurocognitive mechanisms that underlie the experience of body ownership and conscious selfhood in health and in neuropsychiatric illness.},
  number = {11},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2016-03-15},
  date = {2013-11},
  pages = {565-573},
  keywords = {predictive coding,emotion,active inference,experience of body ownership,interoception,rubber hand illusion},
  author = {Seth, Anil K.},
  file = {D\:\\Sauve\\Zotero\\storage\\HTPHXZZ4\\Seth - 2013 - Interoceptive inference, emotion, and the embodied.pdf;D\:\\Sauve\\Zotero\\storage\\HXE4JPGH\\cookieAbsent.html}
}

@article{cannonJamesLangeTheoryEmotions1927,
  title = {The {{James}}-{{Lange}} Theory of Emotions},
  volume = {39},
  journaltitle = {American Journal of Psychology},
  date = {1927},
  pages = {115-1124},
  author = {Cannon, W.B.}
}

@article{jamesPhysicalBasisEmotion1894,
  title = {Physical Basis of Emotion},
  volume = {1},
  journaltitle = {Psychological Review},
  date = {1894},
  pages = {516-529},
  author = {James, W.}
}

@article{peretzAmusicBrainTune2009,
  langid = {english},
  title = {The Amusic Brain: In Tune, out of Key, and Unaware},
  volume = {132},
  issn = {0006-8950, 1460-2156},
  url = {http://brain.oxfordjournals.org/content/132/5/1277},
  doi = {10.1093/brain/awp055},
  shorttitle = {The Amusic Brain},
  abstract = {Like language, music engagement is universal, complex and present early in life. However, ∼4\% of the general population experiences a lifelong deficit in music perception that cannot be explained by hearing loss, brain damage, intellectual deficiencies or lack of exposure. This musical disorder, commonly known as tone-deafness and now termed congenital amusia, affects mostly the melodic pitch dimension. Congenital amusia is hereditary and is associated with abnormal grey and white matter in the auditory cortex and the inferior frontal cortex. In order to relate these anatomical anomalies to the behavioural expression of the disorder, we measured the electrical brain activity of amusic subjects and matched controls while they monitored melodies for the presence of pitch anomalies. Contrary to current reports, we show that the amusic brain can track quarter-tone pitch differences, exhibiting an early right-lateralized negative brain response. This suggests near-normal neural processing of musical pitch incongruities in congenital amusia. It is important because it reveals that the amusic brain is equipped with the essential neural circuitry to perceive fine-grained pitch differences. What distinguishes the amusic from the normal brain is the limited awareness of this ability and the lack of responsiveness to the semitone changes that violate musical keys. These findings suggest that, in the amusic brain, the neural pitch representation cannot make contact with musical pitch knowledge along the auditory-frontal neural pathway.},
  number = {5},
  journaltitle = {Brain},
  urldate = {2016-03-15},
  date = {2009-05-01},
  pages = {1277-1286},
  author = {Peretz, Isabelle and Brattico, Elvira and Järvenpää, Miika and Tervaniemi, Mari},
  file = {D\:\\Sauve\\Zotero\\storage\\RJ7JSFF4\\Peretz et al. - 2009 - The amusic brain in tune, out of key, and unaware.pdf;D\:\\Sauve\\Zotero\\storage\\IWRW27K5\\1277.html},
  eprinttype = {pmid},
  eprint = {19336462}
}

@article{louiToneDeafnessNew2009,
  langid = {english},
  title = {Tone {{Deafness}}: {{A New Disconnection Syndrome}}?},
  volume = {29},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/29/33/10215},
  doi = {10.1523/JNEUROSCI.1701-09.2009},
  shorttitle = {Tone {{Deafness}}},
  abstract = {Communicating with one's environment requires efficient neural interaction between action and perception. Neural substrates of sound perception and production are connected by the arcuate fasciculus (AF). Although AF is known to be involved in language, its roles in non-linguistic functions are unexplored. Here, we show that tone-deaf people, with impaired sound perception and production, have reduced AF connectivity. Diffusion tensor tractography and psychophysics were assessed in tone-deaf individuals and matched controls. Abnormally reduced AF connectivity was observed in the tone deaf. Furthermore, we observed relationships between AF and auditory–motor behavior: superior and inferior AF branches predict psychophysically assessed pitch discrimination and sound production perception abilities, respectively. This neural abnormality suggests that tone deafness leads to a reduction in connectivity resulting in pitch-related impairments. Results support a dual-stream anatomy of sound production and perception implicated in vocal communications. By identifying white matter differences and their psychophysical correlates, results contribute to our understanding of how neural connectivity subserves behavior.},
  number = {33},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-03-15},
  date = {2009-08-19},
  pages = {10215-10220},
  author = {Loui, Psyche and Alsop, David and Schlaug, Gottfried},
  file = {D\:\\Sauve\\Zotero\\storage\\TDXZ8GZH\\Loui et al. - 2009 - Tone Deafness A New Disconnection Syndrome.pdf;D\:\\Sauve\\Zotero\\storage\\DRGBP8RZ\\10215.html},
  eprinttype = {pmid},
  eprint = {19692596}
}

@article{hydeCorticalThicknessCongenital2007,
  langid = {english},
  title = {Cortical {{Thickness}} in {{Congenital Amusia}}: {{When Less Is Better Than More}}},
  volume = {27},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/27/47/13028},
  doi = {10.1523/JNEUROSCI.3039-07.2007},
  shorttitle = {Cortical {{Thickness}} in {{Congenital Amusia}}},
  abstract = {Congenital amusia (or tone deafness) is a lifelong disorder characterized by impairments in the perception and production of music. A previous voxel-based morphometry (VBM) study revealed that amusic individuals had reduced white matter in the right inferior frontal gyrus (IFG) relative to musically intact controls (Hyde et al., 2006). However, this VBM study also revealed associated increases in gray matter in the same right IFG region of amusics. The objective of the present study was to better understand this morphological brain anomaly by way of cortical thickness measures that provide a more specific measure of cortical morphology relative to VBM. We found that amusic subjects (n = 21) have thicker cortex in the right IFG and the right auditory cortex relative to musically intact controls (n = 26). These cortical thickness differences suggest the presence of cortical malformations in the amusic brain, such as abnormal neuronal migration, that may have compromised the normal development of a right frontotemporal pathway.},
  number = {47},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-03-15},
  date = {2007-11-21},
  pages = {13028-13032},
  keywords = {brain,Music,congenital amusia,cortical thickness,MRI,tone deafness},
  author = {Hyde, Krista L. and Lerch, Jason P. and Zatorre, Robert J. and Griffiths, Timothy D. and Evans, Alan C. and Peretz, Isabelle},
  file = {D\:\\Sauve\\Zotero\\storage\\SAX7NX7B\\Hyde et al. - 2007 - Cortical Thickness in Congenital Amusia When Less.pdf;D\:\\Sauve\\Zotero\\storage\\E5KX264D\\13028.html},
  eprinttype = {pmid},
  eprint = {18032676}
}

@article{dixonAutomaticExtractionTempo2001,
  title = {Automatic {{Extraction}} of {{Tempo}} and {{Beat From Expressive Performances}}},
  volume = {30},
  issn = {0929-8215},
  url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.30.1.39.7119},
  doi = {10.1076/jnmr.30.1.39.7119},
  abstract = {We describe a computer program which is able to estimate the tempo and the times of musical beats in expressively performed music. The input data may be either digital audio or a symbolic representation of music such as MIDI. The data is processed off-line to detect the salient rhythmic events and the timing of these events is analysed to generate hypotheses of the tempo at various metrical levels. Based on these tempo hypotheses, a multiple hypothesis search finds the sequence of beat times which has the best fit to the rhythmic events. We show that estimating the perceptual salience of rhythmic events significantly improves the results. No prior knowledge of the tempo, meter or musical style is assumed; all required information is derived from the data. Results are presented for a range of different musical styles, including classical, jazz, and popular works with a variety of tempi and meters. The system calculates the tempo correctly in most cases, the most common error being a doubling or halving of the tempo. The calculation of beat times is also robust. When errors are made concerning the phase of the beat, the system recovers quickly to resume correct beat tracking, despite the fact that there is no high level musical knowledge encoded in the system.},
  number = {1},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-22},
  date = {2001-03-01},
  pages = {39-58},
  author = {Dixon, Simon},
  file = {D\:\\Sauve\\Zotero\\storage\\KH9Z559H\\jnmr.30.1.39.html}
}

@article{vanderweijProbabilisticModelMeter2017,
  langid = {english},
  title = {A {{Probabilistic Model}} of {{Meter Perception}}: {{Simulating Enculturation}}},
  volume = {8},
  issn = {1664-1078},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2017.00824/full},
  doi = {10.3389/fpsyg.2017.00824},
  shorttitle = {A {{Probabilistic Model}} of {{Meter Perception}}},
  abstract = {Enculturation is known to shape the perception of meter in music but this is not explicitly accounted for by current cognitive models of meter perception. We hypothesize that meter perception is a strategy for increasing the predictability of rhythmic patterns and that the way in which it is shaped by the cultural environment can be understood in terms of probabilistic predictive coding. Based on this hypothesis, we present a probabilistic model of meter perception that uses statistical properties of the relation between rhythm and meter to infer meter from quantized rhythms. We show that our model can successfully predict annotated time signatures from quantized rhythmic patterns derived from folk melodies. Furthermore, we show that by inferring meter, our model can better predict the onsets of future events than a similar probabilistic model that does not infer meter. Finally, we demonstrate how our model can be used in a proof-of-concept simulation of enculturation. From this simulation, we derive a class of rhythms that are likely to be interpreted differently by enculturated listeners with different histories of exposure to rhythms.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  urldate = {2017-05-22},
  date = {2017},
  keywords = {predictive coding,cognition,Rhythm,Computational Modeling,enculturation,meter perception},
  author = {van der Weij, Bastiaan and Pearce, Marcus T. and Honing, Henkjan},
  options = {useprefix=true}
}

@article{pearceCompressionbasedModellingMusical2017,
  langid = {english},
  title = {Compression-Based {{Modelling}} of {{Musical Similarity Perception}}},
  volume = {46},
  issn = {10.1080/09298215.2017.1305419},
  url = {http://www.tandfonline.com/eprint/XWP6EzEjbrSI2nkQk3Ue/full},
  doi = {https://doi.org/10.1080/09298215.2017.1305419},
  abstract = {(2017). Compression-based Modelling of Musical Similarity Perception. Journal of New Music Research. Ahead of Print.},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-08},
  date = {2017-03-30},
  pages = {135-155},
  keywords = {Perception,similarity,INFORMATION retrieval,timing,machine learning,representation},
  author = {Pearce, Marcus and Müllensiefen, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\CP943MHX\\Pearce and Müllensiefen - 2017 - Compression-based Modelling of Musical Similarity .pdf;D\:\\Sauve\\Zotero\\storage\\253N7KZ9\\full.html;D\:\\Sauve\\Zotero\\storage\\DGKDVT7G\\09298215.2017.html}
}

@article{gothamHierarchyPositionUsage2017,
  title = {Hierarchy and Position Usage in ‘Mixed’ Metrical Structures},
  volume = {46},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298215.2016.1253752},
  doi = {10.1080/09298215.2016.1253752},
  abstract = {Musical metre is commonly formalised in hierarchical terms, and defined or represented on the basis of regular, interacting pulse streams. These hierarchies are often merely asserted a priori, though systematic studies by Palmer and Krumhansl and Prince and Schmuckler suggest that such hierarchies are also strongly manifest in the relative usage of metrical positions: a strong metrical position is also a frequently used one. If position usage provides an insight into metrical structure, then this may provide a way of engaging with a wider range of metrical structures, including ‘mixed’ metres (5/8, 7/8 ) which are excluded by many systems of well-formedness invoked in the canonical studies of ‘simple’ metres (2/4, 3/4 ). This article assesses whether position usage in mixed metres is similarly indicative of the metrical structures asserted for them by music theory. The complete set of Bartók’s solo piano works in 7/8 (223) provides the primary case study repertoire. This is complemented by a look at the more ambiguous 8/8 (323), as part of considering how position usage might relate to distinctions between syncopation and mixed metre. An introductory discussion of sample size also provides new data and observations for larger corpora of common practice music by Bach. Brief analytical comments keep the study grounded in ‘the music’ throughout.},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-22},
  date = {2017-04-03},
  pages = {103-117},
  keywords = {Rhythm,music information retrieval,Bartók,corpus analysis,metre},
  author = {Gotham, Mark},
  file = {D\:\\Sauve\\Zotero\\storage\\IHTSNNH2\\09298215.2016.html}
}

@article{hartmannInteractionFeaturesPrediction2017,
  title = {Interaction Features for Prediction of Perceptual Segmentation: {{Effects}} of Musicianship and Experimental Task},
  volume = {46},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298215.2016.1230137},
  doi = {10.1080/09298215.2016.1230137},
  shorttitle = {Interaction Features for Prediction of Perceptual Segmentation},
  abstract = {As music unfolds in time, structure is recognised and understood by listeners, regardless of their level of musical expertise. A number of studies have found spectral and tonal changes to quite successfully model boundaries between structural sections. However, the effects of musical expertise and experimental task on computational modelling of structure are not yet well understood. These issues need to be addressed to better understand how listeners perceive the structure of music and to improve automatic segmentation algorithms. In this study, computational prediction of segmentation by listeners was investigated for six musical stimuli via a real-time task and an annotation (non real-time) task. The proposed approach involved computation of novelty curve interaction features and a prediction model of perceptual segmentation boundary density. We found that, compared to non-musicians’, musicians’ segmentation yielded lower prediction rates, and involved more features for prediction, particularly more interaction features; also non-musicians required a larger time shift for optimal segmentation modelling. Prediction of the annotation task exhibited higher rates, and involved more musical features than for the real-time task; in addition, the real-time task required time shifting of the segmentation data for its optimal modelling. We also found that annotation task models that were weighted according to boundary strength ratings exhibited improvements in segmentation prediction rates and involved more interaction features. In sum, musical training and experimental task seem to have an impact on prediction rates and on musical features involved in novelty-based segmentation models. Musical training is associated with higher presence of schematic knowledge, attention to more dimensions of musical change and more levels of the structural hierarchy, and higher speed of musical structure processing. Real-time segmentation is linked with higher response delays, less levels of structural hierarchy attended and higher data noisiness than annotation segmentation. In addition, boundary strength weighting of density was associated with more emphasis given to stark musical changes and to clearer representation of a hierarchy involving high-dimensional musical changes.},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-22},
  date = {2017-04-03},
  pages = {156-174},
  keywords = {musical training,boundary strength,novelty detection,segmentation density,segmentation task},
  author = {Hartmann, Martín and Lartillot, Olivier and Toiviainen, Petri},
  file = {D\:\\Sauve\\Zotero\\storage\\ZWKA54RA\\09298215.2016.html}
}

@article{janssenFindingOccurrencesMelodic2017,
  title = {Finding {{Occurrences}} of {{Melodic Segments}} in {{Folk Songs Employing Symbolic Similarity Measures}}},
  volume = {46},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298215.2017.1316292},
  doi = {10.1080/09298215.2017.1316292},
  abstract = {Much research has been devoted to the classification of folk songs, revealing that variants are recognised based on salient melodic segments, such as phrases and motifs, while other musical material in a melody might vary considerably. In order to judge similarity of melodies on the level of melodic segments, a successful similarity measure is needed which will allow finding occurrences of melodic segments in folk songs reliably. The present study compares several such similarity measures from different music research domains: correlation distance, city block distance, Euclidean distance, local alignment, wavelet transform and structure induction. We evaluate the measures against annotations of phrase occurrences in a corpus of Dutch folk songs, observing whether the measures detect annotated occurrences at the correct positions. Moreover, we investigate the influence of music representation on the success of the various measures, and analyse the robustness of the most successful measures over subsets of the data. Our results reveal that structure induction is a promising approach, but that local alignment and city block distance perform even better when applied to adjusted music representations. These three methods can be combined to find occurrences with increased precision.},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-22},
  date = {2017-04-03},
  pages = {118-134},
  keywords = {music similarity,occurrences,pattern matching,segments,symbolic},
  author = {Janssen, Berit and van Kranenburg, Peter and Volk, Anja},
  file = {D\:\\Sauve\\Zotero\\storage\\J3FRRWIB\\Janssen et al. - 2017 - Finding Occurrences of Melodic Segments in Folk So.pdf;D\:\\Sauve\\Zotero\\storage\\RE79RM6V\\09298215.2017.html}
}

@article{conklinFeatureSetPatterns2008,
  title = {Feature {{Set Patterns}} in {{Music}}},
  volume = {32},
  issn = {0148-9267},
  url = {http://dx.doi.org/10.1162/comj.2008.32.1.60},
  doi = {10.1162/comj.2008.32.1.60},
  number = {1},
  journaltitle = {Computer Music Journal},
  shortjournal = {Computer Music Journal},
  urldate = {2017-05-12},
  date = {2008-02-22},
  pages = {60-70},
  author = {Conklin, Darrell and Bergeron, Mathieu},
  file = {D\:\\Sauve\\Zotero\\storage\\XF8QG2ND\\comj.2008.32.1.html}
}

@article{meredithAlgorithmsDiscoveringRepeated2002,
  title = {Algorithms for Discovering Repeated Patterns in Multidimensional Representations of Polyphonic Music},
  volume = {31},
  issn = {0929-8215},
  url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.31.4.321.14162},
  doi = {10.1076/jnmr.31.4.321.14162},
  abstract = {In previous approaches to repetition discovery in music, the music to be analysed has been represented using strings. However, there are certain types of interesting musical repetitions that cannot be discovered using string algorithms. We propose a geometric approach to repetition discovery in which the music is represented as a multidimensional dataset. Certain types of interesting musical repetition that cannot be found using string algorithms can efficiently be found using algorithms that process multidimensional datasets. Our approach allows polyphonic music to be analysed as efficiently as monophonic music and it can be used to discover polyphonic repeated patterns “with gaps” in the timbre, dynamic and rhythmic structure of a passage as well as its pitch structure. We present two new algorithms: SIA and SIATEC. SIA computes all the maximal repeated patterns in a multidimensional dataset and SIATEC computes all the occurrences of all the maximal repeated patterns in a dataset. For a k -dimensional dataset of size n, the worstcase running time of SIA is O (kn 2 log 2 n) and the worst-case running time of SIATEC is O (kn 3).},
  number = {4},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-12},
  date = {2002-12-01},
  pages = {321-345},
  author = {Meredith, David and Lemström, Kjell and Wiggins, Geraint A.},
  file = {D\:\\Sauve\\Zotero\\storage\\QQA6HCF2\\jnmr.31.4.321.html}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  volume = {27},
  url = {http://dl.acm.org/citation.cfm?id=584093},
  journaltitle = {The Bell System Technical Journal},
  urldate = {2017-04-19},
  date = {1948},
  pages = {379-423, 623-656},
  author = {Shannon, Claude Elwood},
  file = {D\:\\Sauve\\Zotero\\storage\\KDWA6IZZ\\Shannon - 2001 - A mathematical theory of communication.pdf;D\:\\Sauve\\Zotero\\storage\\6T2834Q6\\citation.html}
}

@article{breimanRandomForests2001,
  title = {Random Forests},
  volume = {45},
  url = {http://www.springerlink.com/index/U0P06167N6173512.pdf},
  number = {1},
  journaltitle = {Machine learning},
  urldate = {2017-04-18},
  date = {2001},
  pages = {5--32},
  author = {Breiman, Leo},
  file = {D\:\\Sauve\\Zotero\\storage\\7VRHGZ2R\\Breiman - 2001 - Random forests.pdf;D\:\\Sauve\\Zotero\\storage\\EJVAM55C\\10.html}
}

@inproceedings{bittnerMedleyDBMultitrackDataset2014,
  title = {{{MedleyDB}}: {{A Multitrack Dataset}} for {{Annotation}}-{{Intensive MIR Research}}.},
  volume = {14},
  url = {http://matthiasmauch.de/_pdf/bittner2014medleydb.pdf},
  shorttitle = {{{MedleyDB}}},
  booktitle = {{{ISMIR}}},
  urldate = {2017-04-18},
  date = {2014},
  pages = {155--160},
  author = {Bittner, Rachel M. and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan Pablo},
  file = {D\:\\Sauve\\Zotero\\storage\\7BKBN3ZV\\Bittner et al. - 2014 - MedleyDB A Multitrack Dataset for Annotation-Inte.pdf}
}

@article{selfridge-fieldConceptualRepresentationalIssues1998,
  title = {Conceptual and Representational Issues in Melodic Comparison},
  number = {11},
  journaltitle = {Computing in musicology: a directory of research},
  date = {1998},
  pages = {3--64},
  author = {Selfridge-Field, Eleanor}
}

@book{rycroftNewGroveDictionary1983,
  eprinttype = {jstor},
  eprint = {30249775},
  title = {The {{New Grove Dictionary}} of {{Music}} and {{Musicians}}},
  publisher = {{JSTOR}},
  date = {1983},
  author = {Rycroft, David K. and Sadie, Stanley}
}

@book{tochMelodielehreBeitragZur1923,
  langid = {german},
  location = {{Berlin}},
  title = {Melodielehre: ein Beitrag zur Musiktheorie},
  shorttitle = {Melodielehre},
  publisher = {{Max Hesse}},
  date = {1923},
  author = {Toch, Ernst},
  note = {OCLC: 23833156}
}

@book{poncedeleonamadorMiningDigitalMusic2008,
  title = {Mining Digital Music Score Collections: Melody Extraction and Genre Recognition},
  url = {http://rua.ua.es/dspace/handle/10045/16184},
  shorttitle = {Mining Digital Music Score Collections},
  publisher = {{Intech}},
  urldate = {2017-04-18},
  date = {2008},
  author = {Ponce de León Amador, Pedro José and Iñesta Quereda, José Manuel and Rizo Valero, David},
  file = {D\:\\Sauve\\Zotero\\storage\\X8KAHSSG\\Ponce de León Amador et al. - 2008 - Mining digital music score collections melody ext.pdf;D\:\\Sauve\\Zotero\\storage\\MXMNI4AI\\16184.html}
}

@article{boschComparisonMelodyExtraction2016,
  title = {A Comparison of Melody Extraction Methods Based on Source-Filter Modelling},
  url = {http://m.mr-pc.org/ismir16/website/articles/256_Paper.pdf},
  journaltitle = {Proc. ISMIR, New York},
  urldate = {2017-03-02},
  date = {2016},
  author = {Bosch, Juan J. and Bittner, Rachel M. and Salamon, Justin and Gómez, Emilia},
  file = {D\:\\Sauve\\Zotero\\storage\\7NWK9XQB\\Bosch et al. - 2016 - A comparison of melody extraction methods based on.pdf;D\:\\Sauve\\Zotero\\storage\\RMIITFMQ\\Bosch et al. - 2016 - A comparison of melody extraction methods based on.pdf}
}

@book{fantAcousticTheorySpeech1971,
  title = {Acoustic Theory of Speech Production: With Calculations Based on {{X}}-Ray Studies of {{Russian}} Articulations},
  volume = {2},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=UY0iAAAAQBAJ&oi=fnd&pg=PA5&dq=G.+Fant,+Acoustic+Theory+of+Speech+Production.&ots=4eAVWbeB-U&sig=E3RAvCvFzzqV-7-5L3dlYYhfJlk},
  shorttitle = {Acoustic Theory of Speech Production},
  publisher = {{Walter de Gruyter}},
  urldate = {2017-03-30},
  date = {1971},
  author = {Fant, Gunnar},
  file = {D\:\\Sauve\\Zotero\\storage\\JAFPPUQ6\\books.html}
}

@inproceedings{herreraVibratoExtractionParameterization1998,
  title = {Vibrato Extraction and Parameterization in the Spectral Modeling Synthesis Framework},
  volume = {99},
  url = {http://www.academia.edu/download/34327454/dafx98-perfe.pdf},
  booktitle = {Proceedings of the {{Digital Audio Effects Workshop}} ({{DAFX98}})},
  urldate = {2017-03-30},
  date = {1998},
  author = {Herrera, Perfecto and Bonada, Jordi}
}

@inproceedings{bittnerMelodyExtractionContour2015,
  title = {Melody {{Extraction}} by {{Contour Classification}}.},
  url = {http://ai2-s2-pdfs.s3.amazonaws.com/03e6/8704aea0929e217e0f99449f458ba85d7c44.pdf},
  booktitle = {{{ISMIR}}},
  urldate = {2017-03-30},
  date = {2015},
  pages = {500--506},
  author = {Bittner, Rachel M. and Salamon, Justin and Essid, Slim and Bello, Juan Pablo},
  file = {D\:\\Sauve\\Zotero\\storage\\S2ME7E85\\Bittner et al. - 2015 - Melody Extraction by Contour Classification..pdf}
}

@inproceedings{salamonCurrentChallengesEvaluation2012,
  title = {Current {{Challenges}} in the {{Evaluation}} of {{Predominant Melody Extraction Algorithms}}.},
  volume = {12},
  url = {http://julian-urbano.info/files/publications/040-current-challenges-evaluation-predominant-melody-extraction-algorithms.pdf},
  booktitle = {{{ISMIR}}},
  urldate = {2017-03-30},
  date = {2012},
  pages = {289--294},
  author = {Salamon, Justin and Urbano, Julián},
  file = {D\:\\Sauve\\Zotero\\storage\\D43AA6EW\\Salamon and Urbano - 2012 - Current Challenges in the Evaluation of Predominan.pdf}
}

@article{polinerMelodyTranscriptionMusic2007,
  title = {Melody Transcription from Music Audio: {{Approaches}} and Evaluation},
  volume = {15},
  url = {http://ieeexplore.ieee.org/abstract/document/4156215/},
  shorttitle = {Melody Transcription from Music Audio},
  number = {4},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  urldate = {2017-03-30},
  date = {2007},
  pages = {1247--1256},
  author = {Poliner, Graham E. and Ellis, Daniel PW and Ehmann, Andreas F. and Gómez, Emilia and Streich, Sebastian and Ong, Beesuan},
  file = {D\:\\Sauve\\Zotero\\storage\\AUSCTDFC\\4156215.html}
}

@article{gomezTonalDescriptionPolyphonic2006,
  title = {Tonal Description of Polyphonic Audio for Music Content Processing},
  volume = {18},
  url = {http://pubsonline.informs.org/doi/abs/10.1287/ijoc.1040.0126},
  number = {3},
  journaltitle = {INFORMS Journal on Computing},
  urldate = {2017-03-30},
  date = {2006},
  pages = {294--304},
  author = {Gómez, Emilia},
  file = {D\:\\Sauve\\Zotero\\storage\\AVJPFUXK\\Gómez - 2006 - Tonal description of polyphonic audio for music co.pdf;D\:\\Sauve\\Zotero\\storage\\TBFNRRPT\\ijoc.1040.html}
}

@article{gomezMelodyDescriptionExtraction2003,
  title = {Melody Description and Extraction in the Context of Music Content Processing},
  volume = {32},
  url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.1.23.16799},
  number = {1},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-03-30},
  date = {2003},
  pages = {23--40},
  author = {Gómez, Emilia and Klapuri, Anssi and Meudic, Benoît},
  file = {D\:\\Sauve\\Zotero\\storage\\4DKS45N9\\Gómez et al. - 2003 - Melody description and extraction in the context o.pdf;D\:\\Sauve\\Zotero\\storage\\VHBH5BZ6\\jnmr.32.1.23.html}
}

@article{habibiMusicFeelingsHuman2014,
  title = {Music, Feelings, and the Human Brain.},
  volume = {24},
  url = {http://psycnet.apa.org/journals/pmu/24/1/92/},
  number = {1},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  urldate = {2017-03-20},
  date = {2014},
  pages = {92},
  author = {Habibi, Assal and Damasio, Antonio},
  file = {D\:\\Sauve\\Zotero\\storage\\WSJTEHBF\\Habibi and Damasio - 2014 - Music, feelings, and the human brain..pdf;D\:\\Sauve\\Zotero\\storage\\W66SUPGS\\92.html}
}

@incollection{juslinHowDoesMusic2011,
  title = {How Does Music Evoke Emotions? {{Exploring}} the Underlying Mechanisms},
  url = {https://philpapers.org/rec/JUSHDM},
  shorttitle = {How Does Music Evoke Emotions?},
  booktitle = {Handbook of {{Music}} and {{Emotion}}},
  publisher = {{Oxford University Press}},
  urldate = {2017-03-20},
  date = {2011},
  author = {Juslin, Patrik N. and Liljeström, Simon and Västfjäll, Daniel and Lundqvist, Lars-Olov},
  editor = {Juslin, Patrik N. and Sloboda, John},
  file = {D\:\\Sauve\\Zotero\\storage\\MUCWAFQ4\\JUSHDM.html}
}

@article{barnivAuditoryStreamingOnline2015,
  title = {Auditory {{Streaming}} as an {{Online Classification Process}} with {{Evidence Accumulation}}},
  volume = {10},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144788},
  doi = {10.1371/journal.pone.0144788},
  abstract = {When human subjects hear a sequence of two alternating pure tones, they often perceive it in one of two ways: as one integrated sequence (a single "stream" consisting of the two tones), or as two segregated sequences, one sequence of low tones perceived separately from another sequence of high tones (two "streams"). Perception of this stimulus is thus bistable. Moreover, subjects report on-going switching between the two percepts: unless the frequency separation is large, initial perception tends to be of integration, followed by toggling between integration and segregation phases. The process of stream formation is loosely named “auditory streaming”. Auditory streaming is believed to be a manifestation of human ability to analyze an auditory scene, i.e. to attribute portions of the incoming sound sequence to distinct sound generating entities. Previous studies suggested that the durations of the successive integration and segregation phases are statistically independent. This independence plays an important role in current models of bistability. Contrary to this, we show here, by analyzing a large set of data, that subsequent phase durations are positively correlated. To account together for bistability and positive correlation between subsequent durations, we suggest that streaming is a consequence of an evidence accumulation process. Evidence for segregation is accumulated during the integration phase and vice versa; a switch to the opposite percept occurs stochastically based on this evidence. During a long phase, a large amount of evidence for the opposite percept is accumulated, resulting in a long subsequent phase. In contrast, a short phase is followed by another short phase. We implement these concepts using a probabilistic model that shows both bistability and correlations similar to those observed experimentally.},
  number = {12},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2017-03-09},
  date = {2015-12-15},
  pages = {e0144788},
  keywords = {Perception,Algorithms,Normal distribution,Sensory perception,Sequence analysis,Simulation and modeling,Solid-phase extraction,Vision},
  author = {Barniv, Dana and Nelken, Israel},
  file = {D\:\\Sauve\\Zotero\\storage\\MTC8GEIE\\Barniv and Nelken - 2015 - Auditory Streaming as an Online Classification Pro.pdf;D\:\\Sauve\\Zotero\\storage\\2PZ32DQN\\article.html}
}

@article{salamonMelodyExtractionPolyphonic2012,
  title = {Melody {{Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics}}},
  volume = {20},
  issn = {1558-7916},
  doi = {10.1109/TASL.2012.2188515},
  abstract = {We present a novel system for the automatic extraction of the main melody from polyphonic music recordings. Our approach is based on the creation and characterization of pitch contours, time continuous sequences of pitch candidates grouped using auditory streaming cues. We define a set of contour characteristics and show that by studying their distributions we can devise rules to distinguish between melodic and non-melodic contours. This leads to the development of new voicing detection, octave error minimization and melody selection techniques. A comparative evaluation of the proposed approach shows that it outperforms current state-of-the-art melody extraction systems in terms of overall accuracy. Further evaluation of the algorithm is provided in the form of a qualitative error analysis and the study of the effect of key parameters and algorithmic components on system performance. Finally, we conduct a glass ceiling analysis to study the current limitations of the method, and possible directions for future work are proposed.},
  number = {6},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  date = {2012-08},
  pages = {1759-1770},
  keywords = {Music,humans,speech,music information retrieval,acoustic signal detection,Algorithm design and analysis,Audio content description,audio recording,auditory streaming cues,automatic melody extraction,ceilings,error analysis,error statistics,feature extraction,glass ceiling analysis,Instruments,key parameters components,Materials,melodic contours,melody selection techniques,multi-pitch estimation,Multiple signal classification,nonmelodic contours,octave error minimization,pitch contour,pitch contour characteristics,polyphonic music recording,polyphonic music signals,predominant melody estimation,qualitative error analysis,sequences,Speech processing,state-of-the-art melody extraction systems,time continuous pitch sequences,voice detection},
  author = {Salamon, J. and Gomez, E.},
  file = {D\:\\Sauve\\Zotero\\storage\\2SZCEZVI\\Salamon and Gomez - 2012 - Melody Extraction From Polyphonic Music Signals Us.pdf;D\:\\Sauve\\Zotero\\storage\\XWTZ7GKT\\Salamon and Gómez - 2012 - Melody extraction from polyphonic music signals us.pdf;D\:\\Sauve\\Zotero\\storage\\ETEHFBSU\\6155601.html;D\:\\Sauve\\Zotero\\storage\\FJXWU3CT\\6155601.html}
}

@article{salamonMelodyExtractionPolyphonic2014,
  title = {Melody {{Extraction}} from {{Polyphonic Music Signals}}: {{Approaches}}, Applications, and Challenges},
  volume = {31},
  issn = {1053-5888},
  doi = {10.1109/MSP.2013.2271648},
  shorttitle = {Melody {{Extraction}} from {{Polyphonic Music Signals}}},
  abstract = {Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade, melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of ?melody? from both musical and signal processing perspectives and provide a case study that interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation, and applications that build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.},
  number = {2},
  journaltitle = {IEEE Signal Processing Magazine},
  date = {2014-03},
  pages = {118-134},
  keywords = {Music,INFORMATION retrieval,music information retrieval,Instruments,Multiple signal classification,polyphonic music signals,algorithm design,algorithm evaluation,audio signal processing,Data mining,dominant melody pitch,frequency value sequence,Harmonic analysis,international evaluation campaign,melody extraction algorithms,musical processing perspectives,musical recording,Signal processing algorithms,Time-frequency analysis},
  author = {Salamon, J. and Gomez, E. and Ellis, D. P. W. and Richard, G.},
  file = {D\:\\Sauve\\Zotero\\storage\\3MUEC9JH\\Salamon et al. - 2014 - Melody extraction from polyphonic music signals A.pdf;D\:\\Sauve\\Zotero\\storage\\DFWZZQ9G\\Salamon et al. - 2014 - Melody Extraction from Polyphonic Music Signals A.pdf;D\:\\Sauve\\Zotero\\storage\\EFJP74CX\\6739213.html;D\:\\Sauve\\Zotero\\storage\\KP2DGT2A\\6739213.html}
}

@inproceedings{jordanousVoiceSeparationPolyphonic2008,
  title = {Voice Separation in {{Polyphonic Music}}: A {{Data}}-{{Driven Approach}}.},
  url = {http://www.academia.edu/download/6249561/cr1582.pdf},
  shorttitle = {Voice Separation in {{Polyphonic Music}}},
  booktitle = {{{ICMC}}},
  urldate = {2017-02-16},
  date = {2008},
  author = {Jordanous, Anna}
}

@inproceedings{kilianVoiceSeparationALocal2002,
  location = {{Paris: IRCAM - Centre Pompidou}},
  title = {Voice {{Separation}}-{{A Local Optimization Approach}}.},
  url = {http://www-devel.cs.ubc.ca/~hoos/Publ/KilHoo02.pdf},
  eventtitle = {{{ISMIR}}},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Music Information Retrieval}}},
  urldate = {2017-01-12},
  date = {2002},
  pages = {39-46},
  author = {Kilian, Jürgen and Hoos, Holger H.},
  file = {D\:\\Sauve\\Zotero\\storage\\GHVP67QS\\Kilian and Hoos - 2002 - Voice Separation-A Local Optimization Approach..pdf;D\:\\Sauve\\Zotero\\storage\\TXX5FGXA\\Kilian and Hoos - Voice Separation-A Local Optimization Approach..pdf}
}

@article{boschMelodyExtractionMeans2015,
  title = {Melody Extraction by Means of a Source-Filter Model and Pitch Contour Characterization ({{MIREX}} 2015)},
  url = {http://www.mtg.upf.edu/system/files/publications/Bosch%20MIREX%202015.pdf},
  journaltitle = {11th Music Information Retrieval Evaluation eXchange (MIREX), extended abstract, Málaga, Spain},
  urldate = {2017-03-02},
  date = {2015},
  author = {Bosch, J. and Gómez, Emilia},
  file = {D\:\\Sauve\\Zotero\\storage\\MCZZVZA9\\Bosch and Gómez - 2015 - Melody extraction by means of a source-filter mode.pdf}
}

@inproceedings{boschMelodyExtractionSymphonic2014,
  title = {Melody Extraction in Symphonic Classical Music: A Comparative Study of Mutual Agreement between Humans and Algorithms},
  url = {http://phenicx.upf.edu/system/files/publications/cim14_submission_114_ready.pdf},
  shorttitle = {Melody Extraction in Symphonic Classical Music},
  booktitle = {Proc. 9th {{Conference}} on {{Interdisciplinary Musicology}}–{{CIM14}}, {{Berlin}}, {{Germany}}},
  urldate = {2017-03-02},
  date = {2014},
  author = {Bosch, J. and Gómez, Emilia},
  file = {D\:\\Sauve\\Zotero\\storage\\D87ZVG5U\\Bosch and Gómez - 2014 - Melody extraction in symphonic classical music a .pdf}
}

@inproceedings{boschScoreinformedTimbreIndependent2012,
  title = {Score-Informed and Timbre Independent Lead Instrument Separation in Real-World Scenarios},
  url = {http://ieeexplore.ieee.org/abstract/document/6334320/},
  booktitle = {Signal {{Processing Conference}} ({{EUSIPCO}}), 2012 {{Proceedings}} of the 20th {{European}}},
  publisher = {{IEEE}},
  urldate = {2017-03-02},
  date = {2012},
  pages = {2417--2421},
  author = {Bosch, Juan J. and Kondo, Kazunobu and Marxer, Ricard and Janer, Jordi},
  file = {D\:\\Sauve\\Zotero\\storage\\2TN64QJN\\Bosch et al. - 2012 - Score-informed and timbre independent lead instrum.pdf;D\:\\Sauve\\Zotero\\storage\\PSTA635I\\6334320.html}
}

@article{boschEvaluationCombinationPitch2016,
  title = {Evaluation and Combination of Pitch Estimation Methods for Melody Extraction in Symphonic Classical Music},
  volume = {45},
  url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2016.1182191},
  doi = {https://doi.org/10.1080/09298215.2016.1182191},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-03-02},
  date = {2016},
  pages = {101--117},
  author = {Bosch, Juan J. and Marxer, Ricard and Gómez, Emilia},
  file = {D\:\\Sauve\\Zotero\\storage\\V3KYQKF5\\Bosch et al. - 2016 - Evaluation and combination of pitch estimation met.pdf;D\:\\Sauve\\Zotero\\storage\\9MBRCKP8\\09298215.2016.html;D\:\\Sauve\\Zotero\\storage\\FXYB62ND\\09298215.2016.html;D\:\\Sauve\\Zotero\\storage\\S47WRHX5\\09298215.2016.html}
}

@article{rafiiRepeatingPatternExtraction2013,
  title = {Repeating Pattern Extraction Technique ({{REPET}}): {{A}} Simple Method for Music/Voice Separation},
  volume = {21},
  url = {http://ieeexplore.ieee.org/abstract/document/6269059/},
  shorttitle = {Repeating Pattern Extraction Technique ({{REPET}})},
  number = {1},
  journaltitle = {IEEE transactions on audio, speech, and language processing},
  urldate = {2017-02-16},
  date = {2013},
  pages = {73--84},
  author = {Rafii, Zafar and Pardo, Bryan},
  file = {D\:\\Sauve\\Zotero\\storage\\VHEG6IJW\\Rafii and Pardo - 2013 - Repeating pattern extraction technique (REPET) A .pdf;D\:\\Sauve\\Zotero\\storage\\AAZMHEUD\\6269059.html}
}

@article{durrieuMusicallyMotivatedMidlevel2011,
  title = {A Musically Motivated Mid-Level Representation for Pitch Estimation and Musical Audio Source Separation},
  volume = {5},
  url = {http://ieeexplore.ieee.org/abstract/document/5784290/},
  number = {6},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  urldate = {2017-02-16},
  date = {2011},
  pages = {1180--1191},
  author = {Durrieu, J. and David, Bertrand and Richard, Gaël},
  file = {D\:\\Sauve\\Zotero\\storage\\5KXGBT7W\\5784290.html}
}

@article{paivaMelodyDetectionPolyphonic2006,
  title = {Melody Detection in Polyphonic Musical Signals: {{Exploiting}} Perceptual Rules, Note Salience, and Melodic Smoothness},
  volume = {30},
  url = {http://www.mitpressjournals.org/doi/pdf/10.1162/comj.2006.30.4.80},
  shorttitle = {Melody Detection in Polyphonic Musical Signals},
  number = {4},
  journaltitle = {Computer Music Journal},
  urldate = {2017-02-16},
  date = {2006},
  pages = {80--98},
  author = {Paiva, Rui Pedro and Mendes, Teresa and Cardoso, Amílcar},
  file = {D\:\\Sauve\\Zotero\\storage\\VHTD9JTT\\comj.2006.30.4.html;D\:\\Sauve\\Zotero\\storage\\ZJ5CQD7E\\206774.html}
}

@inproceedings{tachibanaMelodyLineEstimation2010,
  title = {Melody Line Estimation in Homophonic Music Audio Signals Based on Temporal-Variability of Melodic Source},
  url = {http://ieeexplore.ieee.org/abstract/document/5495764/},
  booktitle = {Acoustics Speech and Signal Processing (Icassp), 2010 Ieee International Conference On},
  publisher = {{IEEE}},
  urldate = {2017-02-16},
  date = {2010},
  pages = {425--428},
  author = {Tachibana, Hideyuki and Ono, Takuma and Ono, Nobutaka and Sagayama, Shigeki},
  file = {D\:\\Sauve\\Zotero\\storage\\CZ9VUQFA\\Tachibana et al. - 2010 - Melody line estimation in homophonic music audio s.pdf;D\:\\Sauve\\Zotero\\storage\\W24G54XE\\5495764.html}
}

@incollection{weydeChordandNoteBasedApproaches2016,
  title = {Chord-and {{Note}}-{{Based Approaches}} to {{Voice Separation}}},
  url = {http://link.springer.com/content/pdf/10.1007/978-3-319-25931-4.pdf#page=148},
  booktitle = {Computational {{Music Analysis}}},
  publisher = {{Springer}},
  urldate = {2017-02-16},
  date = {2016},
  pages = {137--154},
  author = {Weyde, Tillman and de Valk, Reinier},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\XG6PK94B\\Weyde and de Valk - 2016 - Chord-and Note-Based Approaches to Voice Separatio.pdf;D\:\\Sauve\\Zotero\\storage\\94KW8PXS\\978-3-319-25931-4.html}
}

@article{mcleodHMMbasedVoiceSeparation2016,
  title = {{{HMM}}-Based Voice Separation of {{MIDI}} Performance},
  volume = {45},
  url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2015.1136650},
  number = {1},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-02-16},
  date = {2016},
  pages = {17--26},
  author = {McLeod, Andrew and Steedman, Mark},
  file = {D\:\\Sauve\\Zotero\\storage\\4QISRXUB\\09298215.2015.html;D\:\\Sauve\\Zotero\\storage\\KPKNTDJZ\\09298215.2015.html}
}

@inproceedings{guiomard-kaganComparingVoiceStream2015,
  title = {Comparing Voice and Stream Segmentation Algorithms},
  url = {https://hal.archives-ouvertes.fr/hal-01246693/},
  booktitle = {International {{Society}} for {{Music Information Retrieval Conference}} ({{ISMIR}} 2015)},
  urldate = {2017-02-16},
  date = {2015},
  pages = {493--499},
  author = {Guiomard-Kagan, Nicolas and Giraud, Mathieu and Groult, Richard and Levé, Florence},
  file = {D\:\\Sauve\\Zotero\\storage\\RDDREHJF\\Guiomard-Kagan et al. - 2015 - Comparing voice and stream segmentation algorithms.pdf;D\:\\Sauve\\Zotero\\storage\\6V8ZUBGS\\hal-01246693.html}
}

@inproceedings{ishigakiPrioritizedContigCombining2011,
  title = {Prioritized Contig Combining to Segregate Voices in Polyphonic Music},
  volume = {119},
  url = {http://www.smc-conference.org/smc11/papers/smc2011_119.pdf},
  booktitle = {Sound and {{Music Computing Conference}} ({{SMC}} 2011)},
  urldate = {2017-02-16},
  date = {2011},
  author = {Ishigaki, Asako and Matsubara, Masaki and Saito, Hiroaki},
  file = {D\:\\Sauve\\Zotero\\storage\\VUVDUF8Q\\Ishigaki et al. - 2011 - Prioritized contig combining to segregate voices i.pdf}
}

@inproceedings{cambouropoulosVoiceSeparationTheoretical2006,
  title = {Voice Separation: Theoretical, Perceptual and Computational Perspectives},
  url = {http://users.auth.gr/~emilios/papers/icmpc2006.pdf},
  shorttitle = {Voice Separation},
  booktitle = {Int. {{Conf}}. on {{Music Perception}} and {{Cognition}} ({{ICMPC}})},
  urldate = {2017-02-16},
  date = {2006},
  author = {Cambouropoulos, Emilios},
  file = {D\:\\Sauve\\Zotero\\storage\\V99Z8BTX\\Cambouropoulos - 2006 - Voice separation theoretical, perceptual and comp.pdf}
}

@article{wieringCognitionbasedSegmentationMusic2009,
  title = {Cognition-Based Segmentation for Music Information Retrieval Systems},
  volume = {38},
  url = {http://www.tandfonline.com/doi/abs/10.1080/09298210903171145},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-02-16},
  date = {2009},
  pages = {139--154},
  author = {Wiering, Frans and de Nooijer, Justin and Volk, Anja and Tabachneck-Schijf, Hermi JM},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\6HXT284M\\09298210903171145.html;D\:\\Sauve\\Zotero\\storage\\KXFXI5NS\\09298210903171145.html}
}

@inproceedings{karydisHorizontalVerticalIntegration2007,
  title = {Horizontal and Vertical Integration/Segregation in Auditory Streaming: A Voice Separation Algorithm for Symbolic Musical Data},
  url = {http://smc-conference.org/smc07/SMC07%20Proceedings/SMC07%20Paper%2050.pdf},
  shorttitle = {Horizontal and Vertical Integration/Segregation in Auditory Streaming},
  booktitle = {Proceedings 4th {{Sound}} and {{Music Computing Conference}} ({{SMC}}’2007)},
  urldate = {2017-02-16},
  date = {2007},
  author = {Karydis, Ioannis and Nanopoulos, Alexandros and Papadopoulos, Apostolos and Cambouropoulos, Emilios and Manolopoulos, Yannis},
  file = {D\:\\Sauve\\Zotero\\storage\\SHKSS5EN\\Karydis et al. - 2007 - Horizontal and vertical integrationsegregation in.pdf}
}

@inproceedings{kirlinVOISELearningSegregate2005,
  title = {{{VOISE}}: {{Learning}} to {{Segregate Voices}} in {{Explicit}} and {{Implicit Polyphony}}.},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.1474&rep=rep1&type=pdf},
  shorttitle = {{{VOISE}}},
  booktitle = {{{ISMIR}}},
  publisher = {{Citeseer}},
  urldate = {2017-02-16},
  date = {2005},
  pages = {552--557},
  author = {Kirlin, Phillip B. and Utgoff, Paul E.},
  file = {D\:\\Sauve\\Zotero\\storage\\USVC5FT2\\Kirlin and Utgoff - 2005 - VOISE Learning to Segregate Voices in Explicit an.pdf}
}

@inproceedings{uitdenbogerdManipulationMusicMelody1998,
  title = {Manipulation of Music for Melody Matching},
  url = {http://dl.acm.org/citation.cfm?id=290776},
  booktitle = {Proceedings of the Sixth {{ACM}} International Conference on {{Multimedia}}},
  publisher = {{ACM}},
  urldate = {2017-02-16},
  date = {1998},
  pages = {235--240},
  author = {Uitdenbogerd, Alexandra L. and Zobel, Justin},
  file = {D\:\\Sauve\\Zotero\\storage\\SIQD56FN\\citation.html}
}

@inproceedings{rizoPatternRecognitionApproach2006,
  title = {A {{Pattern Recognition Approach}} for {{Melody Track Selection}} in {{MIDI Files}}.},
  url = {http://www.academia.edu/download/32238952/ismir2006.pdf},
  booktitle = {{{ISMIR}}},
  urldate = {2017-02-16},
  date = {2006},
  pages = {61--66},
  author = {Rizo, David and De León, Pedro J. Ponce and Pérez-Sancho, Carlos and Pertusa, Antonio and Quereda, José Manuel Iñesta}
}

@inproceedings{rizoMelodyTrackIdentification2006,
  title = {Melody {{Track Identification}} in {{Music Symbolic Files}}.},
  url = {https://vvvvw.aaai.org/Papers/FLAIRS/2006/Flairs06-049.pdf},
  booktitle = {{{FLAIRS Conference}}},
  urldate = {2017-02-16},
  date = {2006},
  pages = {254--259},
  author = {Rizo, David and De León, Pedro J. Ponce and Pertusa, Antonio and Pérez-Sancho, Carlos and Quereda, José Manuel Iñesta}
}

@inproceedings{madsenComputationalModelMelody2007,
  title = {Towards a {{Computational Model}} of {{Melody Identification}} in {{Polyphonic Music}}.},
  url = {http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-072.pdf},
  booktitle = {{{IJCAI}}},
  urldate = {2017-02-16},
  date = {2007},
  pages = {459--464},
  author = {Madsen, Søren Tjagvad and Widmer, Gerhard},
  file = {D\:\\Sauve\\Zotero\\storage\\3UWA4XGV\\Madsen and Widmer - 2007 - Towards a Computational Model of Melody Identifica.pdf}
}

@inproceedings{madsenSeparatingVoicesMIDI2006,
  title = {Separating Voices in {{MIDI}}.},
  url = {https://pdfs.semanticscholar.org/0510/f591c6ae52bd412ef65684af5e7a2764bd15.pdf},
  booktitle = {{{ISMIR}}},
  urldate = {2017-02-16},
  date = {2006},
  pages = {57--60},
  author = {Madsen, Søren Tjagvad and Widmer, Gerhard},
  file = {D\:\\Sauve\\Zotero\\storage\\G4X5T6NU\\Madsen and Widmer - 2006 - Separating voices in MIDI..pdf}
}

@article{duaneAgencyInformationContent2012,
  title = {Agency and Information Content in Eighteenth-and Early Nineteenth-Century String-Quartet Expositions},
  volume = {56},
  url = {http://jmt.dukejournals.org/content/56/1/87.short},
  number = {1},
  journaltitle = {Journal of Music Theory},
  urldate = {2017-02-16},
  date = {2012},
  pages = {87--120},
  author = {Duane, Ben},
  file = {D\:\\Sauve\\Zotero\\storage\\ZXSV6GKG\\87.html}
}

@inproceedings{madsenComplexitybasedApproachMelody2007,
  location = {{Hyderabad, India}},
  title = {A {{Complexity}}-Based {{Approach}} to {{Melody Track Identification}} in {{MIDI Files}}},
  url = {http://ai2-s2-pdfs.s3.amazonaws.com/6f0d/b229f11dd8596cfbbf31bbd1218a34f3612a.pdf},
  booktitle = {Proceedings of the {{International Workshop}} on {{Artificial Intelligence}} and {{Music}}},
  urldate = {2017-02-16},
  date = {2007},
  author = {Madsen, Søren Tjagvad and Widmer, Gerhard},
  file = {D\:\\Sauve\\Zotero\\storage\\VFDVQWF6\\Madsen and Widmer - A Complexity-based Approach to Melody Track Identi.pdf}
}

@article{fribergRecognitionMainMelody2009,
  title = {Recognition of the Main Melody in a Polyphonic Symbolic Score Using Perceptual Knowledge},
  volume = {38},
  url = {http://www.tandfonline.com/doi/abs/10.1080/09298210903215900},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-02-16},
  date = {2009},
  pages = {155--169},
  author = {Friberg, Anders and Ahlbäck, Sven},
  file = {D\:\\Sauve\\Zotero\\storage\\C4F76RG6\\09298210903215900.html;D\:\\Sauve\\Zotero\\storage\\CQAWSEGU\\09298210903215900.html}
}

@article{mccabeModelAuditoryStreaming1997,
  title = {A Model of Auditory Streaming},
  volume = {101},
  issn = {0001-4966},
  doi = {10.1121/1.418176},
  abstract = {Describes a model of streaming that focuses primarily on the formation of sequential associations and suggests a spectral account of streaming rather than one which depends on temporal fine structure. This model demonstrates how streaming might result from interactions between the tonotopic patterns of activity of incoming signals and traces of previous activity that feed back and influence the way in which subsequent signals are processed. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  number = {3},
  journaltitle = {Journal of the Acoustical Society of America},
  shortjournal = {Journal of the Acoustical Society of America},
  date = {1997-03},
  pages = {1611-1621},
  keywords = {Auditory Perception,Models,model of auditory streaming},
  author = {McCabe, Susan L. and Denham, Michael J.},
  file = {D\:\\Sauve\\Zotero\\storage\\4PJXN4C2\\McCabe and Denham - 1997 - A model of auditory streaming.pdf;D\:\\Sauve\\Zotero\\storage\\ICB3ZPD4\\1.html}
}

@article{deutschTwoChannelListening1975,
  title = {Two‐channel Listening to Musical Scales},
  volume = {57},
  issn = {0001-4966},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.380573},
  doi = {10.1121/1.380573},
  abstract = {Ss listened to a dichotic tonal sequence consisting of the repetitive presentation of the C major scale with successive tones alternating from ear to ear. The scale was presented simultaneously in both ascending and descending form, such that when a component of the ascending scale was in one ear, a component of the descending scale was in the other, and vice versa. All Ss channeled this sequence by frequency range: no S channeled by ear of input, and none reported a full ascending or descending scale. Various illusory percepts were obtained, which varied in correlation with the handedness of the listener. Right‐handers tended to perceive the upper tones of the dichotic sequence as emanating from the right earphone and the lower tones from the left, and to maintain this percept when the earphones were places in reverse position. Left‐handers as a group did not display the same localization tendency.Subject Classification: 65.22, 65.54, 65.62; 75.10.},
  number = {5},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  urldate = {2017-02-16},
  date = {1975-05-01},
  pages = {1156-1160},
  author = {Deutsch, Diana},
  file = {D\:\\Sauve\\Zotero\\storage\\Q42XVS6Q\\1975 - Two‐channel listening to musical scales.pdf;D\:\\Sauve\\Zotero\\storage\\FNB6ZREB\\1.html}
}

@article{tougasAuditoryStreamingContinuity1990,
  langid = {english},
  title = {Auditory Streaming and the Continuity Illusion},
  volume = {47},
  issn = {0031-5117, 1532-5962},
  url = {http://link.springer.com/article/10.3758/BF03205976},
  doi = {10.3758/BF03205976},
  abstract = {In the present experiment, auditory stream organization was investigated in the presence of perceptually restored continuity. It was found that auditory streaming processes tend to yield the same perceptual organization independently of the presence or absence of perceptual restoration. Other observations include the dominance of frequency proximity over trajectory as a perceptual organization principle, and the effect of harmonic enrichment on perceptual grouping.},
  number = {2},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  urldate = {2017-02-16},
  date = {1990-03-01},
  pages = {121-126},
  author = {Tougas, Yves and Bregman, Albert S.},
  file = {D\:\\Sauve\\Zotero\\storage\\EJZQB733\\Tougas and Bregman - 1990 - Auditory streaming and the continuity illusion.pdf;D\:\\Sauve\\Zotero\\storage\\PI56DMG4\\BF03205976.html}
}

@article{durrieuSourceFilterModel2010,
  title = {Source/{{Filter Model}} for {{Unsupervised Main Melody Extraction From Polyphonic Audio Signals}}},
  volume = {18},
  issn = {1558-7916},
  doi = {10.1109/TASL.2010.2041114},
  abstract = {Extracting the main melody from a polyphonic music recording seems natural even to untrained human listeners. To a certain extent it is related to the concept of source separation, with the human ability of focusing on a specific source in order to extract relevant information. In this paper, we propose a new approach for the estimation and extraction of the main melody (and in particular the leading vocal part) from polyphonic audio signals. To that aim, we propose a new signal model where the leading vocal part is explicitly represented by a specific source/filter model. The proposed representation is investigated in the framework of two statistical models: a Gaussian Scaled Mixture Model (GSMM) and an extended Instantaneous Mixture Model (IMM). For both models, the estimation of the different parameters is done within a maximum-likelihood framework adapted from single-channel source separation techniques. The desired sequence of fundamental frequencies is then inferred from the estimated parameters. The results obtained in a recent evaluation campaign (MIREX08) show that the proposed approaches are very promising and reach state-of-the-art performances on all test sets.},
  number = {3},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  date = {2010-03},
  pages = {564-575},
  keywords = {Music,humans,PERFORMANCE evaluation,audio recording,polyphonic music recording,audio signal processing,Data mining,Blind audio source separation,Expectation–Maximization (EM) algorithm,extended instantaneous mixture model,filtering theory,Filters,Frequency estimation,Gaussian processes,Gaussian scaled mixture model,Gaussian scaled mixture model (GSMM),main melody extraction,maximum likelihood,maximum likelihood estimation,maximum-likelihood framework,non-negative matrix factorization (NMF),Parameter estimation,polyphonic audio signals,signal model,single-channel source separation,source separation,source/filter model,spectral analysis,statistical model,Testing,unsupervised main melody extraction},
  author = {Durrieu, J. L. and Richard, G. and David, B. and Fevotte, C.},
  file = {D\:\\Sauve\\Zotero\\storage\\RFKKJF3M\\Durrieu et al. - 2010 - SourceFilter Model for Unsupervised Main Melody E.pdf;D\:\\Sauve\\Zotero\\storage\\3SPGD3BG\\5410055.html}
}

@article{fristonPerceptionsHypothesesSaccades2012,
  langid = {english},
  title = {Perceptions as {{Hypotheses}}: {{Saccades}} as {{Experiments}}},
  volume = {3},
  issn = {1664-1078},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00151/abstract},
  doi = {10.3389/fpsyg.2012.00151},
  shorttitle = {Perceptions as {{Hypotheses}}},
  abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organised behaviour: namely, the imperative to minimise the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free energy minimisation. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or simulated into beliefs about the world.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  urldate = {2017-02-02},
  date = {2012},
  keywords = {Salience,Prediction,free energy,Visual search,active inference,Bayesian,exploration,surprise},
  author = {Friston, Karl and Adams, Rick and Perrinet, Laurent and Breakspear, Michael},
  file = {D\:\\Sauve\\Zotero\\storage\\2S5JANEI\\Friston et al. - 2012 - Perceptions as Hypotheses Saccades as Experiments.pdf}
}

@article{patonSkullboundPerceptionPrecision2013,
  langid = {english},
  title = {Skull-Bound Perception and Precision Optimization through Culture},
  volume = {36},
  issn = {1469-1825},
  doi = {10.1017/S0140525X12002191},
  abstract = {Clark acknowledges but resists the indirect mind-world relation inherent in prediction error minimization (PEM). But directness should also be resisted. This creates a puzzle, which calls for reconceptualization of the relation. We suggest that a causal conception captures both aspects. With this conception, aspects of situated cognition, social interaction and culture can be understood as emerging through precision optimization.},
  number = {3},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  date = {2013-06},
  pages = {222},
  keywords = {Attention,brain,cognition,humans,Perception,Cognitive Science},
  author = {Paton, Bryan and Skewes, Josh and Frith, Chris and Hohwy, Jakob},
  eprinttype = {pmid},
  eprint = {23663524}
}

@article{sethExtendingPredictiveProcessing2013,
  title = {Extending Predictive Processing to the Body: {{Emotion}} as Interoceptive Inference},
  volume = {36},
  url = {https://www.researchgate.net/publication/236689214_Extending_predictive_processing_to_the_body_Emotion_as_interoceptive_inference},
  doi = {http://dx.doi.org/10.1017/S0140525X12002270},
  shorttitle = {Extending Predictive Processing to the Body},
  abstract = {Extending predictive processing to the body: Emotion as interoceptive inference on ResearchGate, the professional network for scientists.},
  number = {3},
  journaltitle = {Behavioral and Brain Sciences},
  urldate = {2017-02-02},
  date = {2013},
  pages = {47-58},
  author = {Seth, Anil K. and Critchley, Hugo},
  file = {D\:\\Sauve\\Zotero\\storage\\ZPG2AWT3\\236689214_Extending_predictive_processing_to_the_body_Emotion_as_interoceptive_inference.html}
}

@article{fristonTheoryCorticalResponses2005,
  langid = {english},
  title = {A Theory of Cortical Responses},
  volume = {360},
  issn = {0962-8436},
  doi = {10.1098/rstb.2005.1622},
  abstract = {This article concerns the nature of evoked brain responses and the principles underlying their generation. We start with the premise that the sensory brain has evolved to represent or infer the causes of changes in its sensory inputs. The problem of inference is well formulated in statistical terms. The statistical fundaments of inference may therefore afford important constraints on neuronal implementation. By formulating the original ideas of Helmholtz on perception, in terms of modern-day statistical theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts.It turns out that the problems of inferring the causes of sensory input (perceptual inference) and learning the relationship between input and cause (perceptual learning) can be resolved using exactly the same principle. Specifically, both inference and learning rest on minimizing the brain's free energy, as defined in statistical physics. Furthermore, inference and learning can proceed in a biologically plausible fashion. Cortical responses can be seen as the brain's attempt to minimize the free energy induced by a stimulus and thereby encode the most likely cause of that stimulus. Similarly, learning emerges from changes in synaptic efficacy that minimize the free energy, averaged over all stimuli encountered. The underlying scheme rests on empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organization and responses. The aim of this article is to encompass many apparently unrelated anatomical, physiological and psychophysical attributes of the brain within a single theoretical perspective. In terms of cortical architectures, the theoretical treatment predicts that sensory cortex should be arranged hierarchically, that connections should be reciprocal and that forward and backward connections should show a functional asymmetry (forward connections are driving, whereas backward connections are both driving and modulatory). In terms of synaptic physiology, it predicts associative plasticity and, for dynamic models, spike-timing-dependent plasticity. In terms of electrophysiology, it accounts for classical and extra classical receptive field effects and long-latency or endogenous components of evoked cortical responses. It predicts the attenuation of responses encoding prediction error with perceptual learning and explains many phenomena such as repetition suppression, mismatch negativity (MMN) and the P300 in electroencephalography. In psychophysical terms, it accounts for the behavioural correlates of these physiological phenomena, for example, priming and global precedence. The final focus of this article is on perceptual learning as measured with the MMN and the implications for empirical studies of coupling among cortical areas using evoked sensory responses.},
  number = {1456},
  journaltitle = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
  shortjournal = {Philos. Trans. R. Soc. Lond., B, Biol. Sci.},
  date = {2005-04-29},
  pages = {815-836},
  keywords = {humans,electrophysiology,Learning,Perception,Models; Neurological,Cerebral Cortex,Biophysical Phenomena,Biophysics,Interneurons,Models; Statistical,Synapses},
  author = {Friston, Karl},
  eprinttype = {pmid},
  eprint = {15937014},
  pmcid = {PMC1569488}
}

@article{fristonFreeenergyPrincipleRough2009,
  langid = {english},
  title = {The Free-Energy Principle: A Rough Guide to the Brain?},
  volume = {13},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2009.04.005},
  shorttitle = {The Free-Energy Principle},
  abstract = {This article reviews a free-energy formulation that advances Helmholtz's agenda to find principles of brain function based on conservation laws and neuronal energy. It rests on advances in statistical physics, theoretical biology and machine learning to explain a remarkable range of facts about brain structure and function. We could have just scratched the surface of what this formulation offers; for example, it is becoming clear that the Bayesian brain is just one facet of the free-energy principle and that perception is an inevitable consequence of active exchange with the environment. Furthermore, one can see easily how constructs like memory, attention, value, reinforcement and salience might disclose their simple relationships within this framework.},
  number = {7},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends Cogn. Sci. (Regul. Ed.)},
  date = {2009-07},
  pages = {293-301},
  keywords = {brain,Neural Pathways,humans,Learning,entropy,Recognition (Psychology),Models; Neurological,Environment,Quantum Theory},
  author = {Friston, Karl},
  eprinttype = {pmid},
  eprint = {19559644}
}

@article{toussaintProbabilisticInferenceModel2009,
  title = {Probabilistic Inference as a Model of Planned Behavior},
  volume = {3},
  url = {https://www.researchgate.net/publication/251685706_Probabilistic_inference_as_a_model_of_planned_behavior},
  abstract = {Official Full-Text Publication: Probabilistic inference as a model of planned behavior on ResearchGate, the professional network for scientists.},
  journaltitle = {ResearchGate},
  urldate = {2017-01-19},
  date = {2009-01-01},
  author = {Toussaint, Marc},
  file = {D\:\\Sauve\\Zotero\\storage\\SERKCAJC\\251685706_Probabilistic_inference_as_a_model_of_planned_behavior.html}
}

@article{fristonLearningInferenceBrain2003,
  title = {Learning and Inference in the Brain},
  volume = {16},
  issn = {0893-6080},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608003002454},
  doi = {10.1016/j.neunet.2003.06.005},
  abstract = {This article is about how the brain data mines its sensory inputs. There are several architectural principles of functional brain anatomy that have emerged from careful anatomic and physiologic studies over the past century. These principles are considered in the light of representational learning to see if they could have been predicted a priori on the basis of purely theoretical considerations. We first review the organisation of hierarchical sensory cortices, paying special attention to the distinction between forward and backward connections. We then review various approaches to representational learning as special cases of generative models, starting with supervised learning and ending with learning based upon empirical Bayes. The latter predicts many features, such as a hierarchical cortical system, prevalent top-down backward influences and functional asymmetries between forward and backward connections that are seen in the real brain.

The key points made in this article are: (i) hierarchical generative models enable the learning of empirical priors and eschew prior assumptions about the causes of sensory input that are inherent in non-hierarchical models. These assumptions are necessary for learning schemes based on information theory and efficient or sparse coding, but are not necessary in a hierarchical context. Critically, the anatomical infrastructure that may implement generative models in the brain is hierarchical. Furthermore, learning based on empirical Bayes can proceed in a biologically plausible way. (ii) The second point is that backward connections are essential if the processes generating inputs cannot be inverted, or the inversion cannot be parameterised. Because these processes involve many-to-one mappings, are non-linear and dynamic in nature, they are generally non-invertible. This enforces an explicit parameterisation of generative models (i.e. backward connections) to afford recognition and suggests that forward architectures, on their own, are not sufficient for perception. (iii) Finally, non-linearities in generative models, mediated by backward connections, require these connections to be modulatory, so that representations in higher cortical levels can interact to predict responses in lower levels. This is important in relation to functional asymmetries in forward and backward connections that have been demonstrated empirically.},
  number = {9},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  series = {Neuroinformatics},
  urldate = {2017-01-19},
  date = {2003-11},
  pages = {1325-1352},
  keywords = {predictive coding,inference,information theory,Bayesian,Generative models},
  author = {Friston, Karl},
  file = {D\:\\Sauve\\Zotero\\storage\\6MHFHSPV\\Friston - 2003 - Learning and inference in the brain.pdf;D\:\\Sauve\\Zotero\\storage\\MJPXRDQX\\S0893608003002454.html}
}

@article{slaterEffectTactileCues2016,
  langid = {english},
  title = {The Effect of Tactile Cues on Auditory Stream Segregation Ability of Musicians and Nonmusicians},
  volume = {26},
  issn = {2162-1535 0275-3987},
  doi = {10.1037/pmu0000143},
  abstract = {Difficulty perceiving music is often cited as one of the main problems facing hearing-impaired listeners. It has been suggested that musical enjoyment could be enhanced if sound information absent due to impairment is transmitted via other sensory modalities such as vision or touch. In this study, we test whether tactile cues can be used to segregate 2 interleaved melodies. Twelve musicians and 12 nonmusicians were asked to detect changes in a 4-note repeated melody interleaved with a random melody. In order to perform this task, the listener must be able to segregate the target melody from the random melody. Tactile cues were applied to the listener’s fingers on half of the blocks. Results showed that tactile cues can significantly improve the melodic segregation ability in both musician and nonmusician groups in challenging listening conditions. Overall, the musician group performance was always better; however, the magnitude of improvement with the introduction of tactile cues was similar in both groups. This study suggests that hearing-impaired listeners could potentially benefit from a system transmitting such information via a tactile modality.},
  number = {2},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  date = {2016},
  pages = {162-166},
  keywords = {Auditory Perception,*Musical Ability,*Cues,*Music Perception,*Musicians,Tactual Perception},
  author = {Slater, Kyle D. and Marozeau, Jeremy},
  file = {D\:\\Sauve\\Zotero\\storage\\MT9ADRAE\\Slater and Marozeau - 2016 - The effect of tactile cues on auditory stream segr.pdf;D\:\\Sauve\\Zotero\\storage\\6E4E532Q\\2016-22732-001.html}
}

@article{boroujeniComparisonAuditoryStream2017,
  title = {Comparison of Auditory Stream Segregation in Sighted and Early Blind Individuals},
  volume = {638},
  issn = {0304-3940},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394016309570},
  doi = {10.1016/j.neulet.2016.12.022},
  abstract = {An important characteristic of the auditory system is the capacity to analyze complex sounds and make decisions on the source of the constituent parts of these sounds. Blind individuals compensate for the lack of visual information by an increase input from other sensory modalities, including increased auditory information. The purpose of the current study was to compare the fission boundary (FB) threshold of sighted and early blind individuals through spectral aspects using a psychoacoustic auditory stream segregation (ASS) test. This study was conducted on 16 sighted and 16 early blind adult individuals. The applied stimuli were presented sequentially as the pure tones A and B and as a triplet ABA–ABA pattern at the intensity of 40 dBSL. The A tone frequency was selected as the basis at values of 500, 1000, and 2000 Hz. The B tone was presented with the difference of a 4–100\% above the basis tone frequency. Blind individuals had significantly lower FB thresholds than sighted people. FB was independent of the frequency of the tone A when expressed as the difference in the number of equivalent rectangular bandwidths (ERBs). Early blindness may increase perceptual separation of the acoustic stimuli to form accurate representations of the world.},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  urldate = {2017-01-16},
  date = {2017-01-18},
  pages = {218-221},
  keywords = {plasticity,auditory stream segregation,Blindness,Compensation,Equivalent rectangular bandwidths,Fission boundary threshold},
  author = {Boroujeni, Fatemeh Moghadasi and Heidari, Fatemeh and Rouzbahani, Masoumeh and Kamali, Mohammad},
  file = {D\:\\Sauve\\Zotero\\storage\\39TX9RKM\\Boroujeni et al. - 2017 - Comparison of auditory stream segregation in sight.pdf;D\:\\Sauve\\Zotero\\storage\\SVBAEDP6\\S0304394016309570.html}
}

@article{szaboComputationalModelsAuditory2016,
  title = {Computational {{Models}} of {{Auditory Scene Analysis}}: {{A Review}}},
  volume = {10},
  issn = {1662-4548},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5108797/},
  doi = {10.3389/fnins.2016.00524},
  shorttitle = {Computational {{Models}} of {{Auditory Scene Analysis}}},
  abstract = {Auditory scene analysis (ASA) refers to the process (es) of parsing the complex acoustic input into auditory perceptual objects representing either physical sources or temporal sound patterns, such as melodies, which contributed to the sound waves reaching the ears. A number of new computational models accounting for some of the perceptual phenomena of ASA have been published recently. Here we provide a theoretically motivated review of these computational models, aiming to relate their guiding principles to the central issues of the theoretical framework of ASA. Specifically, we ask how they achieve the grouping and separation of sound elements and whether they implement some form of competition between alternative interpretations of the sound input. We consider the extent to which they include predictive processes, as important current theories suggest that perception is inherently predictive, and also how they have been evaluated. We conclude that current computational models of ASA are fragmentary in the sense that rather than providing general competing interpretations of ASA, they focus on assessing the utility of specific processes (or algorithms) for finding the causes of the complex acoustic signal. This leaves open the possibility for integrating complementary aspects of the models into a more comprehensive theory of ASA.},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front Neurosci},
  urldate = {2017-01-16},
  date = {2016-11-15},
  author = {Szabó, Beáta T. and Denham, Susan L. and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\8Q35XASS\\Szabó et al. - 2016 - Computational Models of Auditory Scene Analysis A.pdf},
  eprinttype = {pmid},
  eprint = {27895552},
  pmcid = {PMC5108797}
}

@article{deikeProbingNeuralMechanisms2016,
  title = {Probing Neural Mechanisms Underlying Auditory Stream Segregation in Humans by Transcranial Direct Current Stimulation ({{tDCS}})},
  volume = {91},
  issn = {0028-3932},
  url = {http://www.sciencedirect.com/science/article/pii/S0028393216303098},
  doi = {10.1016/j.neuropsychologia.2016.08.017},
  abstract = {One hypothesis concerning the neural underpinnings of auditory streaming states that frequency tuning of tonotopically organized neurons in primary auditory fields in combination with physiological forward suppression is necessary for the separation of representations of high-frequency A and low-frequency B tones. The extent of spatial overlap between the tonotopic activations of A and B tones is thought to underlie the perceptual organization of streaming sequences into one coherent or two separate streams. The present study attempts to interfere with these mechanisms by transcranial direct current stimulation (tDCS) and to probe behavioral outcomes reflecting the perception of ABAB streaming sequences. We hypothesized that tDCS by modulating cortical excitability causes a change in the separateness of the representations of A and B tones, which leads to a change in the proportions of one-stream and two-stream percepts. To test this, 22 subjects were presented with ambiguous ABAB sequences of three different frequency separations (∆F) and had to decide on their current percept after receiving sham, anodal, or cathodal tDCS over the left auditory cortex. We could confirm our hypothesis at the most ambiguous ∆F condition of 6 semitones. For anodal compared with sham and cathodal stimulation, we found a significant decrease in the proportion of two-stream perception and an increase in the proportion of one-stream perception. The results demonstrate the feasibility of using tDCS to probe mechanisms underlying auditory streaming through the use of various behavioral measures. Moreover, this approach allows one to probe the functions of auditory regions and their interactions with other processing stages.},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  urldate = {2017-01-16},
  date = {2016-10},
  pages = {262-267},
  keywords = {auditory stream segregation,Perception,Frequency representation; forward suppression,tDCS},
  author = {Deike, Susann and Deliano, Matthias and Brechmann, André},
  file = {D\:\\Sauve\\Zotero\\storage\\T9TAZA8E\\Deike et al. - 2016 - Probing neural mechanisms underlying auditory stre.pdf;D\:\\Sauve\\Zotero\\storage\\VN3A7XZI\\S0028393216303098.html}
}

@article{changDetectiontheoreticAnalysisRelation2016,
  title = {A Detection-Theoretic Analysis of the Relation between Multitone Auditory Streaming and Masking},
  volume = {139},
  issn = {0001-4966},
  url = {https://www.researchgate.net/publication/302070239_A_detection-theoretic_analysis_of_the_relation_between_multitone_auditory_streaming_and_masking},
  doi = {10.1121/1.4949820},
  abstract = {Official Full-Text Publication: A detection-theoretic analysis of the relation between multitone auditory streaming and masking on ResearchGate, the professional network for scientists.},
  number = {4},
  journaltitle = {ResearchGate},
  urldate = {2017-01-16},
  date = {2016-04-01},
  pages = {1990-1990},
  author = {Chang, An-chieh and Lutfi, Robert A. and Lee, Jungmee},
  file = {D\:\\Sauve\\Zotero\\storage\\768NG4K5\\302070239_A_detection-theoretic_analysis_of_the_relation_between_multitone_auditory_streaming_a.html}
}

@thesis{changExaminingRelationAuditory2016,
  title = {Examining the Relation between Auditory Streaming and Masking Using a Detection-Theoretic Approach},
  url = {http://gradworks.umi.com/10/25/10250965.html},
  institution = {{THE UNIVERSITY OF WISCONSIN - MADISON}},
  urldate = {2017-01-16},
  date = {2016},
  author = {Chang, An-chieh},
  file = {D\:\\Sauve\\Zotero\\storage\\J4TXPFJV\\10250965.html}
}

@article{mehtaNeuralCorrelatesAttention2016,
  title = {Neural Correlates of Attention and Streaming in a Perceptually Multistable Auditory Illusion},
  volume = {140},
  issn = {0001-4966},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.4963902},
  doi = {10.1121/1.4963902},
  abstract = {In a complex acoustic environment, acoustic cues and attention interact in the formation of streams within the auditory scene. In this study, a variant of the “octave illusion” [Deutsch (1974). Nature 251, 307–309] was used to investigate the neural correlates of auditory streaming, and to elucidate the effects of attention on the interaction between sequential and concurrent sound segregation in humans. By directing subjects' attention to different frequencies and ears, it was possible to elicit several different illusory percepts with the identical stimulus. The first experiment tested the hypothesis that the illusion depends on the ability of listeners to perceptually stream the target tones from within the alternating sound sequences. In the second experiment, concurrent psychophysical measures and electroencephalography recordings provided neural correlates of the various percepts elicited by the multistable stimulus. The results show that the perception and neural correlates of the auditory illusion can be manipulated robustly by attentional focus and that the illusion is constrained in much the same way as auditory stream segregation, suggesting common underlying mechanisms.},
  number = {4},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  urldate = {2017-01-16},
  date = {2016-10-01},
  pages = {2225-2233},
  author = {Mehta, Anahita H. and Yasin, Ifat and Oxenham, Andrew J. and Shamma, Shihab},
  file = {D\:\\Sauve\\Zotero\\storage\\WMKVB6S3\\Mehta et al. - 2016 - Neural correlates of attention and streaming in a .pdf;D\:\\Sauve\\Zotero\\storage\\M4V2QMRI\\1.html}
}

@article{farkasAssessingValiditySubjective2016,
  title = {Assessing the Validity of Subjective Reports in the Auditory Streaming Paradigm},
  volume = {139},
  issn = {0001-4966},
  url = {http://asa.scitation.org/doi/full/10.1121/1.4945720},
  doi = {10.1121/1.4945720},
  abstract = {While subjective reports provide a direct measure of perception, their validity is not self-evident. Here, the authors tested three possible biasing effects on perceptual reports in the auditory streaming paradigm: errors due to imperfect understanding of the instructions, voluntary perceptual biasing, and susceptibility to implicit expectations. (1) Analysis of the responses to catch trials separately promoting each of the possible percepts allowed the authors to exclude participants who likely have not fully understood the instructions. (2) Explicit biasing instructions led to markedly different behavior than the conventional neutral-instruction condition, suggesting that listeners did not voluntarily bias their perception in a systematic way under the neutral instructions. Comparison with a random response condition further supported this conclusion. (3) No significant relationship was found between social desirability, a scale-based measure of susceptibility to implicit social expectations, and any of the perceptual measures extracted from the subjective reports. This suggests that listeners did not significantly bias their perceptual reports due to possible implicit expectations present in the experimental context. In sum, these results suggest that valid perceptual data can be obtained from subjective reports in the auditory streaming paradigm.},
  number = {4},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  urldate = {2017-01-16},
  date = {2016-04-01},
  pages = {1762-1772},
  author = {Farkas, Dávid and Denham, Susan L. and Bendixen, Alexandra and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\4QNE5Q6T\\Farkas et al. - 2016 - Assessing the validity of subjective reports in th.pdf;D\:\\Sauve\\Zotero\\storage\\5GC2Q2JH\\1.html}
}

@article{luTemporalCoherenceStructure2017,
  langid = {english},
  title = {Temporal Coherence Structure Rapidly Shapes Neuronal Interactions},
  volume = {8},
  issn = {2041-1723},
  url = {http://www.nature.com/ncomms/2017/170105/ncomms13900/full/ncomms13900.html},
  doi = {10.1038/ncomms13900},
  abstract = {One can easily identify if multiple sounds are originating from a single source yet the neural mechanisms underlying this process are unknown. Here the authors show that temporally coherent sounds elicit changes in receptive field dynamics of auditory cortical neurons in ferrets only when paying attention.},
  journaltitle = {Nature Communications},
  urldate = {2017-01-16},
  date = {2017-01-05},
  pages = {13900},
  author = {Lu, Kai and Xu, Yanbo and Yin, Pingbo and Oxenham, Andrew J. and Fritz, Jonathan B. and Shamma, Shihab A.},
  file = {D\:\\Sauve\\Zotero\\storage\\8I9D4VKD\\ncomms13900.html}
}

@article{southwellPredictabilitySalientStudy2017,
  langid = {english},
  title = {Is Predictability Salient? {{A}} Study of Attentional Capture by Auditory Patterns},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/content/372/1714/20160105},
  doi = {10.1098/rstb.2016.0105},
  shorttitle = {Is Predictability Salient?},
  abstract = {In this series of behavioural and electroencephalography (EEG) experiments, we investigate the extent to which repeating patterns of sounds capture attention. Work in the visual domain has revealed attentional capture by statistically predictable stimuli, consistent with predictive coding accounts which suggest that attention is drawn to sensory regularities. Here, stimuli comprised rapid sequences of tone pips, arranged in regular (REG) or random (RAND) patterns. EEG data demonstrate that the brain rapidly recognizes predictable patterns manifested as a rapid increase in responses to REG relative to RAND sequences. This increase is reminiscent of the increase in gain on neural responses to attended stimuli often seen in the neuroimaging literature, and thus consistent with the hypothesis that predictable sequences draw attention. To study potential attentional capture by auditory regularities, we used REG and RAND sequences in two different behavioural tasks designed to reveal effects of attentional capture by regularity. Overall, the pattern of results suggests that regularity does not capture attention.
This article is part of the themed issue ‘Auditory and visual scene analysis’.},
  number = {1714},
  journaltitle = {Phil. Trans. R. Soc. B},
  shortjournal = {Phil. Trans. R. Soc. B},
  urldate = {2017-01-16},
  date = {2017-02-19},
  pages = {20160105},
  author = {Southwell, Rosy and Baumann, Anna and Gal, Cécile and Barascud, Nicolas and Friston, Karl and Chait, Maria},
  file = {D\:\\Sauve\\Zotero\\storage\\QCCZKB7J\\Southwell et al. - 2017 - Is predictability salient A study of attentional .pdf;D\:\\Sauve\\Zotero\\storage\\BXVUP8AK\\20160105.html},
  eprinttype = {pmid},
  eprint = {28044016}
}

@article{denhamPerceptualBistabilityAuditory2013,
  title = {Perceptual Bistability in Auditory Streaming: {{How}} Much Do Stimulus Features Matter?},
  volume = {5},
  issn = {1789-3186},
  url = {http://www.akademiai.com/doi/abs/10.1556/LP.5.2013.Suppl2.6},
  doi = {10.1556/LP.5.2013.Suppl2.6},
  shorttitle = {Perceptual Bistability in Auditory Streaming},
  abstract = {The auditory two-tone streaming paradigm has been used extensively to study the mechanisms that underlie the decomposition of the auditory input into coherent sound sequences. Using longer tone sequences than usual in the literature, we show that listeners hold their first percept of the sound sequence for a relatively long period, after which perception switches between two or more alternative sound organizations, each held on average for a much shorter duration. The first percept also differs from subsequent ones in that stimulus parameters influence its quality and duration to a far greater degree than the subsequent ones. We propose an account of auditory streaming in terms of rivalry between competing temporal associations based on two sets of processes. The formation of associations (discovery of alternative interpretations) mainly affects the first percept by determining which sound group is discovered first and how long it takes for alternative groups to be established. In contrast, subsequent percepts arise from stochastic switching between the alternatives, the dynamics of which are determined by competitive interactions between the set of coexisting interpretations.},
  issue = {Supplement 2},
  journaltitle = {Learning \& Perception},
  shortjournal = {Learning \& Perception},
  urldate = {2017-01-16},
  date = {2013-06-01},
  pages = {73-100},
  author = {Denham, Susan L. and Gyimesi, Kinga and Stefanics, Gábor and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\K3JCJPTZ\\LP.5.2013.Suppl2.html}
}

@article{rankinNeuromechanisticModelAuditory2015,
  title = {Neuromechanistic {{Model}} of {{Auditory Bistability}}},
  volume = {11},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004555},
  doi = {10.1371/journal.pcbi.1004555},
  abstract = {Author Summary Humans have an astonishing ability to separate out different sound sources in a busy room: think of how we can hear individual voices in a bustling coffee shop. Rather than voices, we use sound stimuli in the lab: repeating patterns of high and low tones. The tone sequences are ambiguous and can be interpreted in different ways—either grouped into a single stream, or separated out into different streams. When listening for a long time, one’s perception switches every few seconds, a phenomenon called auditory bistability. Based on knowledge of the organization of brain areas involved in separating out different sound sources and how neurons in these areas respond to the ambiguous sequences, we developed a computational model of auditory bistabilty. Our model is less abstract than existing models and shows how groups of neurons may compete in order to dictate what you perceive. We predict how the difference between the two tone sequences affects what you hear over time and we performed an experiment with human listeners to confirm our prediction. The model provides groundwork to further explore the way the brain deals with the busy and often ambiguous world of sound.},
  number = {11},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-01-16},
  date = {2015-11-12},
  pages = {e1004555},
  keywords = {Attention,Perception,Psychoacoustics,Sensory perception,Simulation and modeling,Vision,Inhibitions,Statistical distributions},
  author = {Rankin, James and Sussman, Elyse and Rinzel, John},
  file = {D\:\\Sauve\\Zotero\\storage\\SUEDH4KJ\\Rankin et al. - 2015 - Neuromechanistic Model of Auditory Bistability.pdf;D\:\\Sauve\\Zotero\\storage\\KZFAGGAZ\\article.html}
}

@article{millModellingEmergenceDynamics2013,
  title = {Modelling the {{Emergence}} and {{Dynamics}} of {{Perceptual Organisation}} in {{Auditory Streaming}}},
  volume = {9},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002925},
  doi = {10.1371/journal.pcbi.1002925},
  abstract = {Author Summary The sound waves produced by objects in the environment mix together before reaching the ears. Before we can make sense of an auditory scene, our brains must solve the puzzle of how to disassemble the sound waveform into groupings that correspond to the original source signals. How is this feat accomplished? We propose that the auditory system continually scans the structure of incoming signals in search of clues to indicate which pieces belong together. For instance, sound events may belong together if they have similar features, or form part of a clear temporal pattern. However this process is complicated by lack of knowledge of future events and the many possible ways in which even a simple sound sequence can be decomposed. The biological solution is multistability: one possible interpretation of a sound is perceived initially, which then gives way to another interpretation, and so on. We propose a model of auditory multistability, in which fragmental descriptions of the signal compete and cooperate to explain the sound scene. We demonstrate, using simplified experimental stimuli, that the model can account for both the contents (perceptual organisations) and the dynamics of human perception in auditory streaming.},
  number = {3},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-01-16},
  date = {2013-03-14},
  pages = {e1002925},
  keywords = {HEARING,Perception,Sensory perception,Simulation and modeling,Vision,Behavior,Dynamical systems,Phase determination},
  author = {Mill, Robert W. and Bőhm, Tamás M. and Bendixen, Alexandra and Winkler, István and Denham, Susan L.},
  file = {D\:\\Sauve\\Zotero\\storage\\U6NR4Z4P\\Mill et al. - 2013 - Modelling the Emergence and Dynamics of Perceptual.pdf;D\:\\Sauve\\Zotero\\storage\\5A2SCS34\\article.html}
}

@article{changAuditoryStreamingTones2015,
  title = {Auditory Streaming of Tones of Uncertain Frequency, Level, and Duration},
  volume = {138},
  issn = {0001-4966},
  url = {http://asa.scitation.org/doi/full/10.1121/1.4936981},
  doi = {10.1121/1.4936981},
  abstract = {Stimulus uncertainty is known to critically affect auditory masking, but its influence on auditory streaming has been largely ignored. Standard ABA-ABA tone sequences were made increasingly uncertain by increasing the sigma of normal distributions from which the frequency, level, or duration of tones were randomly drawn. Consistent with predictions based on a model of masking by Lutfi, Gilbertson, Chang, and Stamas [J. Acoust. Soc. Am. 134, 2160–2170 (2013)], the frequency difference for which A and B tones formed separate streams increased as a linear function of sigma in tone frequency but was much less affected by sigma in tone level or duration.},
  number = {6},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  urldate = {2017-01-16},
  date = {2015-12-01},
  pages = {EL504-EL508},
  author = {Chang, An-Chieh and Lutfi, Robert A. and Lee, Jungmee},
  file = {D\:\\Sauve\\Zotero\\storage\\7DWZ4CSK\\Chang et al. - 2015 - Auditory streaming of tones of uncertain frequency.pdf;D\:\\Sauve\\Zotero\\storage\\9WZG7WNV\\1.html}
}

@article{bregmanAuditorySegregationStream1975,
  langid = {english},
  title = {Auditory Segregation: Stream or Streams?},
  volume = {1},
  issn = {0096-1523},
  shorttitle = {Auditory Segregation},
  abstract = {When auditory material segregates into "streams," is the unattended stream actually organized as an entity? An affirmative answer is suggested by the observation that the organizational structure of the unattended material interacts with the structure of material to which the subject is trying to attend. Specificially, a to-be-rejected stream can, because of its structure, capture from a to-be-judged stream elements that would otherwiise be acceptable members of the to-be-judged stream.},
  number = {3},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  date = {1975-08},
  pages = {263-267},
  keywords = {Attention,Auditory Perception,humans,Adult,Adolescent,Judgment},
  author = {Bregman, A. S. and Rudnicky, A. I.},
  eprinttype = {pmid},
  eprint = {1202149}
}

@article{vondermalsburgNeuralCocktailpartyProcessor1986,
  langid = {english},
  title = {A Neural Cocktail-Party Processor},
  volume = {54},
  issn = {0340-1200},
  abstract = {Sensory segmentation is an outstanding unsolved problem of theoretical, practical and technical importance. The basic idea of a solution is described in the form of a model. The response of "neurons" within the sensory field is temporally unstable. Segmentation is expressed by synchronization within segments and desynchronization between segments. Correlations are generated by an autonomous pattern formation process. Neuronal coupling is the result both of peripheral evidence (similarity of local quality) and of central evidence (common membership in a stored pattern). The model is consistent with known anatomy and physiology. However, a new physiological function, synaptic modulation, has to be postulated. The present paper restricts explicit treatment to the peripheral evidence represented by amplitude modulations globally present in all components of a sound spectrum. Generalization to arbitrary sensory qualities will be the subject of a later paper. The model is an application and illustration of the Correlation Theory of brain function.},
  number = {1},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol Cybern},
  date = {1986},
  pages = {29-40},
  keywords = {Auditory Perception,animals,humans,Models; Neurological,Neuronal Plasticity,Synapses,Action Potentials,Mathematics,Neural Conduction,Neural Inhibition,Neurons; Afferent,Pattern Recognition; Automated},
  author = {von der Malsburg, C. and Schneider, W.},
  options = {useprefix=true},
  eprinttype = {pmid},
  eprint = {3719028}
}

@article{ragertSegregationIntegrationAuditory2014,
  title = {Segregation and {{Integration}} of {{Auditory Streams}} When {{Listening}} to {{Multi}}-{{Part Music}}},
  volume = {9},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0084085},
  doi = {10.1371/journal.pone.0084085},
  abstract = {In our daily lives, auditory stream segregation allows us to differentiate concurrent sound sources and to make sense of the scene we are experiencing. However, a combination of segregation and the concurrent integration of auditory streams is necessary in order to analyze the relationship between streams and thus perceive a coherent auditory scene. The present functional magnetic resonance imaging study investigates the relative role and neural underpinnings of these listening strategies in multi-part musical stimuli. We compare a real human performance of a piano duet and a synthetic stimulus of the same duet in a prioritized integrative attention paradigm that required the simultaneous segregation and integration of auditory streams. In so doing, we manipulate the degree to which the attended part of the duet led either structurally (attend melody vs. attend accompaniment) or temporally (asynchronies vs. no asynchronies between parts), and thus the relative contributions of integration and segregation used to make an assessment of the leader-follower relationship. We show that perceptually the relationship between parts is biased towards the conventional structural hierarchy in western music in which the melody generally dominates (leads) the accompaniment. Moreover, the assessment varies as a function of both cognitive load, as shown through difficulty ratings and the interaction of the temporal and the structural relationship factors. Neurally, we see that the temporal relationship between parts, as one important cue for stream segregation, revealed distinct neural activity in the planum temporale. By contrast, integration used when listening to both the temporally separated performance stimulus and the temporally fused synthetic stimulus resulted in activation of the intraparietal sulcus. These results support the hypothesis that the planum temporale and IPS are key structures underlying the mechanisms of segregation and integration of auditory streams, respectively.},
  number = {1},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2017-01-13},
  date = {2014-01-24},
  pages = {e84085},
  keywords = {Attention,music perception,pitch perception,Bioacoustics,Prefrontal cortex,Sensory perception,Vision,Neuroimaging},
  author = {Ragert, Marie and Fairhurst, Merle T. and Keller, Peter E.},
  file = {D\:\\Sauve\\Zotero\\storage\\ETNRIIKU\\Ragert et al. - 2014 - Segregation and Integration of Auditory Streams wh.pdf;D\:\\Sauve\\Zotero\\storage\\Q4U643WP\\article.html}
}

@article{robertsEffectsBuildupResetting2008,
  langid = {english},
  title = {Effects of the Build-up and Resetting of Auditory Stream Segregation on Temporal Discrimination},
  volume = {34},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.34.4.992},
  abstract = {The tendency to hear a tone sequence as 2 or more streams (segregated) builds up, but a sudden change in properties can reset the percept to 1 stream (integrated). This effect has not hitherto been explored using an objective measure of streaming. Stimuli comprised a 2.0-s fixed-frequency inducer followed by a 0.6-s test sequence of alternating pure tones (3 low [L]-high [H] cycles). Listeners compared intervals for which the test sequence was either isochronous or the H tones were slightly delayed. Resetting of segregation should make identifying the anisochronous interval easier. The HL frequency separation was varied (0-12 semitones), and properties of the inducer and test sequence were set to the same or different values. Inducer properties manipulated were frequency, number of onsets (several short bursts vs. one continuous tone), tone:silence ratio (short vs. extended bursts), level, and lateralization. All differences between the inducer and the L tones reduced temporal discrimination thresholds toward those for the no-inducer case, including properties shown previously not to affect segregation greatly. Overall, it is concluded that abrupt changes in a sequence cause resetting and improve subsequent temporal discrimination.},
  number = {4},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  date = {2008-08},
  pages = {992-1006},
  keywords = {Auditory Perception,Music,Pitch Discrimination,time perception,Acoustic Stimulation,humans,Auditory Threshold,Psychoacoustics,Discrimination (Psychology),Perceptual Masking,Audiometry,Differential Threshold,Models; Psychological},
  author = {Roberts, Brian and Glasberg, Brian R. and Moore, Brian C. J.},
  eprinttype = {pmid},
  eprint = {18665740}
}

@article{micheylAuditoryStreamSegregation2010,
  langid = {english},
  title = {Auditory Stream Segregation and the Perception of Across-Frequency Synchrony},
  volume = {36},
  issn = {1939-1277},
  doi = {10.1037/a0017601},
  abstract = {This study explored the extent to which sequential auditory grouping affects the perception of temporal synchrony. In Experiment 1, listeners discriminated between 2 pairs of asynchronous "target" tones at different frequencies, A and B, in which the B tone either led or lagged. Thresholds were markedly higher when the target tones were temporally surrounded by "captor tones" at the A frequency than when the captor tones were absent or at a remote frequency. Experiment 2 extended these findings to asynchrony detection, revealing that the perception of synchrony, one of the most potent cues for simultaneous auditory grouping, is not immune to competing effects of sequential grouping. Experiment 3 examined the influence of ear separation on the interactions between sequential and simultaneous grouping cues. The results showed that, although ear separation could facilitate perceptual segregation and impair asynchrony detection, it did not prevent the perceptual integration of simultaneous sounds.},
  number = {4},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  date = {2010-08},
  pages = {1029-1039},
  keywords = {Attention,Pitch Discrimination,Cues,time perception,Acoustic Stimulation,humans,Adult,Auditory Threshold,Female,Male,Psychoacoustics,Recognition (Psychology),Young Adult,Adolescent,Sound Spectrography},
  author = {Micheyl, Christophe and Hunter, Cynthia and Oxenham, Andrew J.},
  eprinttype = {pmid},
  eprint = {20695716},
  pmcid = {PMC4315665}
}

@article{pressnitzerTemporalDynamicsAuditory2006,
  langid = {english},
  title = {Temporal Dynamics of Auditory and Visual Bistability Reveal Common Principles of Perceptual Organization},
  volume = {16},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2006.05.054},
  abstract = {When dealing with natural scenes, sensory systems have to process an often messy and ambiguous flow of information. A stable perceptual organization nevertheless has to be achieved in order to guide behavior. The neural mechanisms involved can be highlighted by intrinsically ambiguous situations. In such cases, bistable perception occurs: distinct interpretations of the unchanging stimulus alternate spontaneously in the mind of the observer. Bistable stimuli have been used extensively for more than two centuries to study visual perception. Here we demonstrate that bistable perception also occurs in the auditory modality. We compared the temporal dynamics of percept alternations observed during auditory streaming with those observed for visual plaids and the susceptibilities of both modalities to volitional control. Strong similarities indicate that auditory and visual alternations share common principles of perceptual bistability. The absence of correlation across modalities for subject-specific biases, however, suggests that these common principles are implemented at least partly independently across sensory modalities. We propose that visual and auditory perceptual organization could rely on distributed but functionally similar neural competition mechanisms aimed at resolving sensory ambiguities.},
  number = {13},
  journaltitle = {Current biology: CB},
  shortjournal = {Curr. Biol.},
  date = {2006-07-11},
  pages = {1351-1357},
  keywords = {Auditory Perception,VISUAL perception,Acoustic Stimulation,humans,Adult,SYSNEURO,Photic Stimulation,Volition},
  author = {Pressnitzer, Daniel and Hupé, Jean-Michel},
  file = {D\:\\Sauve\\Zotero\\storage\\U3J29SD8\\Pressnitzer and Hupé - 2006 - Temporal Dynamics of Auditory and Visual Bistabili.pdf;D\:\\Sauve\\Zotero\\storage\\ARQLAWF4\\S0960-9822(06)01637-X.html},
  eprinttype = {pmid},
  eprint = {16824924}
}

@article{micheylTemporalCoherenceHarmonicity2013,
  title = {Temporal Coherence versus Harmonicity in Auditory Stream Formation},
  volume = {133},
  issn = {0001-4966},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.4789866},
  doi = {10.1121/1.4789866},
  abstract = {This study sought to investigate the influence of temporal incoherence and inharmonicity on concurrent stream segregation, using performance-based measures. Subjects discriminated frequency shifts in a temporally regular sequence of target pure tones, embedded in a constant or randomly varying multi-tone background. Depending on the condition tested, the target tones were either temporally coherent or incoherent with, and either harmonically or inharmonically related to, the background tones. The results provide further evidence that temporal incoherence facilitates stream segregation and they suggest that deviations from harmonicity can cause similar facilitation effects, even when the targets and the maskers are temporally coherent.},
  number = {3},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  urldate = {2017-01-13},
  date = {2013-02-06},
  pages = {EL188-EL194},
  author = {Micheyl, Christophe and Kreft, Heather and Shamma, Shihab and Oxenham, Andrew J.},
  file = {D\:\\Sauve\\Zotero\\storage\\MX77S7WT\\1.html}
}

@book{mitchellAttentionAssociativeLearning2010,
  langid = {english},
  title = {Attention and {{Associative Learning}}: {{From Brain}} to {{Behaviour}}},
  isbn = {978-0-19-955053-1},
  shorttitle = {Attention and {{Associative Learning}}},
  abstract = {Attention and learning are two of the most important topics in contemporary cognitive psychology and behavioural neuroscience. Of even more interest is how the two interact. Meaningful stimuli and their meaningful effects are invariably embedded in a complex background of meaningless information. Yet, in order to learn about meaningful relationships between events, an organism needs to be able to extract the relevant from the irrelevant. The ability to direct attention selectively to some stimuli and away from others is one fundamental mechanism by which this filtering of information can occur. But what controls this selective attention? Why are certain stimuli selected and others rejected? What are the neural mechanisms underlying this ability? Are they the same in humans as in other animals? And what are the consequences of damage to this attentional system? These are the questions that this book aims to answer.  The idea of an interaction between attention and learning has experienced a huge surge of interest in recent years. Advances in behavioural neuroscience have made it possible to investigate the neural basis of attention mechanisms; advances in connectionist modelling techniques have allowed us to implement and test more complex computational models of the operation of these mechanisms; and recent studies have implicated impairments in the ability to deploy selective attention appropriately in disorders such as schizophrenia and Parkinson's Disease.  This book brings together leading international learning and attention researchers to provide both a comprehensive and wide-ranging overview of the current state of knowledge of this area as well as new perspectives and directions for the future. There are coherent themes that run throughout the book, but there are also, inevitably, fundamental disagreements between contributors on the role of attention in learning. Together, the views expressed in this book paint a picture of a vibrant and exciting area of psychological research, and will be essential reading for researchers of learning and attention.},
  pagetotal = {423},
  publisher = {{Oxford University Press}},
  date = {2010},
  keywords = {Psychology / Cognitive Psychology & Cognition},
  author = {Mitchell, Chris J. and Pelley, Mike E. Le}
}

@article{millerTrillThreshold1950,
  title = {The Trill Threshold},
  volume = {22},
  issn = {0001-4966},
  doi = {10.1121/1.1906663},
  abstract = {"Two tones of different frequencies alternated successively five times per second. When the difference in frequency was small, the alternation sounded like a continuous up-and-down movement of the pitch. When the difference in frequency was large, the alternation sounded like two unrelated, interrupted tones. The transition point between these two perceptual organizations is called the trill threshold. The trill threshold was measured as a function of frequency for 14 subjects." The median frequency difference (ΔF) at the trill threshold increases as a function of frequency. The ratio [Equation omitted] stays, however, nearly constant at about 0.15 up to frequencies of about 2000 cps; then it decreases. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
  journaltitle = {Journal of the Acoustical Society of America},
  shortjournal = {Journal of the Acoustical Society of America},
  date = {1950},
  pages = {637-638},
  keywords = {HEARING,AUDITION,No terms assigned,TRILL THRESHOLD},
  author = {Miller, George A. and Heise, George A.},
  file = {D\:\\Sauve\\Zotero\\storage\\M6VXSIVR\\1.html}
}

@article{szetoStreamSegregationAlgorithm2006,
  title = {Stream Segregation Algorithm for Pattern Matching in Polyphonic Music Databases},
  volume = {30},
  url = {http://link.springer.com/article/10.1007/s11042-006-0011-9},
  number = {1},
  journaltitle = {Multimedia Tools and Applications},
  urldate = {2017-01-12},
  date = {2006},
  pages = {109--127},
  author = {Szeto, Wai Man and Wong, Man Hon},
  file = {D\:\\Sauve\\Zotero\\storage\\VMK7X56U\\s11042-006-0011-9.html}
}

@inproceedings{szetoStreamSegregationAlgorithm2003,
  title = {A Stream Segregation Algorithm for Polyphonic Music Databases},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1214920},
  booktitle = {Database {{Engineering}} and {{Applications Symposium}}, 2003. {{Proceedings}}. {{Seventh International}}},
  publisher = {{IEEE}},
  urldate = {2017-01-12},
  date = {2003},
  pages = {130--138},
  author = {Szeto, Wai Man and Wong, Man Hon},
  file = {D\:\\Sauve\\Zotero\\storage\\6Z3CD8GF\\1214920.html}
}

@article{marsdenModellingPerceptionMusical1992,
  title = {Modelling the Perception of Musical Voices: A Case Study in Rule-Based Systems},
  shorttitle = {Modelling the Perception of Musical Voices},
  journaltitle = {Computer representations and models in music},
  date = {1992},
  pages = {239--263},
  author = {Marsden, Alan}
}

@inproceedings{chewSeparatingVoicesPolyphonic2004,
  langid = {english},
  title = {Separating {{Voices}} in {{Polyphonic Music}}: {{A Contig Mapping Approach}}},
  isbn = {978-3-540-24458-5 978-3-540-31807-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-540-31807-1_1},
  shorttitle = {Separating {{Voices}} in {{Polyphonic Music}}},
  abstract = {Voice separation is a critical component of music information retrieval, music analysis and automated transcription systems. We present a contig mapping approach to voice separation based on perceptual principles. The algorithm runs in O(n 2) time, uses only pitch height and event boundaries, and requires no user-defined parameters. The method segments a piece into contigs according to voice count, then reconnects fragments in adjacent contigs using a shortest distance strategy. The order of connection is by distance from maximal voice contigs, where the voice ordering is known. This contig-mapping algorithm has been implemented in VoSA, a Java-based voice separation analyzer software. The algorithm performed well when applied to J. S. Bach’s Two- and Three-Part Inventions and the forty-eight Fugues from the Well-Tempered Clavier. We report an overall average fragment consistency of 99.75\%, correct fragment connection rate of 94.50\% and average voice consistency of 88.98\%, metrics which we propose to measure voice separation performance.},
  eventtitle = {International {{Symposium}} on {{Computer Music Modeling}} and {{Retrieval}}},
  booktitle = {Computer {{Music Modeling}} and {{Retrieval}}},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2017-01-12},
  date = {2004-05-26},
  pages = {1-20},
  keywords = {Artificial Intelligence (incl. Robotics),Database Management,Information Storage and Retrieval,Information Systems Applications (incl. Internet),Multimedia Information Systems,Special Purpose and Application-Based Systems},
  author = {Chew, Elaine and Wu, Xiaodan},
  editor = {Wiil, Uffe Kock},
  file = {D\:\\Sauve\\Zotero\\storage\\BUIBPUTC\\10.html},
  doi = {10.1007/978-3-540-31807-1_1}
}

@article{cambouropoulosMIDITraditionalMusical2000,
  title = {From {{MIDI}} to Traditional Musical Notation},
  url = {https://www.researchgate.net/publication/228568548_From_MIDI_to_traditional_musical_notation},
  abstract = {Official Full-Text Publication: From MIDI to traditional musical notation on ResearchGate, the professional network for scientists.},
  journaltitle = {ResearchGate},
  urldate = {2017-01-12},
  date = {2000-01-01},
  author = {Cambouropoulos, Emilios},
  file = {D\:\\Sauve\\Zotero\\storage\\CXMFGTQA\\228568548_From_MIDI_to_traditional_musical_notation.html}
}

@article{boltzProcessingMelodicTemporal1999,
  title = {The Processing of Melodic and Temporal Information: Independent or Unified Dimensions?},
  volume = {28},
  number = {1},
  journaltitle = {Journal of New Music Research},
  date = {1999},
  pages = {67-79},
  author = {Boltz, M. G.}
}

@article{hannonMetricalCategoriesInfancy2005a,
  title = {Metrical Categories in Infancy and Adulthood},
  volume = {16},
  journaltitle = {Psychological Science},
  year = {2005a},
  pages = {48-55},
  author = {Hannon, E. E. and Trehub, S. E.}
}

@article{hannonInfantsUseMeter2005,
  title = {Infants Use Meter to Categorize Rhythms and Melodies: {{Implications}} for Musical Structure Learning},
  volume = {50},
  journaltitle = {Cognitive Psychology},
  date = {2005},
  pages = {354-377},
  author = {Hannon, E. E. and Johnson, S. P.}
}

@article{hannonFamiliarityOverridesComplexity2012,
  title = {Familiarity {{Overrides Complexity}} in {{Rhythm Perception}}: {{A Cross}}-{{Cultural Comparison}} of {{American}} and {{Turkish Listeners}}},
  volume = {38},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {2012},
  pages = {543-548},
  author = {Hannon, E. E. and Soley, G. and Ullal, S.}
}

@book{lerdahlGenerativeTheoryTonal1983,
  langid = {english},
  location = {{Cambridge, MA}},
  title = {A {{Generative Theory}} of {{Tonal Music}}},
  isbn = {978-0-262-26091-6},
  pagetotal = {388},
  publisher = {{MIT Press}},
  date = {1983},
  author = {Lerdahl, Fred and Jackendoff, Ray}
}

@article{gingrasLinkingMelodicExpectation2016,
  langid = {english},
  title = {Linking Melodic Expectation to Expressive Performance Timing and Perceived Musical Tension},
  volume = {42},
  issn = {1939-1277},
  doi = {10.1037/xhp0000141},
  abstract = {This research explored the relations between the predictability of musical structure, expressive timing in performance, and listeners' perceived musical tension. Studies analyzing the influence of expressive timing on listeners' affective responses have been constrained by the fact that, in most pieces, the notated durations limit performers' interpretive freedom. To circumvent this issue, we focused on the unmeasured prelude, a semi-improvisatory genre without notated durations. In Experiment 1, 12 professional harpsichordists recorded an unmeasured prelude on a harpsichord equipped with a MIDI console. Melodic expectation was assessed using a probabilistic model (IDyOM [Information Dynamics of Music]) whose expectations have been previously shown to match closely those of human listeners. Performance timing information was extracted from the MIDI data using a score-performance matching algorithm. Time-series analyses showed that, in a piece with unspecified note durations, the predictability of melodic structure measurably influenced tempo fluctuations in performance. In Experiment 2, another 10 harpsichordists, 20 nonharpsichordist musicians, and 20 nonmusicians listened to the recordings from Experiment 1 and rated the perceived tension continuously. Granger causality analyses were conducted to investigate predictive relations among melodic expectation, expressive timing, and perceived tension. Although melodic expectation, as modeled by IDyOM, modestly predicted perceived tension for all participant groups, neither of its components, information content or entropy, was Granger causal. In contrast, expressive timing was a strong predictor and was Granger causal. However, because melodic expectation was also predictive of expressive timing, our results outline a complete chain of influence from predictability of melodic structure via expressive performance timing to perceived musical tension. (PsycINFO Database Record},
  number = {4},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  date = {2016-04},
  pages = {594-609},
  author = {Gingras, Bruno and Pearce, Marcus T. and Goodchild, Meghan and Dean, Roger T. and Wiggins, Geraint and McAdams, Stephen},
  eprinttype = {pmid},
  eprint = {26594881}
}

@article{sohogluDetectingRepresentingPredictable2016,
  langid = {english},
  title = {Detecting and Representing Predictable Structure during Auditory Scene Analysis},
  volume = {5},
  issn = {2050-084X},
  url = {https://elifesciences.org/content/5/e19113v1},
  doi = {10.7554/eLife.19113},
  abstract = {Everyday environments like a busy street bombard our ears with information. Yet most of the time, the human brain quickly and effortlessly makes sense of this information in a process known as auditory scene analysis. According to one popular theory, the brain is particularly sensitive to regularly repeating features in sensory signals, and uses those regularities to guide scene analysis. Indeed, many biological sounds contain such regularities, like the pitter-patter of footsteps or the fluttering of bird wings. In most previous studies that investigated whether regularity guides auditory scene analysis in humans, listeners attended to one sound stream that repeated slowly. Thus, it was unclear how regularity might benefit scene analysis in more realistic settings that feature many sounds that quickly change over time. Sohoglu and Chait presented listeners with cluttered, artificial auditory scenes comprised of several sources of sound. If the scenes contained regularly repeating sound sources, the listeners were better able to detect new sounds that appeared partway through the scenes. This shows that auditory scene analysis benefits from sound regularity. To understand the neurobiological basis of this effect, Sohoglu and Chait also recorded the brain activity of the listeners using a non-invasive technique called magnetoencephalography. This activity increased when the sound scenes featured regularly repeating sounds. It therefore appears that the brain prioritized the repeating sounds, and this improved the ability of the listeners to detect new sound sources. When the listeners actively focused on listening to the regular sounds, their brain response to new sounds occurred later than seen in volunteers who were not actively listening to the scene. This was unexpected as delayed brain responses are not usually associated with active focusing. However, this effect can be explained if active focusing increases the expectation of new sounds appearing, because previous research has shown that expectation reduces brain responses. The experiments performed by Sohoglu and Chait used a relatively simple form of sound regularity (tone pips repeating at equal time intervals). Future work will investigate more complex forms of regularity to understand the kinds of sensory patterns to which the brain is sensitive.},
  journaltitle = {eLife},
  urldate = {2016-09-28},
  date = {2016-09-07},
  pages = {e19113},
  keywords = {Attention,predictive coding,magnetoencephalography,surprise,change detection,Human,scene analysis},
  author = {Sohoglu, Ediz and Chait, Maria},
  file = {D\:\\Sauve\\Zotero\\storage\\4MSU6BCP\\Sohoglu and Chait - 2016 - Detecting and representing predictable structure d.pdf;D\:\\Sauve\\Zotero\\storage\\5ZTB78QH\\e19113.html},
  eprinttype = {pmid},
  eprint = {27602577}
}

@article{nixCombinedEstimationSpectral2007,
  title = {Combined Estimation of Spectral Envelopes and Sound Source Direction of Concurrent Voices by Multidimensional Statistical Filtering},
  volume = {15},
  url = {http://ieeexplore.ieee.org/abstract/document/4100691/},
  number = {3},
  journaltitle = {IEEE transactions on audio, speech, and language processing},
  date = {2007},
  pages = {995--1008},
  author = {Nix, Johannes and Hohmann, Volker},
  file = {D\:\\Sauve\\Zotero\\storage\\MKIWEVPB\\4100691.html}
}

@thesis{maAuditoryStreamingBehavior2011,
  location = {{College Park, MD}},
  title = {Auditory {{Streaming}}: {{Behavior}}, {{Physiology}}, and {{Modeling}}.},
  institution = {{University of Maryland}},
  type = {doctoral},
  date = {2011},
  author = {Ma, L.}
}

@article{krishnanSegregatingComplexSound2014,
  title = {Segregating Complex Sound Sources through Temporal Coherence},
  volume = {10},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003985},
  number = {12},
  journaltitle = {PLoS computational biology},
  date = {2014},
  pages = {e1003985},
  author = {Krishnan, Lakshmi and Elhilali, Mounya and Shamma, Shihab},
  file = {D\:\\Sauve\\Zotero\\storage\\VQWXNDGX\\article.html}
}

@article{wangOscillatoryCorrelationModel2008,
  title = {An Oscillatory Correlation Model of Auditory Streaming},
  volume = {2},
  url = {http://link.springer.com/article/10.1007/s11571-007-9035-8},
  number = {1},
  journaltitle = {Cognitive neurodynamics},
  date = {2008},
  pages = {7--19},
  author = {Wang, DeLiang and Chang, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\3RQPJQP7\\s11571-007-9035-8.html;D\:\\Sauve\\Zotero\\storage\\B9TK52X3\\PMC2289253.html}
}

@article{wangPrimitiveAuditorySegregation1996,
  title = {Primitive Auditory Segregation Based on Oscillatory Correlation},
  volume = {20},
  url = {http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog2003_3/full},
  number = {3},
  journaltitle = {Cognitive Science},
  date = {1996},
  pages = {409--456},
  author = {Wang, DeLiang},
  file = {D\:\\Sauve\\Zotero\\storage\\8S5C975A\\full.html}
}

@article{wangSeparationSpeechInterfering1999,
  title = {Separation of Speech from Interfering Sounds Based on Oscillatory Correlation},
  volume = {10},
  url = {http://ieeexplore.ieee.org/abstract/document/761727/},
  number = {3},
  journaltitle = {IEEE transactions on neural networks},
  date = {1999},
  pages = {684--697},
  author = {Wang, DeLiang L. and Brown, Guy J.},
  file = {D\:\\Sauve\\Zotero\\storage\\QS82MPXE\\Wang and Brown - 1999 - Separation of speech from interfering sounds based.pdf;D\:\\Sauve\\Zotero\\storage\\SPMBBSTX\\761727.html}
}

@article{pichevarMonophonicSoundSource2007,
  title = {Monophonic Sound Source Separation with an Unsupervised Network of Spiking Neurones},
  volume = {71},
  url = {http://www.sciencedirect.com/science/article/pii/S0925231207002226},
  number = {1},
  journaltitle = {Neurocomputing},
  date = {2007},
  pages = {109--120},
  author = {Pichevar, Ramin and Rouat, Jean},
  file = {D\:\\Sauve\\Zotero\\storage\\A52AIWXN\\Pichevar and Rouat - 2007 - Monophonic sound source separation with an unsuper.pdf;D\:\\Sauve\\Zotero\\storage\\JMXSAMPA\\S0925231207002226.html}
}

@article{hansenIfYouHave2016,
  title = {" {{If You Have}} to {{Ask}}, {{You}}'ll {{Never Know}}": {{Effects}} of {{Specialised Stylistic Expertise}} on {{Predictive Processing}} of {{Music}}},
  volume = {11},
  url = {http://journals.plos.org/plosone/article?id=info%3Adoi/10.1371/journal.pone.0163584},
  shorttitle = {" {{If You Have}} to {{Ask}}, {{You}}'ll {{Never Know}}"},
  number = {10},
  journaltitle = {PloS one},
  date = {2016},
  pages = {e0163584},
  author = {Hansen, Niels Chr and Vuust, Peter and Pearce, Marcus},
  file = {D\:\\Sauve\\Zotero\\storage\\RF99RX6N\\article.html}
}

@inproceedings{sauveEffectMusicalTraining2014,
  location = {{London}},
  title = {The {{Effect}} of {{Musical Training}} on {{Auditory Grouping}}},
  eventtitle = {{{SysMus}} 2014},
  booktitle = {Proceedings of the {{Seventh International Conference}} of {{Students}} of {{Systematic Musicology}}},
  date = {2014},
  author = {Sauvé, Sarah and Stewart, Lauren and Pearce, M. T.}
}

@article{gigerenzerHomoHeuristicusWhy2009,
  title = {Homo Heuristicus: {{Why}} Biased Minds Make Better Inferences},
  volume = {1},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1756-8765.2008.01006.x/full},
  shorttitle = {Homo Heuristicus},
  number = {1},
  journaltitle = {Topics in Cognitive Science},
  date = {2009},
  pages = {107--143},
  author = {Gigerenzer, Gerd and Brighton, Henry},
  file = {D\:\\Sauve\\Zotero\\storage\\GXE3E7NT\\full.html}
}

@article{bakerPerceptionLeitmotivesRichard2017,
  title = {Perception of {{Leitmotives}} in {{Richard Wagner}}'s {{Der Ring}} Des {{Nibelungen}}},
  volume = {8},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5415611/},
  journaltitle = {Frontiers in Psychology},
  date = {2017},
  author = {Baker, David J. and Müllensiefen, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\2PIQKCBU\\PMC5415611.html}
}

@incollection{mullensiefenRecognitionLeitmotivesRichard2016,
  title = {Recognition of Leitmotives in {{Richard Wagner}}’s Music: {{An}} Item Response Theory Approach},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-25226-1_40},
  shorttitle = {Recognition of Leitmotives in {{Richard Wagner}}’s Music},
  booktitle = {Analysis of {{Large}} and {{Complex Data}}},
  publisher = {{Springer}},
  date = {2016},
  pages = {473--483},
  author = {Müllensiefen, Daniel and Baker, David and Rhodes, Christophe and Crawford, Tim and Dreyfus, Laurence},
  file = {D\:\\Sauve\\Zotero\\storage\\BTH9PJT3\\Müllensiefen et al. - 2016 - Recognition of leitmotives in Richard Wagner’s mus.pdf;D\:\\Sauve\\Zotero\\storage\\NK37R2X4\\978-3-319-25226-1_40.html}
}

@article{mcdermottMusicalIntervalsRelative2010,
  title = {Musical Intervals and Relative Pitch: {{Frequency}} Resolution, Not Interval Resolution, Is Special},
  volume = {128},
  url = {http://asa.scitation.org/doi/abs/10.1121/1.3478785},
  shorttitle = {Musical Intervals and Relative Pitch},
  number = {4},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {2010},
  pages = {1943--1951},
  author = {McDermott, Josh H. and Keebler, Michael V. and Micheyl, Christophe and Oxenham, Andrew J.},
  file = {D\:\\Sauve\\Zotero\\storage\\8AN68AHH\\McDermott et al. - 2010 - Musical intervals and relative pitch Frequency re.pdf;D\:\\Sauve\\Zotero\\storage\\CIQCITAQ\\1.html}
}

@article{schmidUnifiedSocioCognitiveFramework2016,
  title = {Toward a {{Unified Socio}}-{{Cognitive Framework}} for {{Salience}} in {{Language}}},
  volume = {7},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4974243/},
  journaltitle = {Frontiers in Psychology},
  date = {2016},
  author = {Schmid, Hans-Jörg and Günther, Franziska},
  file = {D\:\\Sauve\\Zotero\\storage\\NQSP9N28\\PMC4974243.html}
}

@article{jaegerWhatHeckSalience2016,
  title = {What the Heck Is Salience? {{How}} Predictive Language Processing Contributes to Sociolinguistic Perception},
  volume = {7},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4971435/},
  shorttitle = {What the Heck Is Salience?},
  journaltitle = {Frontiers in Psychology},
  date = {2016},
  author = {Jaeger, T. Florian and Weatherholtz, Kodi},
  file = {D\:\\Sauve\\Zotero\\storage\\4BDP3JTV\\PMC4971435.html}
}

@online{AuerSubjectiveObjective,
  title = {Auer: {{Subjective}} and Objective Parameters Determining... - {{Google Scholar}}},
  url = {https://scholar.google.com/scholar_lookup?author=P.+Auer&author=B.+Birgit&author=G.+Beate+&publication_year=1998&title=Subjective+and+objective+parameters+determining+%E2%80%98salience%E2%80%99+in+long-term+dialect+accommodation&journal=J.+Sociolinguist.&volume=2&pages=163-187},
  urldate = {2017-06-09},
  file = {D\:\\Sauve\\Zotero\\storage\\VENFWGDQ\\scholar_lookup.html}
}

@article{blumenthal-dramePerceptualLinguisticSalience2017,
  title = {Perceptual Linguistic Salience: {{Modeling}} Causes and Consequences},
  volume = {8},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5360726/},
  shorttitle = {Perceptual Linguistic Salience},
  journaltitle = {Frontiers in Psychology},
  date = {2017},
  author = {Blumenthal-Dramé, Alice and Hanulíková, Adriana and Kortmann, Bernd},
  file = {D\:\\Sauve\\Zotero\\storage\\PDW7MR4G\\PMC5360726.html}
}

@article{horstmannPerceptualSalienceCaptures2016,
  title = {Perceptual Salience Captures the Eyes on a Surprise Trial},
  volume = {78},
  url = {http://link.springer.com/article/10.3758/s13414-016-1102-y},
  number = {7},
  journaltitle = {Attention, Perception, \& Psychophysics},
  date = {2016},
  pages = {1889--1900},
  author = {Horstmann, Gernot and Becker, Stefanie and Ernst, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\32KNCZBG\\s13414-016-1102-y.html}
}

@article{parkhurstModelingRoleSalience2002,
  title = {Modeling the Role of Salience in the Allocation of Overt Visual Attention},
  volume = {42},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698901002504},
  number = {1},
  journaltitle = {Vision research},
  date = {2002},
  pages = {107--123},
  author = {Parkhurst, Derrick and Law, Klinton and Niebur, Ernst},
  file = {D\:\\Sauve\\Zotero\\storage\\8B497QXA\\S0042698901002504.html}
}

@article{hoffmanSalienceVisualParts1997,
  title = {Salience of Visual Parts},
  volume = {63},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027796007913},
  number = {1},
  journaltitle = {Cognition},
  date = {1997},
  pages = {29--78},
  author = {Hoffman, Donald D. and Singh, Manish},
  file = {D\:\\Sauve\\Zotero\\storage\\2PD2EC32\\S0010027796007913.html}
}

@article{finkHemisphericAsymmetriesGlobal1998,
  title = {Hemispheric Asymmetries in Global⧹ Local Processing Are Modulated by Perceptual Salience},
  volume = {37},
  url = {http://www.sciencedirect.com/science/article/pii/S0028393298000475},
  number = {1},
  journaltitle = {Neuropsychologia},
  date = {1998},
  pages = {31--40},
  author = {Fink, G. R. and Marshall, J. C. and Halligan, P. W. and Dolan, R. J.},
  file = {D\:\\Sauve\\Zotero\\storage\\7BGSDD6H\\Fink et al. - 1998 - Hemispheric asymmetries in global⧹ local processin.pdf;D\:\\Sauve\\Zotero\\storage\\AXD79IRA\\S0028393298000475.html}
}

@article{ellisSelectiveAttentionTransfer2006,
  title = {Selective Attention and Transfer Phenomena in {{L2}} Acquisition: {{Contingency}}, Cue Competition, Salience, Interference, Overshadowing, Blocking, and Perceptual Learning},
  volume = {27},
  url = {http://applij.oxfordjournals.org/content/27/2/164.short},
  shorttitle = {Selective Attention and Transfer Phenomena in {{L2}} Acquisition},
  number = {2},
  journaltitle = {Applied Linguistics},
  date = {2006},
  pages = {164--194},
  author = {Ellis, Nick C.},
  file = {D\:\\Sauve\\Zotero\\storage\\96RX6VSN\\Ellis - 2006 - Selective attention and transfer phenomena in L2 a.pdf;D\:\\Sauve\\Zotero\\storage\\TDPMNK62\\Selective-Attention-and-Transfer-Phenomena-in-L2.html}
}

@article{prudenBirthWordsTenMonthOlds2006,
  title = {The {{Birth}} of {{Words}}: {{Ten}}-{{Month}}-{{Olds Learn Words Through Perceptual Salience}}},
  volume = {77},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8624.2006.00869.x/full},
  shorttitle = {The {{Birth}} of {{Words}}},
  number = {2},
  journaltitle = {Child development},
  date = {2006},
  pages = {266--280},
  author = {Pruden, Shannon M. and Hirsh-Pasek, Kathy and Golinkoff, Roberta Michnick and Hennon, Elizabeth A.},
  file = {D\:\\Sauve\\Zotero\\storage\\289ACWPD\\PMC4621011.html;D\:\\Sauve\\Zotero\\storage\\WVSSD8XV\\full.html}
}

@book{breitkopf371VierstimmigeChoralgesange1875,
  location = {{Leipzig}},
  title = {371 Vierstimmige {{Choralgesänge}} von {{Johann Sebastian Bach}}.},
  edition = {4th edition. Plate Number: V.A.10. Retypeset c. 1915 as Edition Breitkopf 10. Reprinted by Associated Music Publishers, Inc., New York [c. 1940].},
  publisher = {{Alfred Dörffel}},
  date = {1875},
  author = {Breitkopf and Härtel}
}

@incollection{conklinRepresentationDiscoveryVertical2002,
  title = {Representation and Discovery of Vertical Patterns in Music},
  url = {http://link.springer.com/chapter/10.1007/3-540-45722-4_5},
  booktitle = {Music and Artificial Intelligence},
  publisher = {{Springer}},
  date = {2002},
  pages = {32--42},
  author = {Conklin, Darrell},
  file = {D\:\\Sauve\\Zotero\\storage\\9XJAKR6W\\Conklin - 2002 - Representation and discovery of vertical patterns .pdf;D\:\\Sauve\\Zotero\\storage\\2IS3PM6R\\3-540-45722-4_5.html}
}

@article{critchleyWillStudiesMacaque2012,
  title = {Will Studies of Macaque Insula Reveal the Neural Mechanisms of Self-Awareness?},
  volume = {74},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627312003741},
  number = {3},
  journaltitle = {Neuron},
  date = {2012},
  pages = {423--426},
  author = {Critchley, Hugo and Seth, Anil},
  file = {D\:\\Sauve\\Zotero\\storage\\4U2U3MTH\\S0896627312003741.html}
}

@article{sethInteroceptivePredictiveCoding2011,
  title = {An Interoceptive Predictive Coding Model of Conscious Presence},
  volume = {2},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3254200/},
  journaltitle = {Frontiers in psychology},
  date = {2011},
  author = {Seth, Anil K. and Suzuki, Keisuke and Critchley, Hugo D.},
  file = {D\:\\Sauve\\Zotero\\storage\\FMQWA2S3\\PMC3254200.html}
}

@article{paulusInsularViewAnxiety2006,
  title = {An Insular View of Anxiety},
  volume = {60},
  url = {http://www.sciencedirect.com/science/article/pii/S0006322306004768},
  number = {4},
  journaltitle = {Biological psychiatry},
  date = {2006},
  pages = {383--387},
  author = {Paulus, Martin P. and Stein, Murray B.},
  file = {D\:\\Sauve\\Zotero\\storage\\HBUI3V55\\fulltext.html}
}

@article{frithExplainingDelusionsControl2012,
  title = {Explaining Delusions of Control: {{The}} Comparator Model 20years On},
  volume = {21},
  url = {http://www.sciencedirect.com/science/article/pii/S1053810011001632},
  shorttitle = {Explaining Delusions of Control},
  number = {1},
  journaltitle = {Consciousness and cognition},
  date = {2012},
  pages = {52--54},
  author = {Frith, Chris},
  file = {D\:\\Sauve\\Zotero\\storage\\WSS7XSPK\\S1053810011001632.html}
}

@article{silversteinSchizophreniarelatedPhenomenaThat2013,
  title = {Schizophrenia-Related Phenomena That Challenge Prediction Error as the Basis of Cognitive Functioning},
  volume = {36},
  number = {3},
  journaltitle = {The Behavioral and Brain Sciences},
  date = {2013},
  pages = {49-50},
  author = {Silverstein, Steven}
}

@article{alinkStimulusPredictabilityReduces2010,
  title = {Stimulus Predictability Reduces Responses in Primary Visual Cortex},
  volume = {30},
  url = {http://www.jneurosci.org/content/30/8/2960.short},
  number = {8},
  journaltitle = {Journal of Neuroscience},
  date = {2010},
  pages = {2960--2966},
  author = {Alink, Arjen and Schwiedrzik, Caspar M. and Kohler, Axel and Singer, Wolf and Muckli, Lars},
  file = {D\:\\Sauve\\Zotero\\storage\\MSJX4U6X\\2960.html;D\:\\Sauve\\Zotero\\storage\\Z597R4P6\\2960.html}
}

@article{murrayShapePerceptionReduces2002,
  title = {Shape Perception Reduces Activity in Human Primary Visual Cortex},
  volume = {99},
  url = {http://www.pnas.org/content/99/23/15164.short},
  number = {23},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2002},
  pages = {15164--15169},
  author = {Murray, Scott O. and Kersten, Daniel and Olshausen, Bruno A. and Schrater, Paul and Woods, David L.},
  file = {D\:\\Sauve\\Zotero\\storage\\R7UWET8I\\15164.html;D\:\\Sauve\\Zotero\\storage\\WDSM7AP4\\15164.html}
}

@article{summerfieldNeuralRepetitionSuppression2008,
  title = {Neural Repetition Suppression Reflects Fulfilled Perceptual Expectations},
  volume = {11},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2747248/},
  number = {9},
  journaltitle = {Nature neuroscience},
  date = {2008},
  pages = {1004},
  author = {Summerfield, Christopher and Monti, Jim MP and Trittschuh, Emily H. and Mesulam, M.-Marsel and Egner, Tobias},
  file = {D\:\\Sauve\\Zotero\\storage\\MHPV8D5I\\PMC2747248.html}
}

@article{rao16PredictiveCoding2002,
  title = {16 {{Predictive Coding}}, {{Cortical Feedback}}, and {{Spike}}-{{Timing Dependent Plasticity}}},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=mzBlvComcqwC&oi=fnd&pg=PA297&dq=rao+sejnowski+predictive+coding&ots=rn_cyhFvEd&sig=z1FqEwMIW5vYPw2kOSMv6ADigJM},
  journaltitle = {Probabilistic models of the brain},
  date = {2002},
  pages = {297},
  author = {Rao, Rajesh PN and Sejnowski, Terrence J.},
  file = {D\:\\Sauve\\Zotero\\storage\\JUPIUFQR\\books.html}
}

@article{jeheePredictiveFeedbackCan2009,
  title = {Predictive Feedback Can Account for Biphasic Responses in the Lateral Geniculate Nucleus},
  volume = {5},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000373},
  number = {5},
  journaltitle = {PLoS Comput Biol},
  date = {2009},
  pages = {e1000373},
  author = {Jehee, Janneke FM and Ballard, Dana H.},
  file = {D\:\\Sauve\\Zotero\\storage\\J2VAXTDN\\article.html}
}

@article{egnerExpectationSurpriseDetermine2010,
  title = {Expectation and Surprise Determine Neural Population Responses in the Ventral Visual Stream},
  volume = {30},
  url = {http://www.jneurosci.org/content/30/49/16601.short},
  number = {49},
  journaltitle = {Journal of Neuroscience},
  date = {2010},
  pages = {16601--16608},
  author = {Egner, Tobias and Monti, Jim M. and Summerfield, Christopher},
  file = {D\:\\Sauve\\Zotero\\storage\\JRQG8EJE\\16601.html;D\:\\Sauve\\Zotero\\storage\\ZIW795NR\\16601.html}
}

@article{baddeleyWorkingMemory1974,
  title = {Working Memory},
  volume = {8},
  url = {http://www.sciencedirect.com/science/article/pii/S0079742108604521},
  journaltitle = {Psychology of learning and motivation},
  date = {1974},
  pages = {47--89},
  author = {Baddeley, Alan D. and Hitch, Graham},
  file = {D\:\\Sauve\\Zotero\\storage\\RDSU9JVQ\\Baddeley and Hitch - 1974 - Working memory.pdf;D\:\\Sauve\\Zotero\\storage\\AJAFI7QA\\S0079742108604521.html}
}

@inproceedings{broadbentRecentAdvancesUnderstanding1983,
  title = {Recent Advances in Understanding Performance in Noise},
  booktitle = {Noise as a Public Health Problem: {{Proceedings}} of the {{Fourth International Congress}}},
  date = {1983},
  pages = {719--738},
  author = {Broadbent, D. E.}
}

@inproceedings{kirkMotivatingStrokeRehabilitation2016,
  title = {Motivating {{Stroke Rehabilitation Through Music}}: {{A Feasibility Study Using Digital Musical Instruments}} in the {{Home}}},
  url = {http://dl.acm.org/citation.cfm?id=2858376},
  shorttitle = {Motivating {{Stroke Rehabilitation Through Music}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  date = {2016},
  pages = {1781--1785},
  author = {Kirk, Pedro and Grierson, Mick and Bodak, Rebeka and Ward, Nick and Brander, Fran and Kelly, Kate and Newman, Nicholas and Stewart, Lauren},
  file = {D\:\\Sauve\\Zotero\\storage\\3NBQ8QMP\\Kirk et al. - 2016 - Motivating Stroke Rehabilitation Through Music A .pdf;D\:\\Sauve\\Zotero\\storage\\NIK6AA8Z\\citation.html}
}

@article{andersonDynamicAuditorycognitiveSystem2013,
  title = {A Dynamic Auditory-Cognitive System Supports Speech-in-Noise Perception in Older Adults},
  volume = {300},
  url = {http://www.sciencedirect.com/science/article/pii/S0378595513000749},
  journaltitle = {Hearing research},
  date = {2013},
  pages = {18--32},
  author = {Anderson, Samira and White-Schwoch, Travis and Parbery-Clark, Alexandra and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\5S2NC96F\\S0378595513000749.html;D\:\\Sauve\\Zotero\\storage\\6KXACA9A\\PMC3658829.html;D\:\\Sauve\\Zotero\\storage\\7FXKHC7V\\S0378595513000749.html;D\:\\Sauve\\Zotero\\storage\\9RBUWMC2\\PMC3658829.html;D\:\\Sauve\\Zotero\\storage\\9SL5SJ5L\\S0378595513000749.html;D\:\\Sauve\\Zotero\\storage\\A5PWH3WF\\PMC3658829.html}
}

@article{andersonNeuralBasisSpeechinnoise2011,
  title = {A Neural Basis of Speech-in-Noise Perception in Older Adults},
  volume = {32},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3189261/},
  number = {6},
  journaltitle = {Ear and hearing},
  date = {2011},
  pages = {750},
  author = {Anderson, Samira and Parbery-Clark, Alexandra and Yi, Han-Gyol and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\2EZJYJ83\\PMC3189261.html;D\:\\Sauve\\Zotero\\storage\\KREPMA5G\\PMC3189261.html;D\:\\Sauve\\Zotero\\storage\\VFWUJJIZ\\PMC3189261.html}
}

@article{salthouseDecomposingAdultAge1991,
  title = {Decomposing Adult Age Differences in Working Memory.},
  volume = {27},
  url = {http://psycnet.apa.org/journals/dev/27/5/763/},
  number = {5},
  journaltitle = {Developmental psychology},
  date = {1991},
  pages = {763},
  author = {Salthouse, Timothy A. and Babcock, Renee L.},
  file = {D\:\\Sauve\\Zotero\\storage\\KEF78P3K\\763.html}
}

@book{richardsonWorkingMemoryHuman1996,
  title = {Working Memory and Human Cognition},
  volume = {3},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=ikpnDAAAQBAJ&oi=fnd&pg=PP10&dq=age+memory&ots=RPSLc7XgrV&sig=SeSge135ojQonQFBSAMtNjY0McA},
  publisher = {{Oxford University Press on Demand}},
  date = {1996},
  author = {Richardson, John TE},
  file = {D\:\\Sauve\\Zotero\\storage\\QCHCCGD9\\books.html}
}

@book{craikFunctionalAccountAge1986,
  title = {A Functional Account of Age Differences in Memory},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=ajglDwAAQBAJ&oi=fnd&pg=PA147&dq=age+memory&ots=R4DUCYOKz8&sig=QOvxPWSP5uJKeGW4kN42RiZhc6A},
  date = {1986},
  author = {Craik, Fergus IM and Klix, F. and Hagendorf, H.},
  file = {D\:\\Sauve\\Zotero\\storage\\JX92JIFQ\\books.html}
}

@article{rimmeleAgerelatedChangesUse2012,
  langid = {english},
  title = {Age-Related Changes in the Use of Regular Patterns for Auditory Scene Analysis},
  volume = {289},
  issn = {1878-5891},
  doi = {10.1016/j.heares.2012.04.006},
  abstract = {A recent approach to auditory processing suggests a close relationship of regularity processing in auditory sensory memory (ASM) and stream segregation, such that within-stream regularities can be used to stabilize stream segregation. The present study investigates age-related changes in how regular patterns are used for auditory scene analysis (ASA), when the stream containing the regularity is attended or unattended. In order to accomplish an intensity level deviant detection task, participants had to segregate the task-relevant pure tone sequence from an irrelevant distractor pure tone sequence, which randomly varied in level. In three conditions a simple spectro-temporal regularity ("Isochronous"), a more complex spectro-temporal regularity ("Rhythmic"), or no regularity ("Random") was embedded in either the attended target sequence (Experiment 1), or the unattended distractor sequence (Experiment 2). When the sequence containing the regularity was attended, older participants showed a similar increase of performance to younger adults in the conditions with regular patterns ("Isochronous" and "Rhythmic") compared to the "Random" condition. In contrast, when the sequence containing the regularity was unattended, older adults showed a specific performance decline compared to younger adults in the "Isochronous" condition. Results suggest a link between impaired automatic processing of regularities in ASM, and age-related deficits in the use of regular patterns for ASA.},
  number = {1-2},
  journaltitle = {Hearing Research},
  shortjournal = {Hear. Res.},
  date = {2012-07},
  pages = {98-107},
  keywords = {Auditory Perception,Cues,time perception,Acoustic Stimulation,humans,aging,Adult,Auditory Threshold,Middle Aged,Female,Male,Time Factors,Psychoacoustics,Analysis of Variance,Young Adult,Age Factors,Aged,Noise,Perceptual Masking,Audiometry; Pure-Tone,Pattern Recognition; Physiological},
  author = {Rimmele, Johanna and Schröger, Erich and Bendixen, Alexandra},
  file = {D\:\\Sauve\\Zotero\\storage\\VABVGBJE\\Rimmele et al. - 2012 - Age-related changes in the use of regular patterns.pdf;D\:\\Sauve\\Zotero\\storage\\8N9E7ZAP\\S0378595512000925.html;D\:\\Sauve\\Zotero\\storage\\EPANQMJX\\S0378595512000925.html},
  eprinttype = {pmid},
  eprint = {22543088}
}

@article{charnessAgingProblemsolvingPerformance1985,
  title = {Aging and Problem-Solving Performance},
  url = {https://psy.fsu.edu/~charness/preprints/agehp/},
  journaltitle = {Aging and human performance},
  date = {1985},
  pages = {225--259},
  author = {Charness, Neil},
  file = {D\:\\Sauve\\Zotero\\storage\\ZFVF6QX6\\agehp.html}
}

@article{halpernEffectsAgingMusical1996,
  title = {Effects of Aging and Musical Experience on the Representation of Tonal Hierarchies.},
  volume = {11},
  url = {http://psycnet.apa.org/journals/pag/11/2/235/},
  number = {2},
  journaltitle = {Psychology and Aging},
  date = {1996},
  pages = {235},
  author = {Halpern, Andrea R. and Kwak, SeYeul and Bartlett, James C. and Dowling, W. Jay},
  file = {D\:\\Sauve\\Zotero\\storage\\SR24NEHM\\Halpern et al. - 1996 - Effects of aging and musical experience on the rep.pdf;D\:\\Sauve\\Zotero\\storage\\TI457QX8\\235.html}
}

@article{halpernAgingExperienceRecognition1995,
  title = {Aging and Experience in the Recognition of Musical Transpositions.},
  volume = {10},
  url = {http://psycnet.apa.org/journals/pag/10/3/325/},
  number = {3},
  journaltitle = {Psychology and Aging},
  date = {1995},
  pages = {325},
  author = {Halpern, Andrea R. and Bartlett, James C. and Dowling, W. Jay},
  file = {D\:\\Sauve\\Zotero\\storage\\U6RSBFHD\\Halpern et al. - 1995 - Aging and experience in the recognition of musical.pdf;D\:\\Sauve\\Zotero\\storage\\6QCND2VP\\325.html}
}

@article{bartlettRecognitionFamiliarUnfamiliar1995,
  title = {Recognition of Familiar and Unfamiliar Melodies in Normal Aging and {{Alzheimer}}’s Disease},
  volume = {23},
  url = {http://www.springerlink.com/index/28666614362Q8Q8U.pdf},
  number = {5},
  journaltitle = {Memory \& Cognition},
  date = {1995},
  pages = {531--546},
  author = {Bartlett, James C. and Halpern, Andrea R. and Dowling, W. Jay},
  file = {D\:\\Sauve\\Zotero\\storage\\HEABJPIN\\Bartlett et al. - 1995 - Recognition of familiar and unfamiliar melodies in.pdf;D\:\\Sauve\\Zotero\\storage\\UCFUS5UF\\10.html}
}

@article{haysContributionMusicPositive2002,
  title = {The Contribution of Music to Positive Aging: {{A}} Review},
  volume = {7},
  url = {http://www.springerlink.com/index/qk4834x5j703601w.pdf},
  shorttitle = {The Contribution of Music to Positive Aging},
  number = {3},
  journaltitle = {Journal of Aging and Identity},
  date = {2002},
  pages = {165--175},
  author = {Hays, Terrence and Bright, Ruth and Minichiello, Victor},
  file = {D\:\\Sauve\\Zotero\\storage\\64WNNFAH\\Hays et al. - 2002 - The contribution of music to positive aging A rev.pdf;D\:\\Sauve\\Zotero\\storage\\GZMZUKMN\\10.html}
}

@article{defflerContextualInformationMemory2011,
  title = {Contextual Information and Memory for Unfamiliar Tunes in Older and Younger Adults.},
  volume = {26},
  url = {http://psycnet.apa.org/journals/pag/26/4/900/},
  number = {4},
  journaltitle = {Psychology and aging},
  date = {2011},
  pages = {900},
  author = {Deffler, Samantha A. and Halpern, Andrea R.},
  file = {D\:\\Sauve\\Zotero\\storage\\7JAFX673\\900.html}
}

@article{andrewsIdentificationSpeededSlowed1998,
  title = {Identification of Speeded and Slowed Familiar Melodies by Younger, Middle-Aged, and Older Musicians and Nonmusicians.},
  volume = {13},
  url = {http://psycnet.apa.org/journals/pag/13/3/462/},
  number = {3},
  journaltitle = {Psychology and aging},
  date = {1998},
  pages = {462},
  author = {Andrews, Melinda W. and Dowling, W. Jay and Bartlett, James C. and Halpern, Andrea R.},
  file = {D\:\\Sauve\\Zotero\\storage\\M5C9X2UX\\Andrews et al. - 1998 - Identification of speeded and slowed familiar melo.pdf;D\:\\Sauve\\Zotero\\storage\\2W4BUNU6\\462.html}
}

@article{mensSpeechUnderstandingNoise2011,
  title = {Speech Understanding in Noise with an Eyeglass Hearing Aid: Asymmetric Fitting and the Head Shadow Benefit of Anterior Microphones},
  volume = {50},
  url = {http://www.tandfonline.com/doi/abs/10.3109/14992027.2010.521199},
  shorttitle = {Speech Understanding in Noise with an Eyeglass Hearing Aid},
  number = {1},
  journaltitle = {International journal of audiology},
  date = {2011},
  pages = {27--33},
  author = {Mens, Lucas HM},
  file = {D\:\\Sauve\\Zotero\\storage\\G9BT8XBX\\14992027.2010.html}
}

@article{kreismanImprovementsSpeechUnderstanding2010,
  title = {Improvements in Speech Understanding with Wireless Binaural Broadband Digital Hearing Instruments in Adults with Sensorineural Hearing Loss},
  volume = {14},
  url = {http://journals.sagepub.com/doi/abs/10.1177/1084713810364396},
  number = {1},
  journaltitle = {Trends in amplification},
  date = {2010},
  pages = {3--11},
  author = {Kreisman, Brian M. and Mazevski, Annette G. and Schum, Donald J. and Sockalingam, Ravichandran},
  file = {D\:\\Sauve\\Zotero\\storage\\88UU6RHA\\1084713810364396.html;D\:\\Sauve\\Zotero\\storage\\QHPQGV7F\\PMC4111507.html}
}

@article{mcadamsHearingMusicalStreams1979,
  eprinttype = {jstor},
  eprint = {4617866},
  title = {Hearing Musical Streams},
  journaltitle = {Computer Music Journal},
  date = {1979},
  pages = {26--60},
  author = {McAdams, Stephen and Bregman, Albert},
  file = {D\:\\Sauve\\Zotero\\storage\\QW9SQ62D\\McAdams and Bregman - 1979 - Hearing musical streams.pdf;D\:\\Sauve\\Zotero\\storage\\6C2QXDXV\\4617866.html}
}

@unpublished{benetosASyMMuSWorkshopAudioSymbolic2015,
  venue = {{London, UK}},
  title = {{{ASyMMuS Workshop}} on {{Audio}}-{{Symbolic Music Similarity Modelling}}},
  type = {Workshop},
  date = {2015-07-08},
  author = {Benetos, Emmanouil}
}

@article{harrisonApplyingModernPsychometric2017,
  title = {Applying Modern Psychometric Techniques to Melodic Discrimination Testing: {{Item}} Response Theory, Computerised Adaptive Testing, and Automatic Item Generation.},
  volume = {In press.},
  journaltitle = {Scientific Reports},
  date = {2017},
  author = {Harrison, P. M. C. and Müllensiefen, Daniel and Collins, T.}
}

@inproceedings{deliegeSimilarityProcessesCategorisation1997,
  title = {Similarity in Processes of Categorisation: {{Imprint}} Formation as a Prototype Effect in Music Listening},
  url = {http://www.dai.ed.ac.uk/conferences/simcat/del.htm},
  shorttitle = {Similarity in Processes of Categorisation},
  booktitle = {Proceedings of the {{Interdisciplinary}} Workshop on Similarity and Categorisation},
  publisher = {{University of Edinburgh Edinburgh, UK}},
  date = {1997},
  pages = {59--65},
  author = {Deliège, Irene},
  file = {D\:\\Sauve\\Zotero\\storage\\DTF688HQ\\del.html}
}

@article{deliegeSimilarityRelationsListening2007,
  title = {Similarity Relations in Listening to Music: {{How}} Do They Come into Play?},
  volume = {11},
  url = {http://journals.sagepub.com/doi/abs/10.1177/1029864907011001021},
  shorttitle = {Similarity Relations in Listening to Music},
  issue = {1\_suppl},
  journaltitle = {Musicae Scientiae},
  date = {2007},
  pages = {9--37},
  author = {Deliège, Irène},
  file = {D\:\\Sauve\\Zotero\\storage\\RRP94WSC\\1029864907011001021.html}
}

@article{deliegeSpecialIssueMusic2003,
  title = {Special Issue on Music Similarity},
  journaltitle = {Musica Scientae},
  date = {2003},
  author = {Deliège, I.}
}

@book{deliegeSimilarityPerceptionCategorization2001,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2001.18.3.233},
  title = {Similarity Perception↔ Categorization↔ Cue Abstraction},
  publisher = {{JSTOR}},
  date = {2001},
  author = {Deliège, Irène},
  file = {D\:\\Sauve\\Zotero\\storage\\3VJ6CCUM\\mp.2001.18.3.html}
}

@inproceedings{aucouturierMusicSimilarityMeasures2002,
  title = {Music Similarity Measures: {{What}}'s the Use?},
  url = {http://www.music.mcgill.ca/~ich/classes/mumt614/similarity/AucouturierSimilar.pdf},
  shorttitle = {Music Similarity Measures},
  booktitle = {{{ISMIR}}},
  date = {2002},
  pages = {13--17},
  author = {Aucouturier, Jean-Julien and Pachet, Francois and others},
  file = {D\:\\Sauve\\Zotero\\storage\\55NDSV6D\\Aucouturier et al. - 2002 - Music similarity measures What's the use.pdf}
}

@article{westModelbasedApproachConstructing2007,
  title = {A Model-Based Approach to Constructing Music Similarity Functions},
  volume = {2007},
  url = {http://dl.acm.org/citation.cfm?id=1289110},
  number = {1},
  journaltitle = {EURASIP Journal on Applied Signal Processing},
  date = {2007},
  pages = {149--149},
  author = {West, Kris and Lamere, Paul},
  file = {D\:\\Sauve\\Zotero\\storage\\TKAKEWBB\\West and Lamere - 2007 - A model-based approach to constructing music simil.pdf;D\:\\Sauve\\Zotero\\storage\\ICQW76WF\\citation.html}
}

@inproceedings{slaneyLearningMetricMusic2008,
  title = {Learning a Metric for Music Similarity},
  url = {https://www.slaney.org/malcolm/yahoo/Slaney2008-MusicSimilarityMetricsISMIR.pdf},
  booktitle = {International {{Symposium}} on {{Music Information Retrieval}} ({{ISMIR}})},
  date = {2008},
  author = {Slaney, Malcolm and Weinberger, Kilian and White, William},
  file = {D\:\\Sauve\\Zotero\\storage\\8RQKPP6F\\Slaney et al. - 2008 - Learning a metric for music similarity.pdf}
}

@inproceedings{flexerMIREXMetaanalysisHubness2012,
  title = {A {{MIREX Meta}}-Analysis of {{Hubness}} in {{Audio Music Similarity}}.},
  url = {http://ismir2012.ismir.net/event/papers/175_ISMIR_2012.pdf},
  booktitle = {{{ISMIR}}},
  date = {2012},
  pages = {175--180},
  author = {Flexer, Arthur and Schnitzer, Dominik and Schlüter, Jan},
  file = {D\:\\Sauve\\Zotero\\storage\\3757HDZ6\\Flexer et al. - 2012 - A MIREX Meta-analysis of Hubness in Audio Music Si.pdf}
}

@inproceedings{liContentbasedMusicSimilarity2004,
  title = {Content-Based Music Similarity Search and Emotion Detection},
  volume = {5},
  url = {http://ieeexplore.ieee.org/abstract/document/1327208/},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, 2004. {{Proceedings}}.({{ICASSP}}'04). {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  date = {2004},
  pages = {V--705},
  author = {Li, Tao and Ogihara, Mitsunori},
  file = {D\:\\Sauve\\Zotero\\storage\\FFBKTERU\\Li and Ogihara - 2004 - Content-based music similarity search and emotion .pdf;D\:\\Sauve\\Zotero\\storage\\VFENC5GQ\\1327208.html}
}

@article{bogdanovUnifyingLowlevelHighlevel2011,
  title = {Unifying Low-Level and High-Level Music Similarity Measures},
  volume = {13},
  url = {http://ieeexplore.ieee.org/abstract/document/5728926/},
  number = {4},
  journaltitle = {IEEE Transactions on Multimedia},
  date = {2011},
  pages = {687--701},
  author = {Bogdanov, Dmitry and Serrà, Joan and Wack, Nicolas and Herrera, Perfecto and Serra, Xavier},
  file = {D\:\\Sauve\\Zotero\\storage\\I6WCURKG\\Bogdanov et al. - 2011 - Unifying low-level and high-level music similarity.pdf;D\:\\Sauve\\Zotero\\storage\\KAUTE9J7\\5728926.html}
}

@inproceedings{pohleRhythmGeneralMusic2009,
  title = {On {{Rhythm}} and {{General Music Similarity}}.},
  url = {http://www.cp.jku.at/people/schedl/Research/Publications/pdf/pohle_ismir_2009.pdf},
  booktitle = {{{ISMIR}}},
  date = {2009},
  pages = {525--530},
  author = {Pohle, Tim and Schnitzer, Dominik and Schedl, Markus and Knees, Peter and Widmer, Gerhard},
  file = {D\:\\Sauve\\Zotero\\storage\\757HXDN7\\Pohle et al. - 2009 - On Rhythm and General Music Similarity..pdf}
}

@inproceedings{pampalkImprovementsAudioBasedMusic2005,
  title = {Improvements of {{Audio}}-{{Based Music Similarity}} and {{Genre Classificaton}}.},
  volume = {5},
  url = {http://www.cp.jku.at/research/papers/pampalk_ismir_2005.pdf},
  booktitle = {{{ISMIR}}},
  publisher = {{London, UK}},
  date = {2005},
  pages = {634--637},
  author = {Pampalk, Elias and Flexer, Arthur and Widmer, Gerhard and others},
  file = {D\:\\Sauve\\Zotero\\storage\\H62MDMUC\\Pampalk et al. - 2005 - Improvements of Audio-Based Music Similarity and G.pdf}
}

@article{berenzweigLargescaleEvaluationAcoustic2004,
  title = {A Large-Scale Evaluation of Acoustic and Subjective Music-Similarity Measures},
  volume = {28},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/014892604323112257},
  number = {2},
  journaltitle = {Computer Music Journal},
  date = {2004},
  pages = {63--76},
  author = {Berenzweig, Adam and Logan, Beth and Ellis, Daniel PW and Whitman, Brian},
  file = {D\:\\Sauve\\Zotero\\storage\\AT6ANMKK\\Berenzweig et al. - 2004 - A large-scale evaluation of acoustic and subjectiv.pdf;D\:\\Sauve\\Zotero\\storage\\5GVR65TU\\014892604323112257.html}
}

@inproceedings{mardirossianKeyDistributionsMusical2005,
  title = {Key Distributions as Musical Fingerprints for Similarity Assessment},
  url = {http://ieeexplore.ieee.org/abstract/document/1565887/},
  booktitle = {Multimedia, {{Seventh IEEE International Symposium}} On},
  publisher = {{IEEE}},
  date = {2005},
  pages = {6--pp},
  author = {Mardirossian, Arpi and Chew, Elaine},
  file = {D\:\\Sauve\\Zotero\\storage\\9SQ5XRCM\\1565887.html}
}

@inproceedings{mardirossianMusicSummarizationKey2006,
  title = {Music {{Summarization Via Key Distributions}}: {{Analyses}} of {{Similarity Assessment Across Variations}}.},
  url = {https://pdfs.semanticscholar.org/79a8/096ec06f2a7178c57890e70e0adff22d5e0d.pdf},
  shorttitle = {Music {{Summarization Via Key Distributions}}},
  booktitle = {{{ISMIR}}},
  date = {2006},
  pages = {234--239},
  author = {Mardirossian, Arpi and Chew, Elaine},
  file = {D\:\\Sauve\\Zotero\\storage\\92EZZ2UD\\Mardirossian and Chew - 2006 - Music Summarization Via Key Distributions Analyse.pdf}
}

@article{dowlingImportanceIntervalInformation1981,
  title = {The Importance of Interval Information in Long-Term Memory for Melodies.},
  volume = {1},
  url = {http://psycnet.apa.org/journals/pmu/1/1/30/},
  number = {1},
  journaltitle = {Psychomusicology: A Journal of Research in Music Cognition},
  date = {1981},
  pages = {30},
  author = {Dowling, W. Jay and Bartlett, James C.},
  file = {D\:\\Sauve\\Zotero\\storage\\I6RXVRIC\\Dowling and Bartlett - 1981 - The importance of interval information in long-ter.pdf;D\:\\Sauve\\Zotero\\storage\\5GNDM7IM\\30.html}
}

@article{bartlettRecognitionTransposedMelodies1980,
  title = {Recognition of Transposed Melodies: A Key-Distance Effect in Developmental Perspective.},
  volume = {6},
  url = {http://psycnet.apa.org/journals/xhp/6/3/501/},
  shorttitle = {Recognition of Transposed Melodies},
  number = {3},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {1980},
  pages = {501},
  author = {Bartlett, James C. and Dowling, W. Jay},
  file = {D\:\\Sauve\\Zotero\\storage\\Q5CI3DQH\\Bartlett and Dowling - 1980 - Recognition of transposed melodies a key-distance.pdf;D\:\\Sauve\\Zotero\\storage\\ITDU24MU\\501.html}
}

@article{rollandDiscoveringPatternsMusical1999,
  title = {Discovering Patterns in Musical Sequences},
  volume = {28},
  url = {http://www.tandfonline.com/doi/abs/10.1076/0929-8215(199912)28:04;1-O;FT334},
  number = {4},
  journaltitle = {Journal of New Music Research},
  date = {1999},
  pages = {334--350},
  author = {Rolland, Pierre-Yves},
  file = {D\:\\Sauve\\Zotero\\storage\\TXK5MVI9\\0929-8215(199912)2804\;1-O\;FT334.html}
}

@book{nettlStudyEthnomusicologyThirtyone2010,
  title = {The Study of Ethnomusicology: {{Thirty}}-One Issues and Concepts},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=_vlrrG7HvP4C&oi=fnd&pg=PR3&dq=bruno+nettl&ots=jAI17MgskK&sig=7MuMaASA7RwI15sYITI7cVNQkvc},
  shorttitle = {The Study of Ethnomusicology},
  publisher = {{University of Illinois Press}},
  date = {2010},
  author = {Nettl, Bruno},
  file = {D\:\\Sauve\\Zotero\\storage\\3ZVFMSVB\\Nettl - 2010 - The study of ethnomusicology Thirty-one issues an.pdf;D\:\\Sauve\\Zotero\\storage\\4WTKHGBD\\books.html}
}

@article{margulisAestheticResponsesRepetition2013,
  title = {Aesthetic Responses to Repetition in Unfamiliar Music},
  volume = {31},
  url = {http://journals.sagepub.com/doi/abs/10.2190/EM.31.1.c},
  number = {1},
  journaltitle = {Empirical Studies of the Arts},
  date = {2013},
  pages = {45--57},
  author = {Margulis, Elizabeth Hellmuth},
  file = {D\:\\Sauve\\Zotero\\storage\\HE37TP2Z\\EM.31.1.html}
}

@article{margulisMusicalRepetitionDetection2012,
  title = {Musical Repetition Detection across Multiple Exposures},
  volume = {29},
  url = {http://mp.ucpress.edu/content/29/4/377.short},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2012},
  pages = {377--385},
  author = {Margulis, Elizabeth Hellmuth},
  file = {D\:\\Sauve\\Zotero\\storage\\6QQ4KH24\\Margulis - 2012 - Musical repetition detection across multiple expos.pdf;D\:\\Sauve\\Zotero\\storage\\RWKTN3PA\\377.html}
}

@book{margulisRepeatHowMusic2014,
  title = {On Repeat: {{How}} Music Plays the Mind},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=4a6cAQAAQBAJ&oi=fnd&pg=PP1&dq=music+repetition+margulis&ots=w4JLN6C7--&sig=5IU3XVRffwdmPMIAXfvfAHnsJkc},
  shorttitle = {On Repeat},
  publisher = {{Oxford University Press}},
  date = {2014},
  author = {Margulis, Elizabeth Hellmuth},
  file = {D\:\\Sauve\\Zotero\\storage\\7KACEUX9\\books.html}
}

@article{bendixenRegularityExtractionApplication2007,
  title = {Regularity Extraction and Application in Dynamic Auditory Stimulus Sequences},
  volume = {19},
  url = {http://www.mitpressjournals.org/doi/abs/10.1162/jocn.2007.19.10.1664},
  number = {10},
  journaltitle = {Journal of Cognitive Neuroscience},
  date = {2007},
  pages = {1664--1677},
  author = {Bendixen, Alexandra and Roeber, Urte and Schröger, Erich},
  file = {D\:\\Sauve\\Zotero\\storage\\JN4NHA2D\\jocn.2007.19.10.html}
}

@article{bendixenMemoryTraceFormation2008,
  title = {Memory Trace Formation for Abstract Auditory Features and Its Consequences in Different Attentional Contexts},
  volume = {78},
  url = {http://www.sciencedirect.com/science/article/pii/S0301051108000677},
  number = {3},
  journaltitle = {Biological psychology},
  date = {2008},
  pages = {231--241},
  author = {Bendixen, Alexandra and Schröger, Erich},
  file = {D\:\\Sauve\\Zotero\\storage\\R9XW9QNC\\Bendixen and Schröger - 2008 - Memory trace formation for abstract auditory featu.pdf;D\:\\Sauve\\Zotero\\storage\\RPTJTBU7\\S0301051108000677.html}
}

@article{bendixenRapidExtractionAuditory2008,
  title = {Rapid Extraction of Auditory Feature Contingencies},
  volume = {41},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811908002875},
  number = {3},
  journaltitle = {Neuroimage},
  date = {2008},
  pages = {1111--1119},
  author = {Bendixen, Alexandra and Prinz, Wolfgang and Horváth, János and Trujillo-Barreto, Nelson J. and Schröger, Erich},
  file = {D\:\\Sauve\\Zotero\\storage\\UP645VMN\\Bendixen et al. - 2008 - Rapid extraction of auditory feature contingencies.pdf;D\:\\Sauve\\Zotero\\storage\\NVPGVAXZ\\S1053811908002875.html}
}

@article{mcauleyTimeOurLives2006,
  title = {The Time of Our Lives: Life Span Development of Timing and Event Tracking.},
  volume = {135},
  url = {http://psycnet.apa.org/journals/xge/135/3/348/},
  shorttitle = {The Time of Our Lives},
  number = {3},
  journaltitle = {Journal of Experimental Psychology: General},
  date = {2006},
  pages = {348},
  author = {McAuley, J. Devin and Jones, Mari Riess and Holub, Shayla and Johnston, Heather M. and Miller, Nathaniel S.},
  file = {D\:\\Sauve\\Zotero\\storage\\NRYQ6BNY\\McAuley et al. - 2006 - The time of our lives life span development of ti.pdf;D\:\\Sauve\\Zotero\\storage\\2LVJA5BF\\2006-09007-002.html;D\:\\Sauve\\Zotero\\storage\\75GMGPXM\\348.html}
}

@article{langeCanRegularContext2010,
  title = {Can a Regular Context Induce Temporal Orienting to a Target Sound?},
  volume = {78},
  url = {http://www.sciencedirect.com/science/article/pii/S0167876010006732},
  number = {3},
  journaltitle = {International Journal of Psychophysiology},
  date = {2010},
  pages = {231--238},
  author = {Lange, Kathrin},
  file = {D\:\\Sauve\\Zotero\\storage\\KQTGRZF9\\S0167876010006732.html}
}

@article{honingBeatInductionInnate2009,
  title = {Is {{Beat Induction Innate}} or {{Learned}}?},
  volume = {1169},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1749-6632.2009.04761.x/full},
  number = {1},
  journaltitle = {Annals of the New York Academy of Sciences},
  date = {2009},
  pages = {93--96},
  author = {Honing, Henkjan and Ladinig, Olivia and Háden, Gábor P. and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\M7AKKVSJ\\Honing et al. - 2009 - Is Beat Induction Innate or Learned.pdf;D\:\\Sauve\\Zotero\\storage\\KUSCSNCZ\\full.html}
}

@inproceedings{burgoyneExpertGroundTruth2011,
  title = {An {{Expert Ground Truth Set}} for {{Audio Chord Recognition}} and {{Music Analysis}}.},
  volume = {11},
  isbn = {978-0-615-54865-4},
  url = {http://ismir2011.ismir.net/papers/OS8-1.pdf},
  booktitle = {{{ISMIR}}},
  date = {2011},
  pages = {633--638},
  author = {Burgoyne, John Ashley and Wild, Jonathan and Fujinaga, Ichiro},
  file = {D\:\\Sauve\\Zotero\\storage\\CV4T9TKI\\Burgoyne et al. - 2011 - An Expert Ground Truth Set for Audio Chord Recogni.pdf}
}

@article{bendixenEarlyElectrophysiologicalIndicators2012,
  title = {Early Electrophysiological Indicators for Predictive Processing in Audition: {{A}} Review},
  volume = {83},
  issn = {0167-8760},
  doi = {10.1016/j.ijpsycho.2011.08.003},
  shorttitle = {Early Electrophysiological Indicators for Predictive Processing in Audition},
  abstract = {The auditory system essentially deals with sequential type of input and thus requires processing that is particularly suited to extract stimulus relations within a sequence. Evidence from a variety of paradigms converges to show that the auditory system automatically uses stimulus predictability for facilitating its sequential processing. This type of predictive processing does not require attentional processing of the sounds or cognitive control of the predictions, nor does it involve the preparation of motor responses to the auditory stimuli. We will present a taxonomy of paradigms and resulting electrophysiological indicators for such automatic predictive processing in terms of event-related potential components and oscillatory activity. These indicators will include signals of fulfilled predictions (match signals such as N1 attenuation, repetition positivity, and early evoked gamma band response enhancement) as well as signals of violated predictions (mismatch signals such as the mismatch negativity and stimulus omission responses). We will show how recent approaches have revealed particularly early indicators of predictive processing down to the level of the auditory middle-latency responses. We will discuss the strength of the various indicators in terms of a truly predictive account of auditory processing (as opposed to, e.g., a retrospective verification of predictions). Finally, we will discuss the benefits of a predictive system within and beyond auditory processing. In conclusion, we argue in favor of the overwhelming evidence for predictions in audition, flexibly instantiated on different levels and timescales, and we aim to provide guidance along a variety of research paradigms illustrating the existence of these predictions. (PsycINFO Database Record (c) 2012 APA, all rights reserved). (journal abstract)},
  number = {2},
  journaltitle = {International Journal of Psychophysiology},
  shortjournal = {International Journal of Psychophysiology},
  date = {2012-02},
  pages = {120-131},
  keywords = {Auditory Evoked Potentials,Oscillatory Network,electrophysiology,automatic predictive processing},
  author = {Bendixen, Alexandra and SanMiguel, Iria and Schröger, Erich},
  file = {D\:\\Sauve\\Zotero\\storage\\58SCVSG5\\Bendixen et al. - 2012 - Early electrophysiological indicators for predicti.pdf;D\:\\Sauve\\Zotero\\storage\\9C48IKI6\\S0167876011002376.html}
}

@article{andreouRoleTemporalRegularity2011,
  langid = {english},
  title = {The Role of Temporal Regularity in Auditory Segregation},
  volume = {280},
  issn = {1878-5891},
  doi = {10.1016/j.heares.2011.06.001},
  abstract = {The idea that predictive modelling and extraction of regularities plays a pivotal role in auditory segregation has recently attracted considerable attention. The present study investigated the effect of one basic form of regularity, rhythmic regularity, on auditory stream segregation. We departed from the classic streaming paradigm and developed a new stimulus, Rand-AB, consisting of two, concurrently presented, temporally uncorrelated, tone sequences (with frequencies A and B). To evaluate segregation, we used an objective measure of the extent to which listeners are able to selectively attend to one of the sequences in the presence of the other. Performance was quantified on a difficult pattern detection task which involves detecting a rarely occurring pattern of amplitude modulation applied to three consecutive A or B tones. In all cases the attended sequence was temporally irregular (with a random inter-tone-interval (ITI) between 100 and 400 ms) and the regularity status of the competing sequence was set to one of four conditions: (1) random ITI between 100 and 400 ms (2) isochronous with ITI = 400 ms. (3) isochronous with ITI = 250 ms (equal to the mean rate of the attended sequence) (4) isochronous with ITI = 100 ms. For a frequency separation of 2 (but not 4) semi tones we observed improved performance in conditions (3) and (4) relative to (1), suggesting that stream segregation is facilitated when the distracter sequence is temporally regular, but that the effect of temporal regularity as a cue for segregation is limited to relatively fast rates and to situations where frequency separation is insufficient for segregation. These findings provide new evidence to support models of streaming that involve segregation based on the formation of predictive models.},
  number = {1-2},
  journaltitle = {Hearing Research},
  shortjournal = {Hear. Res.},
  date = {2011-10},
  pages = {228-235},
  keywords = {Auditory Perception,Cues,Auditory Cortex,Acoustic Stimulation,Auditory Pathways,humans,Models; Biological,Adult,Female,Male,SOUND,Periodicity},
  author = {Andreou, Lefkothea-Vasiliki and Kashino, Makio and Chait, Maria},
  file = {D\:\\Sauve\\Zotero\\storage\\AHM5W5W7\\S0378595511001614.html},
  eprinttype = {pmid},
  eprint = {21683778}
}

@article{meinzEffectsAgeExperience1998,
  title = {The Effects of Age and Experience on Memory for Visually Presented Music},
  volume = {53},
  url = {http://psychsocgerontology.oxfordjournals.org/content/53B/1/P60.short},
  number = {1},
  journaltitle = {The Journals of Gerontology Series B: Psychological Sciences and Social Sciences},
  urldate = {2017-05-28},
  date = {1998},
  pages = {P60--P69},
  author = {Meinz, Elizabeth J. and Salthouse, Timothy A.},
  file = {D\:\\Sauve\\Zotero\\storage\\2PDQU82Q\\Meinz and Salthouse - 1998 - The effects of age and experience on memory for vi.pdf;D\:\\Sauve\\Zotero\\storage\\XB6NHVWR\\The-Effects-of-Age-and-Experience-on-Memory-for.html}
}

@article{halpernPerceptionModeRhythm1998,
  title = {Perception of Mode, Rhythm, and Contour in Unfamiliar Melodies: {{Effects}} of Age and Experience},
  volume = {15},
  url = {http://mp.ucpress.edu/content/15/4/335.abstract},
  shorttitle = {Perception of Mode, Rhythm, and Contour in Unfamiliar Melodies},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2017-05-28},
  date = {1998},
  pages = {335--355},
  author = {Halpern, Andrea R. and Bartlett, James C. and Dowling, W. Jay},
  file = {D\:\\Sauve\\Zotero\\storage\\IJHSJA2P\\Halpern et al. - 1998 - Perception of mode, rhythm, and contour in unfamil.pdf;D\:\\Sauve\\Zotero\\storage\\KBRQJFCD\\335.html}
}

@article{halpernAgingMemoryMusic2002,
  title = {Aging and Memory for Music: {{A}} Review.},
  volume = {18},
  url = {http://psycnet.apa.org/journals/pmu/18/1-2/10/},
  shorttitle = {Aging and Memory for Music},
  number = {1-2},
  journaltitle = {Psychomusicology: A Journal of Research in Music Cognition},
  urldate = {2017-05-28},
  date = {2002},
  pages = {10},
  author = {Halpern, Andrea R. and Bartlett, James C.},
  file = {D\:\\Sauve\\Zotero\\storage\\BV9RW6FN\\Halpern and Bartlett - 2002 - Aging and memory for music A review..pdf;D\:\\Sauve\\Zotero\\storage\\PZJUAFSS\\10.html}
}

@article{dowlingMelodyRecognitionFast2008,
  title = {Melody Recognition at Fast and Slow Tempos: {{Effects}} of Age, Experience, and Familiarity},
  volume = {70},
  url = {http://www.springerlink.com/index/6801R18158770025.pdf},
  shorttitle = {Melody Recognition at Fast and Slow Tempos},
  number = {3},
  journaltitle = {Attention, Perception, \& Psychophysics},
  urldate = {2017-05-28},
  date = {2008},
  pages = {496--502},
  author = {Dowling, W. Jay and Bartlett, James C. and Halpern, Andrea R. and Andrews, Melinda W.},
  file = {D\:\\Sauve\\Zotero\\storage\\QR42NHKG\\Dowling et al. - 2008 - Melody recognition at fast and slow tempos Effect.pdf;D\:\\Sauve\\Zotero\\storage\\F2BJXXQN\\10.3758PP.70.3.html}
}

@article{halpernImplicitMemoryMusic2000,
  title = {Implicit Memory for Music in {{Alzheimer}}'s Disease.},
  volume = {14},
  url = {http://psycnet.apa.org/journals/neu/14/3/391/},
  number = {3},
  journaltitle = {Neuropsychology},
  urldate = {2017-05-28},
  date = {2000},
  pages = {391},
  author = {Halpern, Andrea R. and O'connor, Margaret G.},
  file = {D\:\\Sauve\\Zotero\\storage\\GF2NEG4W\\391.html}
}

@article{pearceAgerelatedPatternsEmotions2015,
  title = {Age-Related Patterns in Emotions Evoked by Music.},
  volume = {9},
  url = {http://psycnet.apa.org/journals/aca/9/3/248/},
  number = {3},
  journaltitle = {Psychology of Aesthetics, Creativity, and the Arts},
  urldate = {2017-05-28},
  date = {2015},
  pages = {248},
  author = {Pearce, Marcus T. and Halpern, Andrea R.},
  file = {D\:\\Sauve\\Zotero\\storage\\EQHPE7EZ\\Pearce and Halpern - 2015 - Age-related patterns in emotions evoked by music..pdf;D\:\\Sauve\\Zotero\\storage\\EUZE8WWA\\248.html}
}

@article{halpernThatNoteSounds2017,
  title = {That Note Sounds Wrong! {{Age}}-Related Effects in Processing of Musical Expectation},
  volume = {113},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262616300434},
  journaltitle = {Brain and Cognition},
  urldate = {2017-05-28},
  date = {2017},
  pages = {1--9},
  author = {Halpern, Andrea R. and Zioga, Ioanna and Shankleman, Martin and Lindsen, Job and Pearce, Marcus T. and Bhattarcharya, Joydeep},
  file = {D\:\\Sauve\\Zotero\\storage\\3W4R7MC2\\S0278262616300434.html;D\:\\Sauve\\Zotero\\storage\\4FPYVAE4\\S0278262616300434.html;D\:\\Sauve\\Zotero\\storage\\JEW3KNBN\\S0278262616300434.html}
}

@article{jergerEffectsAgeSex1980,
  title = {Effects of Age and Sex on Auditory Brainstem Response},
  volume = {106},
  url = {http://archotol.jamanetwork.com/article.aspx?articleid=608454},
  number = {7},
  journaltitle = {Archives of Otolaryngology},
  urldate = {2017-05-23},
  date = {1980},
  pages = {387--391},
  author = {Jerger, James and Hall, James},
  file = {D\:\\Sauve\\Zotero\\storage\\BFWGT7NB\\608454.html}
}

@article{gebauerEverchangingCyclesMusical2012,
  title = {Ever-Changing Cycles of Musical Pleasure: {{The}} Role of Dopamine and Anticipation.},
  volume = {22},
  url = {http://psycnet.apa.org/journals/pmu/22/2/152/},
  shorttitle = {Ever-Changing Cycles of Musical Pleasure},
  number = {2},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  urldate = {2017-05-22},
  date = {2012},
  pages = {152},
  author = {Gebauer, Line and Kringelbach, Morten L. and Vuust, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\74HF9WTK\\Gebauer et al. - 2012 - Ever-changing cycles of musical pleasure The role.pdf;D\:\\Sauve\\Zotero\\storage\\4T53WNE5\\152.html}
}

@article{volkStudySyncopationUsing2008,
  title = {The {{Study}} of {{Syncopation Using Inner Metric Analysis}}: {{Linking Theoretical}} and {{Experimental Analysis}} of {{Metre}} in {{Music}}},
  volume = {37},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298210802680758},
  doi = {10.1080/09298210802680758},
  shorttitle = {The {{Study}} of {{Syncopation Using Inner Metric Analysis}}},
  abstract = {This paper investigates the influence of syncopation on the metric structure of musical pieces using the computational model of Inner Metric Analysis. Inner Metric Analysis generates metric hierarchies evoked by the note onsets of a piece. Syncopation in the rhythmic structure influences these hierarchies in different ways. The comparison of local and global perspectives on the metric structure allows one to distinguish between different amounts of syncopation present in a musical piece. This paper shows that the study of syncopation using Inner Metric Analysis contributes to explaining tapping performances of listeners. Hence comparing the structural descriptions generated by the model to results of a listening experiment helps to link music theoretic and perceptual studies.},
  number = {4},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-22},
  date = {2008-12-01},
  pages = {259-273},
  author = {Volk, Anja},
  file = {D\:\\Sauve\\Zotero\\storage\\ETJKJJIP\\09298210802680758.html}
}

@article{desainComputationalModelsBeat1999,
  title = {Computational {{Models}} of {{Beat Induction}}: {{The Rule}}-{{Based Approach}}},
  volume = {28},
  issn = {0929-8215},
  url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.28.1.29.3123},
  doi = {10.1076/jnmr.28.1.29.3123},
  shorttitle = {Computational {{Models}} of {{Beat Induction}}},
  abstract = {This paper is a report of ongoing research on the computational modeling of beat induction which aims at achieving a better understanding of the perceptual processes involved by ordering and reformulating existing models. One family of rule-based beat induction models is described (Longuet-Higgins and Lee, 1982; Lee, 1985; Longuet-Higgins, 1994), along with the presentation of analysis methods that allow an evaluation of the models in terms of their in- and output spaces, ing from internal detail. It builds on work described in (Desain and Honing, 1994b). The present paper elaborates these methods and presents the results obtained. It will be shown that they can be used to characterize the differences between these models, a point that was difficult to assess previously. Furthermore, the first results of using the method to improve the existing rule-based models are presented, by describing the most effective version of a specific rule, and the most effective parameter settings.},
  number = {1},
  journaltitle = {Journal of New Music Research},
  urldate = {2017-05-22},
  date = {1999-03-01},
  pages = {29-42},
  author = {Desain, Peter and Honing, Henkjan},
  file = {D\:\\Sauve\\Zotero\\storage\\2IXZ3NF9\\jnmr.28.1.29.html}
}

@article{jonesControlledAttendingFunction1982,
  langid = {english},
  title = {Controlled Attending as a Function of Melodic and Temporal Context},
  volume = {32},
  issn = {0031-5117, 1532-5962},
  url = {http://link.springer.com/article/10.3758/BF03206225},
  doi = {10.3758/BF03206225},
  abstract = {Melodic and rhythmic context were systematically varied in a pattern recognition task involving pairs (standard-comparison) of nine-tone auditory sequences. The experiment was designed to test the hypothesis that rhythmic context can direct attention toward or away from tones which instantiate higher order melodic rules. Three levels of melodic structure (one, two, no higher order rules) were crossed with four levels of rhythm [isochronous, dactyl (A U U), anapest (U U A), irregular]. Rhythms were designed to shift accent locations on three centrally embedded tones. Listeners were more accurate in detecting violations of higher order melodic rules when the rhythmic context induced accents on tones which instantiated these rules. Effects are discussed in terms of attentional rhythmicity.},
  number = {3},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  urldate = {2016-09-05},
  date = {1982},
  pages = {211-218},
  author = {Jones, Mari Riess and Boltz, Marilyn and Kidd, Gary},
  file = {D\:\\Sauve\\Zotero\\storage\\HVHIE56C\\Jones et al. - Controlled attending as a function of melodic and .pdf;D\:\\Sauve\\Zotero\\storage\\QBN8C8CJ\\BF03206225.html}
}

@article{palmerIndependentTemporalPitch1987,
  langid = {english},
  title = {Independent Temporal and Pitch Structures in Determination of Musical Phrases},
  volume = {13},
  issn = {0096-1523},
  abstract = {In two experiments we addressed the roles of temporal and pitch structures in judgments of melodic phrases. Musical excerpts were rated on how good or complete a phrase they made. In Experiment 1, trials in the temporal condition retained the original temporal pattern but were equitonal; trials in the pitch condition retained the original pitch pattern but were equitemporal; and trials in the melody condition contained both temporal and pitch patterns. In Experiment 2, one pattern (pitch or temporal) was shifted in phase and recombined with the other pattern to create the pitch and temporal conditions. In the melody condition, both patterns were shifted together. In both experiments, ratings in the temporal and pitch conditions were uncorrelated, and the melody condition ratings were accurately predicted by a linear combination of the pitch and temporal condition ratings. These results were consistent across musicians with varying levels of experience.},
  number = {1},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  date = {1987-02},
  pages = {116-126},
  keywords = {Music,Pitch Discrimination,humans,Adult,Psychoacoustics,Adolescent,Set (Psychology)},
  author = {Palmer, C. and Krumhansl, C. L.},
  eprinttype = {pmid},
  eprint = {2951485}
}

@article{palmerPitchTemporalContributions1987,
  langid = {english},
  title = {Pitch and Temporal Contributions to Musical Phrase Perception: {{Effects}} of Harmony, Performance Timing, and Familiarity},
  volume = {41},
  issn = {0031-5117, 1532-5962},
  url = {http://link.springer.com/article/10.3758/BF03210485},
  doi = {10.3758/BF03210485},
  shorttitle = {Pitch and Temporal Contributions to Musical Phrase Perception},
  abstract = {Four experiments assessed pitch and temporal contributions to phrase judgments made on excerpts from classical music. In Experiment 1, pitch-condition trials retained the original pitch pattern but were equitemporal, temporal-condition trials retained the original temporal pattern but were equitonal, and combined-condition trials contained both patterns. In Experiment 2, one pattern was shifted in phase and recombined with the other pattern to create the pitch and temporal conditions; in the combined condition, both patterns were shifted together. The stimuli in Experiments 3 and 4 used durations from a recorded performance. In all experiments, a linear combination of the pitch- and temporal-condition ratings accurately predicted the combinedcondition ratings. Familiarity with the music resulted in a higher correlation between pitch- and temporal-condition ratings, but did not alter the additive relationship; performance timing had little effect.},
  number = {6},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  urldate = {2016-08-15},
  date = {1987},
  pages = {505-518},
  author = {Palmer, Caroline and Krumhansl, Carol L.},
  file = {D\:\\Sauve\\Zotero\\storage\\NQIUPZSU\\Palmer and Krumhansl - Pitch and temporal contributions to musical phrase.pdf;D\:\\Sauve\\Zotero\\storage\\4JRD6PSU\\BF03210485.html}
}

@software{schutteMATLABToolbosUsed2012,
  title = {{{MATLAB}} Toolbos Used to Handle {{MIDI}} Data},
  url = {http://www.kenschutte.com/midi#Files},
  urldate = {2014-12-05},
  date = {2012},
  author = {Schutte, K}
}

@article{hannonTuningMusicalRhythms2005,
  langid = {english},
  title = {Tuning in to Musical Rhythms: {{Infants}} Learn More Readily than Adults},
  volume = {102},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/102/35/12639},
  doi = {10.1073/pnas.0504254102},
  shorttitle = {Tuning in to Musical Rhythms},
  abstract = {Domain-general tuning processes may guide the acquisition of perceptual knowledge in infancy. Here, we demonstrate that 12-month-old infants show an adult-like, culture-specific pattern of responding to musical rhythms, in contrast to the culture-general responding that is evident at 6 months of age. Nevertheless, brief exposure to foreign music enables 12-month-olds, but not adults, to perceive rhythmic distinctions in foreign musical contexts. These findings may indicate a sensitive period early in life for acquiring rhythm in particular or socially and biologically important structures more generally.},
  number = {35},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {PNAS},
  urldate = {2016-08-15},
  date = {2005-08-30},
  pages = {12639-12643},
  keywords = {Development,Learning,Perception},
  author = {Hannon, Erin E. and Trehub, Sandra E.},
  file = {D\:\\Sauve\\Zotero\\storage\\K9PUMK9H\\Hannon and Trehub - 2005 - Tuning in to musical rhythms Infants learn more r.pdf;D\:\\Sauve\\Zotero\\storage\\2XWEE7VC\\12639.html},
  eprinttype = {pmid},
  eprint = {16105946}
}

@article{pelletierEffectMusicDecreasing2004,
  langid = {english},
  title = {The Effect of Music on Decreasing Arousal Due to Stress: A Meta-Analysis},
  volume = {41},
  issn = {0022-2917},
  shorttitle = {The Effect of Music on Decreasing Arousal Due to Stress},
  abstract = {A meta-analytic review of research articles using music to decrease arousal due to stress was conducted on 22 quantitative studies. Results demonstrated that music alone and music assisted relaxation techniques significantly decreased arousal (d = +.67). Further analysis of each study revealed that the amount of stress reduction was significantly different when considering age, type of stress, music assisted relaxation technique, musical preference, previous music experience, and type of intervention. Implications and suggestions for future research are discussed.},
  number = {3},
  journaltitle = {Journal of Music Therapy},
  shortjournal = {J Music Ther},
  date = {2004},
  pages = {192-214},
  keywords = {Music,humans,Arousal,Relaxation Therapy,Stress; Psychological},
  author = {Pelletier, Cori L.},
  eprinttype = {pmid},
  eprint = {15327345}
}

@article{khalfaEffectsRelaxingMusic2003,
  langid = {english},
  title = {Effects of Relaxing Music on Salivary Cortisol Level after Psychological Stress},
  volume = {999},
  issn = {0077-8923},
  abstract = {The goal of the present study was to determine whether relaxing music (as compared to silence) might facilitate recovery from a psychologically stressful task. To this aim, changes in salivary cortisol levels were regularly monitored in 24 students before and after the Trier Social Stress Test. The data show that in the presence of music, the salivary cortisol level ceased to increase after the stressor, whereas in silence it continued to increase for 30 minutes.},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Ann. N. Y. Acad. Sci.},
  date = {2003-11},
  pages = {374-376},
  keywords = {Music,humans,Adult,Male,emotions,Relaxation Therapy,Stress; Psychological,Anxiety,Hydrocortisone,Music therapy,Salivary Glands},
  author = {Khalfa, Stephanie and Bella, Simone Dalla and Roy, Mathieu and Peretz, Isabelle and Lupien, Sonia J.},
  eprinttype = {pmid},
  eprint = {14681158}
}

@article{pearceTwoTheoriesAttention2010,
  title = {Two Theories of Attention: {{A}} Review and a Possible Integration},
  url = {https://books.google.co.uk/books?hl=en&lr=&id=wn-hHPt_ftgC&oi=fnd&pg=PA11&dq=pearce+mackintosh+Two+theories+of+attention&ots=jxIwCgyHz0&sig=sK-nORMHOm5hWdZVodZKqhWewzU},
  shorttitle = {Two Theories of Attention},
  journaltitle = {Attention and associative learning: From brain to behaviour},
  urldate = {2016-08-12},
  date = {2010},
  pages = {11--39},
  author = {Pearce, John M. and Mackintosh, Nicholas J.},
  file = {D\:\\Sauve\\Zotero\\storage\\EA26Q8HC\\books.html}
}

@article{mackintoshTheoryAttentionVariations1975,
  title = {A Theory of Attention: {{Variations}} in the Associability of Stimuli with Reinforcement},
  volume = {82},
  issn = {1939-1471(Electronic);0033-295X(Print)},
  doi = {10.1037/h0076778},
  shorttitle = {A Theory of Attention},
  abstract = {Review of the literature indicates that, according to theories of selective attention, learning about a stimulus depends on attending to that stimulus; this is represented in 2-stage models by saying that Ss switch in analyzers as well as learning stimulus-response associations. It is argued that this assumption, however, is equally well represented in a formal model by the incorporation of a stimulus-specific learning-rate parameter, a, into the equations describing changes in the associative strength of stimuli. Previous theories of selective attention have also assumed that (a) Ss learn to attend to and ignore relevant and irrelevant stimuli (i.e., that a may increase or decrease depending on the correlation of a stimulus with reinforcement); and (b) there is an inverse relationship between the probabilities of attending to different stimuli (i.e., that an increase in a to one stimulus is accompanied by a decrease in a to others). The first assumption has been used to explain the phenomena of acquired distinctiveness and dimensional transfer, the second to explain those of overshadowing and blocking. It is argued that although the first assumption is justified by the data, the second is not: Overshadowing and blocking are better explained by the choice of an appropriate rule for changing a, such that a decreases to stimuli that signal no change from the probability of reinforcement predicted by other stimuli. (65 ref)},
  number = {4},
  journaltitle = {Psychological Review},
  date = {1975},
  pages = {276-298},
  keywords = {selective attention,*Associative Processes,*Learning Rate,*Learning Theory,*Reinforcement},
  author = {Mackintosh, N. J.}
}

@article{pearceModelPavlovianLearning1980,
  title = {A Model for {{Pavlovian}} Learning: {{Variations}} in the Effectiveness of Conditioned but Not of Unconditioned Stimuli},
  volume = {87},
  issn = {1939-1471(Electronic);0033-295X(Print)},
  doi = {10.1037/0033-295X.87.6.532},
  shorttitle = {A Model for {{Pavlovian}} Learning},
  abstract = {Part 1 of this discussion summarizes several formal models of exicitatory classical conditioning. It is suggested that a central problem for all of them is the explanation of cases in which learning does not occur in spite of the fact that the CS is a signal for the reinforcer. A new model is proposed that deals with this problem by specifying that certain procedures cause a CS to lose effectiveness; in particular, it is argued that a CS will lose associability when its consequences are accurately predicted. In contrast to other current models, the effectiveness of the reinforcer remains constant throughout conditioning. Part 2 presents a reformulation of the nature of the learning produced by inhibitory-conditioning procedures and a discussion of the way in which such learning can be accommodated within the model outlined for excitatory learning. (47 ref)},
  number = {6},
  journaltitle = {Psychological Review},
  date = {1980},
  pages = {532-552},
  keywords = {Models,*Classical Conditioning},
  author = {Pearce, John M. and Hall, Geoffrey}
}

@article{eerolaExpectancyViolationInformationTheoreticModels2016,
  title = {Expectancy-{{Violation}} and {{Information}}-{{Theoretic Models}} of {{Melodic Complexity}}},
  volume = {11},
  issn = {1559-5749},
  url = {http://emusicology.org/article/view/4836},
  doi = {10.18061/emr.v11i1.4836},
  number = {1},
  journaltitle = {Empirical Musicology Review},
  urldate = {2016-08-10},
  date = {2016-07-08},
  pages = {2-17},
  author = {Eerola, Tuomas}
}

@article{bittmanRecreationalMusicmakingAlters2013,
  title = {Recreational Music-Making Alters Gene Expression Pathways in Patients with Coronary Heart Disease},
  volume = {19},
  issn = {1234-1010},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3628588/},
  doi = {10.12659/MSM.883807},
  abstract = {Background
Psychosocial stress profoundly impacts long-term cardiovascular health through adverse effects on sympathetic nervous system activity, endothelial dysfunction, and atherosclerotic development. Recreational Music Making (RMM) is a unique stress amelioration strategy encompassing group music-based activities that has great therapeutic potential for treating patients with stress-related cardiovascular disease.

Material/Methods
Participants (n=34) with a history of ischemic heart disease were subjected to an acute time-limited stressor, then randomized to RMM or quiet reading for one hour. Peripheral blood gene expression using GeneChip® Human Genome U133A 2.0 arrays was assessed at baseline, following stress, and after the relaxation session.

Results
Full gene set enrichment analysis identified 16 molecular pathways differentially regulated (P{$<$}0.005) during stress that function in immune response, cell mobility, and transcription. During relaxation, two pathways showed a significant change in expression in the control group, while 12 pathways governing immune function and gene expression were modulated among RMM participants. Only 13\% (2/16) of pathways showed differential expression during stress and relaxation.

Conclusions
Human stress and relaxation responses may be controlled by different molecular pathways. Relaxation through active engagement in Recreational Music Making may be more effective than quiet reading at altering gene expression and thus more clinically useful for stress amelioration.},
  journaltitle = {Medical Science Monitor : International Medical Journal of Experimental and Clinical Research},
  shortjournal = {Med Sci Monit},
  urldate = {2016-08-10},
  date = {2013-02-25},
  pages = {139-147},
  author = {Bittman, Barry and Croft, Daniel T. and Brinker, Jeannie and van Laar, Ryan and Vernalis, Marina N. and Ellsworth, Darrell L.},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\HNK2Z283\\Bittman et al. - 2013 - Recreational music-making alters gene expression p.pdf},
  eprinttype = {pmid},
  eprint = {23435350},
  pmcid = {PMC3628588}
}

@article{castillo-perezEffectsMusicTherapy2010,
  title = {Effects of Music Therapy on Depression Compared with Psychotherapy},
  volume = {37},
  issn = {0197-4556},
  url = {http://www.sciencedirect.com/science/article/pii/S0197455610000857},
  doi = {10.1016/j.aip.2010.07.001},
  abstract = {This paper reports a study testing the effects of music on depression and compares them with the effects of psychotherapy. There are mainly three conventional treatments for depression: psychotherapy, pharmaceutical treatments, and electroconvulsive therapy. Because conventional treatment has proven to be poorly successful, new means of treatment must be found that might improve depression when used together with other therapies. A randomized controlled clinical trial was performed with a convenience sample of 79 patients aged 25–60 years with low- and medium-grade depression. The Zung Depression Scale was employed for selection purposes. Patients were randomly assigned to the music-therapy group (classical and baroque music) (n = 41), or the psychotherapy group based on conductive-behavioral therapy (n = 38). The music therapy was applied for 50 min a day, every day, for eight weeks. At the end, the music-therapy group had less depressive symptoms than the psychotherapy group, and this was proven to be statistically significant with the Friedman test. We propose that patients with low- and medium-grade depression can use music to enhance the effects of psychological support.},
  number = {5},
  journaltitle = {The Arts in Psychotherapy},
  shortjournal = {The Arts in Psychotherapy},
  urldate = {2016-08-10},
  date = {2010-11},
  pages = {387-390},
  keywords = {Music therapy,Depression,Psychotherapy,Randomized controlled trial},
  author = {Castillo-Pérez, Sergio and Gómez-Pérez, Virginia and Velasco, Minerva Calvillo and Pérez-Campos, Eduardo and Mayoral, Miguel-Angel},
  file = {D\:\\Sauve\\Zotero\\storage\\S44MHFAX\\S0197455610000857.html}
}

@article{carpentierEffectsMusicPhysiological2007,
  title = {Effects of {{Music}} on {{Physiological Arousal}}: {{Explorations}} into {{Tempo}} and {{Genre}}},
  volume = {10},
  issn = {1521-3269},
  url = {http://dx.doi.org/10.1080/15213260701533045},
  doi = {10.1080/15213260701533045},
  shorttitle = {Effects of {{Music}} on {{Physiological Arousal}}},
  abstract = {Two experiments explore the validity of conceptualizing musical beats as auditory structural features and the potential for increases in tempo to lead to greater sympathetic arousal, measured using skin conductance. In the first experiment, fast- and slow-paced rock and classical music excerpts were compared to silence. As expected, skin conductance response (SCR) frequency was greater during music processing than during silence. Skin conductance level (SCL) data showed that fast-paced music elicits greater activation than slow-paced music. Genre significantly interacted with tempo in SCR frequency, with faster tempo increasing activation for classical music and decreasing it for rock music. A second experiment was conducted to explore the possibility that the presumed familiarity of the genre led to this interaction. Although further evidence was found for conceptualizing musical beat onsets as auditory structure, the familiarity explanation was not supported.},
  number = {3},
  journaltitle = {Media Psychology},
  urldate = {2016-08-03},
  date = {2007-09-28},
  pages = {339-363},
  author = {Carpentier, Francesca R. Dillman and Potter, Robert F.},
  file = {D\:\\Sauve\\Zotero\\storage\\CCR5MIVS\\15213260701533045.html}
}

@article{husainEffectsMusicalTempo2002,
  eprinttype = {jstor},
  eprint = {10.1525/mp.2002.20.2.151},
  title = {Effects of {{Musical Tempo}} and {{Mode}} on {{Arousal}}, {{Mood}}, and {{Spatial Abilities}}},
  volume = {20},
  issn = {0730-7829},
  doi = {10.1525/mp.2002.20.2.151},
  abstract = {We examined effects of tempo and mode on spatial ability, arousal, and mood. A Mozart sonata was performed by a skilled pianist and recorded as a MIDI file. The file was edited to produce four versions that varied in tempo (fast or slow) and mode (major or minor). Participants listened to a single version and completed measures of spatial ability, arousal, and mood. Performance on the spatial task was superior after listening to music at a fast rather than a slow tempo, and when the music was presented in major rather than minor mode. Tempo manipulations affected arousal but not mood, whereas mode manipulations affected mood but not arousal. Changes in arousal and mood paralleled variation on the spatial task. The findings are consistent with the view that the "Mozart effect" is a consequence of changes in arousal and mood.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  shortjournal = {Music Perception: An Interdisciplinary Journal},
  date = {2002},
  pages = {151-171},
  author = {Husain, Gabriela and Thompson, William Forde and Schellenberg, E. Glenn}
}

@article{deanTimeSeriesAnalysis2014,
  title = {Time Series Analysis of Real-Time Music Perception: Approaches to the Assessment of Individual and Expertise Differences in Perception of Expressed Affect},
  volume = {8},
  issn = {1745-9737},
  url = {http://dx.doi.org/10.1080/17459737.2014.928752},
  doi = {10.1080/17459737.2014.928752},
  shorttitle = {Time Series Analysis of Real-Time Music Perception},
  abstract = {We use time series analysis methods to detect differences between individuals and expertise groups in continuous perceptions of the arousal expressed by Wishart's electroacoustic piece Red Bird. The study is part of a project in which we characterise dynamic perception of the structure and affective expression of music. We find that individual series of perceptions of expressed arousal often show considerable periods of stasis. This may challenge conventional time series methodologies, so we test their validity by application of a general linear autoregressive moving average (GLARMA) approach, which supports it. Acoustic intensity is a dominant predictor of perceived arousal in this piece. We show that responses are time-variant and that animate sounds influence the conditional variance of perceived arousal. Using vector autoregression and cross-sectional time series analysis (which preserves the integrity of each individual response series), we find differences between musical expertise groups (non-musicians, musicians, and electroacoustic musicians). Individual differences within each group are greater than those between expertise groups. The companion paper applies the developed methods to all four pieces in our overall project (Dean, R.T., F. Bailes, and W.T.M. Dunsmuir. 2014. “Shared and Distinct Mechanisms of Individual and Expertise-Group Perception of Expressed Arousal in Four Works.” Journal of Mathematics and Music 8 (3): 207–223). An Online Supplement is available at http://dx.doi.org/10.1080/17459737.2014.928752.},
  number = {3},
  journaltitle = {Journal of Mathematics and Music},
  urldate = {2016-08-03},
  date = {2014-09-02},
  pages = {183-205},
  author = {Dean, Roger T. and Bailes, Freya and Dunsmuir, William T. M.},
  file = {D\:\\Sauve\\Zotero\\storage\\4J4892S8\\17459737.2014.html}
}

@article{thompsonSeeingMusicPerformance2005,
  title = {Seeing Music Performance: {{Visual}} Inﬂuences on Perception and Experience},
  volume = {2005},
  issn = {1613-3692},
  url = {http://www.degruyter.com/view/j/semi.2005.2005.issue-156/semi.2005.2005.156.203/semi.2005.2005.156.203.xml},
  doi = {10.1515/semi.2005.2005.156.203},
  shorttitle = {Seeing Music Performance},
  abstract = {Drawing from ethnographic, empirical, and historical / cultural perspectives, we examine the extent to which visual aspects of music contribute to the communication that takes place between performers and their listeners. First, we introduce a framework for understanding how media and genres shape aural and visual experiences of music. Second, we present case studies of two performances, and describe the relation between visual and aural aspects of performance. Third, we report empirical evidence that visual aspects of performance reliably inﬂuence perceptions of musical structure (pitch related features) and affective interpretations of music. Finally, we trace new and old media trajectories of aural and visual dimensions of music, and highlight how our conceptions, perceptions and appreciation of music are intertwined with technological innovation and media deployment strategies.},
  number = {156},
  journaltitle = {Semiotica},
  urldate = {2016-08-02},
  date = {2005},
  pages = {203--227},
  author = {Thompson, William Forde and Graham, Phil and Russo, Frank A.}
}

@article{vinesCrossmodalInteractionsPerception2006,
  title = {Cross-Modal Interactions in the Perception of Musical Performance},
  volume = {101},
  issn = {0010-0277},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027705001538},
  doi = {10.1016/j.cognition.2005.09.003},
  abstract = {We investigate the dynamics of sensory integration for perceiving musical performance, a complex natural behavior. Thirty musically trained participants saw, heard, or both saw and heard, performances by two clarinetists. All participants used a sliding potentiometer to make continuous judgments of tension (a measure correlated with emotional response) and continuous judgments of phrasing (a measure correlated with perceived musical structure) as performances were presented. The data analysis sought to reveal relations between the sensory modalities (vision and audition) and to quantify the effect of seeing the performances on participants' overall subjective experience of the music. In addition to traditional statistics, functional data analysis techniques were employed to analyze time-varying aspects of the data. The auditory and visual channels were found to convey similar experiences of phrasing but different experiences of tension through much of the performances. We found that visual information served both to augment and to reduce the experience of tension at different points in the musical piece (as revealed by functional linear modeling and functional significance testing). In addition, the musicians' movements served to extend the sense of phrasing, to cue the beginning of new phrases, to indicate musical interpretation, and to anticipate changes in emotional content. Evidence for an interaction effect suggests that there may exist an emergent quality when musical performances are both seen and heard. The investigation augments knowledge of human communicative processes spanning language and music, and involving multiple modalities of emotion and information transfer.},
  number = {1},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  urldate = {2016-08-02},
  date = {2006-08},
  pages = {80-113},
  keywords = {emotion,music cognition,Cross-modal interactions,Gesture},
  author = {Vines, Bradley W. and Krumhansl, Carol L. and Wanderley, Marcelo M. and Levitin, Daniel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\EJSPTNTH\\S0010027705001538.html}
}

@article{trainorComparisonInfantsAdults1992,
  title = {A Comparison of Infants' and Adults' Sensitivity to {{Western}} Musical Structure},
  volume = {18},
  issn = {1939-1277(Electronic);0096-1523(Print)},
  doi = {10.1037/0096-1523.18.2.394},
  abstract = {28 adults and 48 8-mo-old infants listened to repeated transpositions of a 10-note melody exemplifying the rules of Western tonal music. They were tested for their detection of 2 types of changes to that melody: (1) a 4-semitone change in 1 note that remained within the key and implied dominant harmony (diatonic change) or (2) a 1-semitone change in the same note that went outside the key (nondiatonic change). Adults easily detected the nondiatonic change but had difficulty with the diatonic change. Infants detected both changes equally well, performing better than adults in some circumstances. These findings imply that there are qualitative differences in infants' and adults' processing of musical information.},
  number = {2},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {1992},
  pages = {394-402},
  keywords = {Age Differences,Cognitive Processes,*Music,*Auditory Perception,*Familiarity},
  author = {Trainor, Laurel J. and Trehub, Sandra E.}
}

@article{balkwillCrossCulturalInvestigationPerception1999,
  langid = {english},
  title = {A {{Cross}}-{{Cultural Investigation}} of the {{Perception}} of {{Emotion}} in {{Music}}: {{Psychophysical}} and {{Cultural Cues}}},
  volume = {17},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/17/1/43},
  doi = {10.2307/40285811},
  shorttitle = {A {{Cross}}-{{Cultural Investigation}} of the {{Perception}} of {{Emotion}} in {{Music}}},
  abstract = {Studies of the link between music and emotion have primarily focused on listeners' sensitivity to emotion in the music of their own culture. This sensitivity may reflect listeners' enculturation to the conventions of their culture's tonal system. However, it may also reflect responses to psychophysical dimensions of sound that are independent of musical experience. A model of listeners' perception of emotion in music is proposed in which emotion in music is communicated through a combination of universal and cultural cues. Listeners may rely on either of these cues, or both, to arrive at an understanding of musically expressed emotion. The current study addressed the hypotheses derived from this model using a cross-cultural approach. The following questions were investigated: Can people identify the intended emotion in music from an unfamiliar tonal system? If they can, is their sensitivity to intended emotions associated with perceived changes in psychophysical dimensions of music? Thirty Western listeners rated the degree of joy, sadness, anger, and peace in 12 Hindustani raga excerpts (field recordings obtained in North India). In accordance with the raga-rasa system, each excerpt was intended to convey one of the four moods or "rasas" that corresponded to the four emotions rated by listeners. Listeners also provided ratings of four psychophysical variables: tempo, rhythmic complexity, melodic complexity, and pitch range. Listeners were sensitive to the intended emotion in ragas when that emotion was joy, sadness, or anger. Judgments of emotion were significantly related to judgments of psychophysical dimensions, and, in some cases, to instrument timbre. The findings suggest that listeners are sensitive to musically expressed emotion in an unfamiliar tonal system, and that this sensitivity is facilitated by psychophysical cues.},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-08-02},
  date = {1999-10-01},
  pages = {43-64},
  author = {Balkwill, Laura-Lee and Thompson, William Forde},
  file = {D\:\\Sauve\\Zotero\\storage\\BAQYLGMJ\\Balkwill and Thompson - 1999 - A cross-cultural investigation of the perception o.pdf;D\:\\Sauve\\Zotero\\storage\\F8YMIABN\\43.html;D\:\\Sauve\\Zotero\\storage\\WWFS9MB5\\43.html}
}

@article{cirelliMeasuringNeuralEntrainment2016,
  title = {Measuring {{Neural Entrainment}} to {{Beat}} and {{Meter}} in {{Infants}}: {{Effects}} of {{Music Background}}},
  url = {http://journal.frontiersin.org/article/10.3389/fnins.2016.00229/full},
  doi = {10.3389/fnins.2016.00229},
  shorttitle = {Measuring {{Neural Entrainment}} to {{Beat}} and {{Meter}} in {{Infants}}},
  abstract = {Caregivers often engage in musical interactions with their infants. For example, parents across cultures sing lullabies and playsongs to their infants from birth. Behavioral studies indicate that infants not only extract beat information, but also group these beats into metrical hierarchies by as early as 6 months of age. However, it is not known how this is accomplished in the infant brain. An EEG frequency-tagging approach has been used successfully with adults to measure neural entrainment to auditory rhythms. The current study is the first to use this technique with infants in order to investigate how infants' brains encode rhythms. Furthermore, we examine how infant and parent music background is associated with individual differences in rhythm encoding. In Experiment 1, EEG was recorded while 7-month-old infants listened to an ambiguous rhythmic pattern that could be perceived to be in two different meters. In Experiment 2, EEG was recorded while 15-month-old infants listened to a rhythmic pattern with an unambiguous meter. In both age groups, information about music background (parent music training, infant music classes, hours of music listening) was collected. Both age groups showed clear EEG responses frequency-locked to the rhythms, at frequencies corresponding to both beat and meter. For the younger infants (Experiment 1), the amplitudes at duple meter frequencies were selectively enhanced for infants enrolled in music classes compared to those who had not engaged in such classes. For the older infants (Experiment 2), amplitudes at beat and meter frequencies were larger for infants with musically-trained compared to musically-untrained parents. These results suggest that the frequency-tagging method is sensitive to individual differences in beat and meter processing in infancy and could be used to track developmental changes.},
  journaltitle = {Auditory Cognitive Neuroscience},
  shortjournal = {Front. Neurosci},
  urldate = {2016-08-02},
  date = {2016},
  pages = {229},
  keywords = {Music,Rhythm,electroencephalography,meter,frequency-tagging,infancy,neural entrainment,steady-state evoked potentials},
  author = {Cirelli, Laura K. and Spinelli, Christina and Nozaradan, Sylvie and Trainor, Laurel J.}
}

@article{leechInformationalFactorsIdentifying2009,
  title = {Informational Factors in Identifying Environmental Sounds in Natural Auditory Scenes},
  volume = {126},
  issn = {0001-4966},
  url = {http://scitation.aip.org/content/asa/journal/jasa/126/6/10.1121/1.3238160},
  doi = {10.1121/1.3238160},
  abstract = {In a non-linguistic analog of the “cocktail-party” scenario, informational and contextual factors were found to affect the recognition of everyday environmental sounds embedded in naturalistic auditory scenes. Short environmental sound targets were presented in a dichotic background scene composed of either a single stereo background scene or a composite background scene created by playing different background scenes to the different ears. The side of presentation, time of onset, and number of target sounds were varied across trials to increase the uncertainty for the participant. Half the sounds were contextually congruent with the background sound (i.e., consistent with the meaningful real-world sound environment represented in the auditory scene) and half were incongruent. The presence of a single competing background scene decreased identification accuracy, suggesting an informational masking effect. In tandem, there was a contextual pop-out effect, with contextually incongruent sounds identified more accurately. However, when targets were incongruent with the real-world context of the background scene, informational masking was reduced. Acoustic analyses suggested that this contextual pop-out effect was driven by a mixture of perceptual differences between the target and background, as well as by higher-level cognitive factors. These findings indicate that identification of environmental sounds in naturalistic backgrounds is an active process that requires integrating perceptual, attentional, and cognitive resources.},
  number = {6},
  journaltitle = {The Journal of the Acoustical Society of America},
  urldate = {2016-07-19},
  date = {2009-12-01},
  pages = {3147-3155},
  keywords = {acoustics,Acoustic sensing,Acoustic source localization,Acoustical effects,Acoustical measurements},
  author = {Leech, Robert and Gygi, Brian and Aydelott, Jennifer and Dick, Frederic},
  file = {D\:\\Sauve\\Zotero\\storage\\XZ87F37T\\Leech et al. - 2009 - Informational factors in identifying environmental.pdf;D\:\\Sauve\\Zotero\\storage\\NA365JNE\\1.html}
}

@article{micheylInfluenceMusicalPsychoacoustical2006,
  title = {Influence of Musical and Psychoacoustical Training on Pitch Discrimination},
  volume = {219},
  issn = {0378-5955},
  url = {http://www.sciencedirect.com/science/article/pii/S037859550600133X},
  doi = {10.1016/j.heares.2006.05.004},
  abstract = {This study compared the influence of musical and psychoacoustical training on auditory pitch discrimination abilities. In a first experiment, pitch discrimination thresholds for pure and complex tones were measured in 30 classical musicians and 30 non-musicians, none of whom had prior psychoacoustical training. The non-musicians’ mean thresholds were more than six times larger than those of the classical musicians initially, and still about four times larger after 2 h of training using an adaptive two-interval forced-choice procedure; this difference is two to three times larger than suggested by previous studies. The musicians’ thresholds were close to those measured in earlier psychoacoustical studies using highly trained listeners, and showed little improvement with training; this suggests that classical musical training can lead to optimal or nearly optimal pitch discrimination performance. A second experiment was performed to determine how much additional training was required for the non-musicians to obtain thresholds as low as those of the classical musicians from experiment 1. Eight new non-musicians with no prior training practiced the frequency discrimination task for a total of 14 h. It took between 4 and 8 h of training for their thresholds to become as small as those measured in the classical musicians from experiment 1. These findings supplement and qualify earlier data in the literature regarding the respective influence of musical and psychoacoustical training on pitch discrimination performance.},
  number = {1–2},
  journaltitle = {Hearing Research},
  shortjournal = {Hearing Research},
  urldate = {2016-07-19},
  date = {2006-09},
  pages = {36-47},
  keywords = {Musicians,pitch,Auditory training,Frequency discrimination,Perceptual learning},
  author = {Micheyl, Christophe and Delhommeau, Karine and Perrot, Xavier and Oxenham, Andrew J.},
  file = {D\:\\Sauve\\Zotero\\storage\\UF5PTAH9\\Micheyl et al. - 2006 - Influence of musical and psychoacoustical training.pdf;D\:\\Sauve\\Zotero\\storage\\X7RSTXGH\\S037859550600133X.html}
}

@article{kishon-rabinPitchDiscriminationAre2011,
  title = {Pitch {{Discrimination}}: {{Are Professional Musicians Better}} than {{Non}}-{{Musicians}}?},
  volume = {12},
  issn = {2191-0286},
  url = {http://www.degruyter.com/view/j/jbcpp.2001.12.2/jbcpp.2001.12.2.125/jbcpp.2001.12.2.125.xml},
  doi = {10.1515/JBCPP.2001.12.2.125},
  shorttitle = {Pitch {{Discrimination}}},
  number = {2},
  journaltitle = {Journal of Basic and Clinical Physiology and Pharmacology},
  urldate = {2016-07-19},
  date = {2011},
  pages = {125--144},
  author = {Kishon-Rabin, L. and Amir, O. and Vexler, Y. and Zaltz, Y.}
}

@unpublished{knightInvestigatingRoleAuditory2016,
  venue = {{London, UK}},
  title = {Investigating the Role of Auditory and Cognitive Factors for Various Speech-Perception-in-Noise Situations in Older Listeners},
  url = {http://c4dm.eecs.qmul.ac.uk/wancm2016/WANCM_programme.pdf},
  eventtitle = {Workshop on {{Auditory Neuroscience}}, {{Cognition}} and {{Modelling}}},
  date = {2016},
  author = {Knight, Sarah and Heinrich, Antje}
}

@article{schellenbergMusicLessonsEnhance2004,
  langid = {english},
  title = {Music {{Lessons Enhance IQ}}},
  volume = {15},
  issn = {0956-7976, 1467-9280},
  url = {http://pss.sagepub.com/content/15/8/511},
  doi = {10.1111/j.0956-7976.2004.00711.x},
  abstract = {The idea that music makes you smarter has received considerable attention from scholars and the media. The present report is the first to test this hypothesis directly with random assignment of a large sample of children (N = 144) to two different types of music lessons (keyboard or voice) or to control groups that received drama lessons or no lessons. IQ was measured before and after the lessons. Compared with children in the control groups, children in the music groups exhibited greater increases in full-scale IQ. The effect was relatively small, but it generalized across IQ subtests, index scores, and a standardized measure of academic achievement. Unexpectedly, children in the drama group exhibited substantial pre- to post-test improvements in adaptive social behavior that were not evident in the music groups.},
  number = {8},
  journaltitle = {Psychological Science},
  shortjournal = {Psychological Science},
  urldate = {2016-07-19},
  date = {2004-08-01},
  pages = {511-514},
  author = {Schellenberg, E. Glenn},
  file = {D\:\\Sauve\\Zotero\\storage\\K7VV3GNZ\\Schellenberg - 2004 - Music Lessons Enhance IQ.pdf;D\:\\Sauve\\Zotero\\storage\\X9N84K9S\\511.html},
  eprinttype = {pmid},
  eprint = {15270994}
}

@article{abioguMusicEducationYouth2015,
  title = {Music {{Education}} and {{Youth Empowerment}}: {{A Conceptual Clarification}}},
  volume = {5},
  shorttitle = {Music {{Education}} and {{Youth Empowerment}}},
  number = {1},
  journaltitle = {Open Journal of Philosophy},
  date = {2015},
  pages = {117--122},
  author = {Abiogu, G. C. and Mbaji, I. N. and Adeogun, A. O.},
  file = {D\:\\Sauve\\Zotero\\storage\\46T9EH29\\undefined.html}
}

@article{kunertIndependentPsychometricEvaluation2016,
  title = {An {{Independent Psychometric Evaluation}} of the {{PROMS Measure}} of {{Music Perception Skills}}},
  volume = {11},
  issn = {1932-6203},
  url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0159103},
  doi = {10.1371/journal.pone.0159103},
  abstract = {The Profile of Music Perception Skills (PROMS) is a recently developed measure of perceptual music skills which has been shown to have promising psychometric properties. In this paper we extend the evaluation of its brief version to three kinds of validity using an individual difference approach. The brief PROMS displays good discriminant validity with working memory, given that it does not correlate with backward digit span ( r  = .04). Moreover, it shows promising criterion validity (association with musical training ( r  = .45), musicianship status ( r  = .48), and self-rated musical talent ( r  = .51)). Finally, its convergent validity, i.e. relation to an unrelated measure of music perception skills, was assessed by correlating the brief PROMS to harmonic closure judgment accuracy. Two independent samples point to good convergent validity of the brief PROMS ( r  = .36;  r  = .40). The same association is still significant in one of the samples when including self-reported music skill in a partial correlation ( r  partial  = .30;  r  partial  = .17). Overall, the results show that the brief version of the PROMS displays a very good pattern of construct validity. Especially its tuning subtest stands out as a valuable part for music skill evaluations in Western samples. We conclude by briefly discussing the choice faced by music cognition researchers between different musical aptitude measures of which the brief PROMS is a well evaluated example.},
  number = {7},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2016-07-18},
  date = {2016-07-11},
  pages = {e0159103},
  keywords = {Attention,music perception,Memory,psychometrics,music cognition,Working memory,Aptitude tests,Long-term memory},
  author = {Kunert, Richard and Willems, Roel M. and Hagoort, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\3Z7UAGJA\\Kunert et al. - 2016 - An Independent Psychometric Evaluation of the PROM.pdf;D\:\\Sauve\\Zotero\\storage\\9SDHB65P\\article.html}
}

@article{lawAssessingMusicalAbilities2012,
  title = {Assessing {{Musical Abilities Objectively}}: {{Construction}} and {{Validation}} of the {{Profile}} of {{Music Perception Skills}}},
  volume = {7},
  issn = {1932-6203},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3532219/},
  doi = {10.1371/journal.pone.0052508},
  shorttitle = {Assessing {{Musical Abilities Objectively}}},
  abstract = {A common approach for determining musical competence is to rely on information about individuals’ extent of musical training, but relying on musicianship status fails to identify musically untrained individuals with musical skill, as well as those who, despite extensive musical training, may not be as skilled. To counteract this limitation, we developed a new test battery (Profile of Music Perception Skills; PROMS) that measures perceptual musical skills across multiple domains: tonal (melody, pitch), qualitative (timbre, tuning), temporal (rhythm, rhythm-to-melody, accent, tempo), and dynamic (loudness). The PROMS has satisfactory psychometric properties for the composite score (internal consistency and test-retest r{$>$}.85) and fair to good coefficients for the individual subtests (.56 to.85). Convergent validity was established with the relevant dimensions of Gordon’s Advanced Measures of Music Audiation and Musical Aptitude Profile (melody, rhythm, tempo), the Musical Ear Test (rhythm), and sample instrumental sounds (timbre). Criterion validity was evidenced by consistently sizeable and significant relationships between test performance and external musical proficiency indicators in all three studies (.38 to.62, p{$<$}.05 to p{$<$}.01). An absence of correlations between test scores and a nonmusical auditory discrimination task supports the battery’s discriminant validity (−.05, ns). The interrelationships among the various subtests could be accounted for by two higher order factors, sequential and sensory music processing. A brief version of the full PROMS is introduced as a time-efficient approximation of the full version of the battery.},
  number = {12},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  urldate = {2016-07-18},
  date = {2012-12-28},
  author = {Law, Lily N. C. and Zentner, Marcel},
  file = {D\:\\Sauve\\Zotero\\storage\\4TT7E94S\\Law and Zentner - 2012 - Assessing Musical Abilities Objectively Construct.pdf},
  eprinttype = {pmid},
  eprint = {23285071},
  pmcid = {PMC3532219}
}

@article{meredithPs13PitchSpelling2006,
  title = {The Ps13 Pitch Spelling Algorithm},
  volume = {35},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298210600834961},
  doi = {10.1080/09298210600834961},
  abstract = {In the ps13 pitch spelling algorithm, the pitch name of a note is assumed to depend on the local key and voice-leading. In the first stage of ps13, the pitch name implied for a note by a tonic is assumed to be the one that lies closest to that tonic on the line of fifths. The strength with which a pitch name is implied for a note is assumed to be proportional to the sum of the frequencies of occurrence, within a context around the note, of the tonics that imply that pitch name. In the second stage of ps13, certain neighbour-note and passing-note errors in the output of the first stage are corrected. An implementation of ps13, called PS13, spelt correctly 99.31\% of the notes in a 195972 note test corpus, . A post-processing phase was added to PS13 in which the pitch names computed by PS13 are transposed by a diminished second if this brings them closer on the line of fifths to the pitch names of the notes in their vicinity. This version of the algorithm spelt 99.43\% of the notes in correctly. When the second stage was removed altogether from PS13, 99.44\% of the notes in were spelt correctly. The ps13-based algorithms achieved higher note accuracies than the algorithms of Temperley, Longuet-Higgins, Cambouropoulos and Chew and Chen on both and a “noisy” version of containing temporal deviations similar to those that occur in MIDI files derived from human performances.},
  number = {2},
  journaltitle = {Journal of New Music Research},
  urldate = {2016-07-17},
  date = {2006-06-01},
  pages = {121-159},
  author = {Meredith, David},
  file = {D\:\\Sauve\\Zotero\\storage\\HQ5DNTTQ\\09298210600834961.html}
}

@article{bassSuppressionAuditoryN12008,
  langid = {english},
  title = {Suppression of the Auditory {{N1}} Event-Related Potential Component with Unpredictable Self-Initiated Tones: Evidence for Internal Forward Models with Dynamic Stimulation},
  volume = {70},
  issn = {0167-8760},
  doi = {10.1016/j.ijpsycho.2008.06.005},
  shorttitle = {Suppression of the Auditory {{N1}} Event-Related Potential Component with Unpredictable Self-Initiated Tones},
  abstract = {Internally operating forward model mechanisms enable the organism to discriminate the sensory consequences of one's own actions from other sensory events. The present event-related potential (ERP) study compared the processing of self-initiated tones with the processing of externally-initiated but otherwise identical tones. In different conditions, frequency and onset of the sound were either predictable or unpredictable. The amplitudes of the N1 component of the ERP for the self-initiated relative to the ones for externally-initiated sounds were significantly attenuated even when the particular frequency or sound onset could not be predicted by the subject. These results support internal forward model mechanisms which dynamically predict the sensorial consequences of ones own motor acts even in face of uncertainties with respect to the frequency of the sound and its onset. Moreover, the attenuation effect was reduced when the frequency was unpredictable suggesting that it is easier to discriminate a self-initiated sound with exact foreknowledge.},
  number = {2},
  journaltitle = {International Journal of Psychophysiology: Official Journal of the International Organization of Psychophysiology},
  shortjournal = {Int J Psychophysiol},
  date = {2008-11},
  pages = {137-143},
  keywords = {Evoked Potentials,electroencephalography,Acoustic Stimulation,humans,Adult,Female,Male,Discrimination (Psychology),Young Adult,Models; Neurological,Brain Mapping,Adolescent,Data Interpretation; Statistical,Movement,Self Stimulation},
  author = {Bäss, Pamela and Jacobsen, Thomas and Schröger, Erich},
  eprinttype = {pmid},
  eprint = {18627782}
}

@article{nagaiBrainActivityRelating2004,
  langid = {english},
  title = {Brain Activity Relating to the Contingent Negative Variation: An {{fMRI}} Investigation},
  volume = {21},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2003.10.036},
  shorttitle = {Brain Activity Relating to the Contingent Negative Variation},
  abstract = {The contingent negative variation (CNV) is a long-latency electroencephalography (EEG) surface negative potential with cognitive and motor components, observed during response anticipation. CNV is an index of cortical arousal during orienting and attention, yet its functional neuroanatomical basis is poorly understood. We used functional magnetic resonance imaging (fMRI) with simultaneous EEG and recording of galvanic skin response (GSR) to investigate CNV-related central neural activity and its relationship to peripheral autonomic arousal. In a group analysis, blood oxygenation level dependent (BOLD) activity during the period of CNV generation was enhanced in thalamus, somatomotor cortex, bilateral midcingulate, supplementary motor, and insular cortices. Enhancement of CNV-related activity in anterior and midcingulate, SMA, and insular cortices was associated with decreases in peripheral sympathetic arousal. In a subset of subjects in whom we acquired simultaneous EEG and fMRI data, we observed activity in bilateral thalamus, anterior cingulate, and supplementary motor cortex that was modulated by trial-by-trial amplitude of CNV. These findings provide a likely functional neuroanatomical substrate for the CNV and demonstrate modulation of components of this neural circuitry by peripheral autonomic arousal. Moreover, these data suggest a mechanistic model whereby thalamocortical interactions regulate CNV amplitude.},
  number = {4},
  journaltitle = {NeuroImage},
  shortjournal = {Neuroimage},
  date = {2004-04},
  pages = {1232-1241},
  keywords = {brain,electroencephalography,Magnetic Resonance Imaging,Contingent Negative Variation,Neural Pathways,humans,Peripheral Nervous System,Adult,Female,Male,REACTION time,Cerebral Cortex,Dominance; Cerebral,Brain Mapping,Arousal,Galvanic Skin Response,Gyrus Cinguli,Image Enhancement,Image Processing; Computer-Assisted,Oxygen,Psychomotor Performance,Sympathetic Nervous System,Thalamus},
  author = {Nagai, Y. and Critchley, H. D. and Featherstone, E. and Fenwick, P. B. C. and Trimble, M. R. and Dolan, R. J.},
  eprinttype = {pmid},
  eprint = {15050551}
}

@article{walterContingentNegativeVariation1964,
  title = {Contingent Negative Variation: An Electric Sign of Sensori-Motor Association and Expectancy in the Human Brain},
  volume = {203},
  url = {http://adsabs.harvard.edu/abs/1964Natur.203..380W%EF%BF%BD%C3%9C},
  shorttitle = {Contingent Negative Variation},
  journaltitle = {Nature},
  urldate = {2016-07-17},
  date = {1964},
  pages = {380--384},
  author = {Walter, W. and Cooper, R. and Aldridge, V. J. and McCallum, W. C. and Winter, A. L.},
  file = {D\:\\Sauve\\Zotero\\storage\\7XZNCEEZ\\1964Natur.203..html}
}

@article{barVisualObjectsContext2004,
  langid = {english},
  title = {Visual Objects in Context},
  volume = {5},
  issn = {1471-003X},
  url = {http://www.nature.com/nrn/journal/v5/n8/abs/nrn1476.html},
  doi = {10.1038/nrn1476},
  abstract = {We see the world in scenes, where visual objects occur in rich surroundings, often embedded in a typical context with other related objects. How does the human brain analyse and use these common associations? This article reviews the knowledge that is available, proposes specific mechanisms for the contextual facilitation of object recognition, and highlights important open questions. Although much has already been revealed about the cognitive and cortical mechanisms that subserve recognition of individual objects, surprisingly little is known about the neural underpinnings of contextual analysis and scene perception. Building on previous findings, we now have the means to address the question of how the brain integrates individual elements to construct the visual experience.},
  number = {8},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nat Rev Neurosci},
  urldate = {2016-07-17},
  date = {2004-08},
  pages = {617-629},
  author = {Bar, Moshe},
  file = {D\:\\Sauve\\Zotero\\storage\\KW9ZTC7W\\nrn1476.html}
}

@article{jehielRevisitingGamesIncomplete2008,
  title = {Revisiting Games of Incomplete Information with Analogy-Based Expectations},
  volume = {62},
  issn = {0899-8256},
  url = {http://www.sciencedirect.com/science/article/pii/S0899825607001182},
  doi = {10.1016/j.geb.2007.06.006},
  abstract = {This paper studies the effects of analogy-based expectations in static two-player games of incomplete information. Players are assumed to be boundedly rational in the way they forecast their opponent's state-contingent strategy: they bundle states into analogy classes and play best-responses to their opponent's average strategy in those analogy classes. We provide general properties of analogy-based expectation equilibria and apply the model to a variety of well known games. We characterize conditions on the analogy partitions for successful coordination in coordination games under incomplete information [Rubinstein, A., 1989. The electronic mail game: Strategic behavior under ‘almost common knowledge’. Amer. Econ. Rev. 79, 385–391], we show how analogy grouping of the receiver may facilitate information transmission in Crawford and Sobel's cheap talk games [Crawford, V.P., Sobel, J., 1982. Strategic information transmission. Econometrica 50, 1431–1451], and we show how analogy grouping may give rise to betting in zero-sum betting games such as those studied to illustrate the no trade theorem.},
  number = {2},
  journaltitle = {Games and Economic Behavior},
  shortjournal = {Games and Economic Behavior},
  urldate = {2016-07-17},
  date = {2008-03},
  pages = {533-557},
  keywords = {Analogy expectation,Bayesian games,Betting,Bounded rationality,Coordination,Incomplete information,Strategic information transmission},
  author = {Jehiel, Philippe and Koessler, Frédéric},
  file = {D\:\\Sauve\\Zotero\\storage\\WKMTHIIM\\Jehiel and Koessler - 2008 - Revisiting games of incomplete information with an.pdf;D\:\\Sauve\\Zotero\\storage\\M5SEN32M\\S0899825607001182.html}
}

@article{jehielAnalogybasedExpectationEquilibrium2005,
  title = {Analogy-Based Expectation Equilibrium},
  volume = {123},
  issn = {0022-0531},
  url = {http://www.sciencedirect.com/science/article/pii/S0022053104000250},
  doi = {10.1016/j.jet.2003.12.003},
  abstract = {In complex situations, agents use simplified representations to learn how their environment may react. I assume that agents bundle nodes at which other agents must move into analogy classes, and agents only try to learn the average behavior in every class. Specifically, I propose a new solution concept for multi-stage games with perfect information: at every node players choose best-responses to their analogy-based expectations, and expectations correctly represent the average behavior in every class. The solution concept is shown to differ from existing concepts, and it is applied to a variety of games, in particular the centipede game, and ultimatum/bargaining games. The approach explains in a new way why players may Pass for a large number of periods in the centipede game, and why the responder need not be stuck to his reservation value in ultimatum games. Some possible avenues for endogenizing the analogy grouping are also suggested.},
  number = {2},
  journaltitle = {Journal of Economic Theory},
  shortjournal = {Journal of Economic Theory},
  urldate = {2016-07-17},
  date = {2005-08},
  pages = {81-104},
  keywords = {expectation,Bounded rationality,Game theory,Reasoning by analogy},
  author = {Jehiel, Philippe},
  file = {D\:\\Sauve\\Zotero\\storage\\3JJUTVW9\\Jehiel - 2005 - Analogy-based expectation equilibrium.pdf;D\:\\Sauve\\Zotero\\storage\\D9RTT7CW\\S0022053104000250.html}
}

@article{cuddyExpectanciesGeneratedMelodic1995,
  langid = {english},
  title = {Expectancies Generated by Melodic Intervals: Perceptual Judgments of Melodic Continuity},
  volume = {57},
  issn = {0031-5117},
  shorttitle = {Expectancies Generated by Melodic Intervals},
  abstract = {The present study tested quantified predictors based on the bottom-up principles of Narmour's (1990) implication-realization model of melodic expectancy against continuity ratings collected for a tone that followed a two-tone melodic beginning. Twenty-four subjects (12 musically trained, 12 untrained) were presented with each of eight melodic intervals--two successive tones which they were asked to consider as the beginning of a melody. On each trial, a melodic interval was followed by a third tone, one of the 25 chromatic notes within the range one octave below to one octave above the second tone of the interval. The subjects were asked to rate how well the third tone continued the melody. A series of regression analyses was performed on the continuation ratings, and a final model to account for the variance in the ratings is proposed. Support was found for three of Narmour's principles and a modified version of a fourth. Support was also found for predictor variables based on the pitch organization of tonal harmonic music. No significant differences between the levels of musical training were encountered.},
  number = {4},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Percept Psychophys},
  date = {1995-05},
  pages = {451-462},
  keywords = {Attention,Music,Pitch Discrimination,humans,Adult,Female,Male,Psychoacoustics,Set (Psychology)},
  author = {Cuddy, L. L. and Lunney, C. A.},
  eprinttype = {pmid},
  eprint = {7596743}
}

@article{manzaraEntropyMusicExperiment1992,
  eprinttype = {jstor},
  eprint = {1513213},
  title = {On the {{Entropy}} of {{Music}}: {{An Experiment}} with {{Bach Chorale Melodies}}},
  volume = {2},
  issn = {0961-1215},
  doi = {10.2307/1513213},
  shorttitle = {On the {{Entropy}} of {{Music}}},
  abstract = {The information content, or 'entropy', of a piece of music cannot be determined in the abstract, but depends on the listener's familiarity with, and knowledge of, the genre to which it belongs. This paper describes an ongoing experiment designed to investigate human listeners' models of music by having them guess successive notes in a piece. The first part of the experiment was administered by a computer program, and in order to elicit subjective probabilities, listeners gambled on their guesses. The study was restricted to the music of the Bach Chorales and, in particular, to the succession of pitches that comprise the melody-although this methodology is also generally applicable to other musical parameters and to other genres.},
  number = {1},
  journaltitle = {Leonardo Music Journal},
  shortjournal = {Leonardo Music Journal},
  date = {1992},
  pages = {81-88},
  author = {Manzara, Leonard C. and Witten, Ian H. and James, Mark}
}

@article{pettenInteractionsSentenceContext1990,
  langid = {english},
  title = {Interactions between Sentence Context and Word Frequencyinevent-Related Brainpotentials},
  volume = {18},
  issn = {0090-502X, 1532-5946},
  url = {http://link.springer.com/article/10.3758/BF03197127},
  doi = {10.3758/BF03197127},
  abstract = {Event-related brain potentials (ERPs) were recorded as subjects silently read a set of unrelated sentences. The ERP responses elicited by open-class words were sorted according word frequency and the ordinal position of the eliciting word within its sentence. We observed a strong inverse correlation between sentence position and the amplitude of the N400 component of the ERP. In addition, we found that less frequent words were associated with larger N400s than were more frequent words, but only if theeliciting words occurred early in their respective sentences. We take this interaction between sentence position and word frequency as evidence that frequency does not play a mandatory role in wordrecognition, but can besuperseded by the contextual constraint provided by a sentence.},
  number = {4},
  journaltitle = {Memory \& Cognition},
  shortjournal = {Memory \& Cognition},
  urldate = {2016-07-16},
  date = {1990},
  pages = {380-393},
  author = {Petten, Cyma Van and Kutas, Marta},
  file = {D\:\\Sauve\\Zotero\\storage\\W64PEC29\\Petten and Kutas - Interactions between sentence context and word fre.pdf;D\:\\Sauve\\Zotero\\storage\\E84QIWAX\\10.html}
}

@article{koelschBrainIndicesMusic2000,
  langid = {english},
  title = {Brain Indices of Music Processing: "Nonmusicians" Are Musical},
  volume = {12},
  issn = {0898-929X},
  shorttitle = {Brain Indices of Music Processing},
  abstract = {Only little systematic research has examined event-related brain potentials (ERPs) elicited by the cognitive processing of music. The present study investigated how music processing is influenced by a preceding musical context, affected by the task relevance of unexpected chords, and influenced by the degree and the probability of violation. Four experiments were conducted in which "nonmusicians" listened to chord sequences, which infrequently contained a chord violating the sound expectancy of listeners. Integration of in-key chords into the musical context was reflected as a late negative-frontal deflection in the ERPs. This negative deflection declined towards the end of a chord sequence, reflecting normal buildup of musical context. Brain waves elicited by chords with unexpected notes revealed two ERP effects: an early right-hemispheric preponderant-anterior negativity, which was taken to reflect the violation of sound expectancy; and a late bilateral-frontal negativity. The late negativity was larger compared to in-key chords and taken to reflect the higher degree of integration needed for unexpected chords. The early right-anterior negativity (ERAN) was unaffected by the task relevance of unexpected chords. The amplitudes of both early and late negativities were found to be sensitive to the degree of musical expectancy induced by the preceding harmonic context, and to the probability for deviant acoustic events. The employed experimental design opens a new field for the investigation of music processing. Results strengthen the hypothesis of an implicit musical ability of the human brain.},
  number = {3},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {J Cogn Neurosci},
  date = {2000-05},
  pages = {520-541},
  keywords = {brain,Auditory Perception,Music,electroencephalography,Acoustic Stimulation,humans,Adult,Female,Male,Evoked Potentials; Auditory,Probability},
  author = {Koelsch, S. and Gunter, T. and Friederici, A. D. and Schröger, E.},
  eprinttype = {pmid},
  eprint = {10931776}
}

@collection{juslinMusicEmotionTheory2001,
  location = {{New York, NY, US}},
  title = {Music and Emotion: {{Theory}} and Research},
  volume = {viii},
  isbn = {0-19-263189-6 (Hardcover); 0-19-263188-8 (Paperback)},
  shorttitle = {Music and Emotion},
  abstract = {This volume presents an integrative review of the relationship between music and emotions. The first section reflects the various interdisciplinary perspectives, taking on board views from philosophy, psychology, musicology, biology, anthropology, and sociology. The second section addresses the role of our emotions in the composition of music, the ways that emotions can be communicated via musical structure, and the use of music to express emotions within the cinema. The third section looks at the emotions of the performer--how they communicate emotion, how their emotional state affects their performance. The final section looks at the ways in which our emotions are guided and influenced while listening to music, whether actively or passively.},
  pagetotal = {487},
  series = {Series in Affective Science.},
  publisher = {{Oxford University Press}},
  date = {2001},
  keywords = {Musicians,*Music,*Emotions,Artists},
  editor = {Juslin, Patrik N. and Sloboda, John A.},
  file = {D\:\\Sauve\\Zotero\\storage\\6NP89T9Z\\index.html}
}

@article{koelschAmygdalaActivityCan2008,
  langid = {english},
  title = {Amygdala Activity Can Be Modulated by Unexpected Chord Functions during Music Listening},
  volume = {19},
  issn = {1473-558X},
  doi = {10.1097/WNR.0b013e32831a8722},
  abstract = {Numerous earlier studies have investigated the cognitive processing of musical syntax with regular and irregular chord sequences. However, irregular sequences may also be perceived as unexpected, and therefore have a different emotional valence than regular sequences. We provide behavioral data showing that irregular chord functions presented in chord sequence paradigms are perceived as less pleasant than regular sequences. A reanalysis of functional MRI data showed increased blood oxygen level-dependent signal changes bilaterally in the amygdala in response to music-syntactically irregular (compared with regular) chord functions. The combined data indicate that music-syntactically irregular events elicit brain activity related to emotional processes, and that, in addition to intensely pleasurable music or highly unpleasant music, single chord functions can also modulate amygdala activity.},
  number = {18},
  journaltitle = {Neuroreport},
  shortjournal = {Neuroreport},
  date = {2008-12-03},
  pages = {1815-1819},
  keywords = {brain,Auditory Perception,Music,Pitch Discrimination,Magnetic Resonance Imaging,Acoustic Stimulation,humans,Female,Male,Young Adult,emotions,amygdala,REACTION time,Functional Laterality,Psychomotor Performance},
  author = {Koelsch, Stefan and Fritz, Thomas and Schlaug, Gottfried},
  eprinttype = {pmid},
  eprint = {19050462}
}

@article{bodMemoryBasedModelsMelodic2002,
  title = {Memory-{{Based Models}} of {{Melodic Analysis}}: {{Challenging}} the {{Gestalt Principles}}},
  volume = {31},
  issn = {0929-8215},
  url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.31.1.27.8106},
  doi = {10.1076/jnmr.31.1.27.8106},
  shorttitle = {Memory-{{Based Models}} of {{Melodic Analysis}}},
  abstract = {We argue for a memory-based approach to music analysis which works with concrete musical experiences rather than with rules or principles. New pieces of music are analyzed by combining fragments from structures of previously encountered pieces. The occurrence-frequencies of the fragments are used to determine the preferred analysis of a piece. We test some instances of this approach against a set of 1,000 manually annotated folksongs from the Essen Folksong Collection, yielding up to 85.9\% phrase accuracy. A qualitative analysis of our results indicates that there are grouping phenomena that challenge the commonly accepted Gestalt principles of proximity, similarity and parallelism. These grouping phenomena can neither be explained by other musical factors, such as meter and harmony. We argue that music perception may be much more memory-based than previously assumed.},
  number = {1},
  journaltitle = {Journal of New Music Research},
  urldate = {2016-07-16},
  date = {2002-03-01},
  pages = {27-36},
  author = {Bod, Rens},
  file = {D\:\\Sauve\\Zotero\\storage\\7JFF4U88\\jnmr.31.1.27.html}
}

@book{bharuchaMUSACTConnectionistModel1987,
  title = {{{MUSACT}}: {{A}} Connectionist Model of Musical Harmony. {{InProceedings}} of the {{Ninth Annual Conference}} of the {{Cognitive Science Society}} (Pp. 508–517)},
  shorttitle = {{{MUSACT}}},
  publisher = {{Hillsdale, NJ: Erlbaum}},
  date = {1987},
  author = {Bharucha, J. J.}
}

@book{koelschBrainMusic2012,
  langid = {english},
  title = {Brain and {{Music}}},
  isbn = {978-1-119-94311-2},
  abstract = {A comprehensive survey of the latest neuroscientific research into the effects of music on the brain   Covers a variety of topics fundamental for music perception, including musical syntax, musical semantics, music and action, music and emotion Includes general introductory chapters to engage a broad readership, as well as a wealth of detailed research material for experts Offers the most empirical (and most systematic) work on the topics of neural correlates of musical syntax and musical semantics Integrates research from different domains (such as music, language, action and emotion both theoretically and empirically, to create a comprehensive theory of music psychology},
  pagetotal = {323},
  publisher = {{John Wiley \& Sons}},
  date = {2012-03-22},
  keywords = {Psychology / General,Science / Life Sciences / Neuroscience},
  author = {Koelsch, Stefan}
}

@article{snyderEffectsAttentionNeuroelectric2006,
  title = {Effects of {{Attention}} on {{Neuroelectric Correlates}} of {{Auditory Stream Segregation}}},
  volume = {18},
  issn = {0898-929X},
  url = {http://dx.doi.org/10.1162/089892906775250021},
  doi = {10.1162/089892906775250021},
  abstract = {A general assumption underlying auditory scene analysis is that the initial grouping of acoustic elements is independent of attention. The effects of attention on auditory stream segregation were investigated by recording event-related potentials (ERPs) while participants either attended to sound stimuli and indicated whether they heard one or two streams or watched a muted movie. The stimuli were pure-tone ABA-patterns that repeated for 10.8 sec with a stimulus onset asynchrony between A and B tones of 100 msec in which the A tone was fixed at 500 Hz, the B tone could be 500, 625, 750, or 1000 Hz, and was a silence. In both listening conditions, an enhancement of the auditory-evoked response (P1-N1-P2 and N1c) to the B tone varied with f and correlated with perception of streaming. The ERP from 150 to 250 msec after the beginning of the repeating ABA-patterns became more positive during the course of the trial and was diminished when participants ignored the tones, consistent with behavioral studies indicating that streaming takes several seconds to build up. The N1c enhancement and the buildup over time were larger at right than left temporal electrodes, suggesting a right-hemisphere dominance for stream segregation. Sources in Heschl's gyrus accounted for the ERP modulations related to f-based segregation and buildup. These findings provide evidence for two cortical mechanisms of streaming: automatic segregation of sounds and attention-dependent buildup process that integrates successive tones within streams over several seconds.},
  number = {1},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {Journal of Cognitive Neuroscience},
  urldate = {2016-07-16},
  date = {2006-01-01},
  pages = {1-13},
  author = {Snyder, Joel S. and Alain, Claude and Picton, Terence W.},
  file = {D\:\\Sauve\\Zotero\\storage\\AQ876MFW\\089892906775250021.html}
}

@article{marozeauEffectVisualCues2010,
  langid = {english},
  title = {The Effect of Visual Cues on Auditory Stream Segregation in Musicians and Non-Musicians},
  volume = {5},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0011297},
  abstract = {BACKGROUND: The ability to separate two interleaved melodies is an important factor in music appreciation. This ability is greatly reduced in people with hearing impairment, contributing to difficulties in music appreciation. The aim of this study was to assess whether visual cues, musical training or musical context could have an effect on this ability, and potentially improve music appreciation for the hearing impaired.
METHODS: Musicians (N = 18) and non-musicians (N = 19) were asked to rate the difficulty of segregating a four-note repeating melody from interleaved random distracter notes. Visual cues were provided on half the blocks, and two musical contexts were tested, with the overlap between melody and distracter notes either gradually increasing or decreasing.
CONCLUSIONS: Visual cues, musical training, and musical context all affected the difficulty of extracting the melody from a background of interleaved random distracter notes. Visual cues were effective in reducing the difficulty of segregating the melody from distracter notes, even in individuals with no musical training. These results are consistent with theories that indicate an important role for central (top-down) processes in auditory streaming mechanisms, and suggest that visual cues may help the hearing-impaired enjoy music.},
  number = {6},
  journaltitle = {PloS One},
  shortjournal = {PLoS ONE},
  date = {2010},
  pages = {e11297},
  keywords = {Music,music perception,pitch perception,humans,Perception,music cognition,Bioacoustics,Sensory perception,Vision,Photic Stimulation,Vision; Ocular,Visual impairments,Music cognition,Music perception,Pitch perception},
  author = {Marozeau, Jeremy and Innes-Brown, Hamish and Grayden, David B. and Burkitt, Anthony N. and Blamey, Peter J.},
  file = {D\:\\Sauve\\Zotero\\storage\\6MRAPKH7\\Marozeau et al. - 2010 - The Effect of Visual Cues on Auditory Stream Segre.pdf;D\:\\Sauve\\Zotero\\storage\\YILNH4IY\\Marozeau et al. - 2010 - The Effect of Visual Cues on Auditory Stream Segre.pdf;D\:\\Sauve\\Zotero\\storage\\BBDX2SGP\\article.html;D\:\\Sauve\\Zotero\\storage\\PZJZ9PXG\\article.html},
  eprinttype = {pmid},
  eprint = {20585606},
  pmcid = {PMC2890685}
}

@article{hilliardEffectFamiliarityBackground1979,
  title = {Effect of Familiarity with Background Music on Performance of Simple and Difficult Reading Comprehension Tasks},
  volume = {49},
  issn = {1558-688X(Electronic);0031-5125(Print)},
  doi = {10.2466/pms.1979.49.3.713},
  abstract = {For 64 undergraduates, varied musical selections did not offset scores on Sequential Tests of Educational Progress, but scores on the easier sections were higher than those on more difficult ones. Scores made with familiar music were higher than those made with unfamiliar music. (2 ref)},
  number = {3},
  journaltitle = {Perceptual and Motor Skills},
  date = {1979},
  pages = {713-714},
  keywords = {college students,*Music,*Reading Comprehension,*Familiarity,*Performance,*Test Scores,Difficulty Level (Test)},
  author = {Hilliard, Mark O and Tolin, Philip},
  file = {D\:\\Sauve\\Zotero\\storage\\2DGQZ7JC\\1981-24429-001.html;D\:\\Sauve\\Zotero\\storage\\XRC25WXN\\1981-24429-001.html}
}

@article{fishmanAuditoryStreamSegregation2004,
  title = {Auditory Stream Segregation in Monkey Auditory Cortex: Effects of Frequency Separation, Presentation Rate, and Tone Duration},
  volume = {116},
  issn = {0001-4966},
  url = {http://scitation.aip.org/content/asa/journal/jasa/116/3/10.1121/1.1778903},
  doi = {10.1121/1.1778903},
  shorttitle = {Auditory Stream Segregation in Monkey Auditory Cortex},
  abstract = {Auditory stream segregation refers to the organization of sequential sounds into “perceptual streams” reflecting individual environmental sound sources. In the present study, sequences of alternating high and low tones, “…ABAB…,” similar to those used in psychoacoustic experiments on stream segregation, were presented to awake monkeys while neural activity was recorded in primary auditory cortex (A1). Tone frequency separation (ΔF), tone presentation rate (PR), and tone duration (TD) were systematically varied to examine whether neural responses correlate with effects of these variables on perceptual stream segregation. “A” tones were fixed at the best frequency of the recording site, while “B” tones were displaced in frequency from “A” tones by an amount =Δ F . As PR increased, “B” tone responses decreased in amplitude to a greater extent than “A” tone responses, yielding neural response patterns dominated by “A” tone responses occurring at half the alternation rate. Increasing TD facilitated the differential attenuation of “B” tone responses. These findings parallel psychoacoustic data and suggest a physiological model of stream segregation whereby increasing ΔF, PR, or TD enhances spatial differentiation of “A” tone and “B” tone responses along the tonotopic map in A1.},
  number = {3},
  journaltitle = {The Journal of the Acoustical Society of America},
  urldate = {2016-07-16},
  date = {2004-09-01},
  pages = {1656-1670},
  keywords = {auditory grouping,Acoustical effects,Acoustic modeling,Psychological acoustics,Sound discrimination},
  author = {Fishman, Yonatan I. and Arezzo, Joseph C. and Steinschneider, Mitchell},
  file = {D\:\\Sauve\\Zotero\\storage\\MAPB46MM\\1.html}
}

@book{wundtGrundzugePhysiologischenPsychologie1874,
  location = {{Leipzig}},
  title = {Grundzuge Der {{Physiologischen Psychologie}}},
  publisher = {{Engelmann}},
  date = {1874},
  author = {Wundt, W.}
}

@book{sullyOutlinesPsychology1981,
  location = {{New York}},
  title = {Outlines of {{Psychology}}},
  publisher = {{Appleton \& Co.}},
  date = {1981},
  author = {Sully, J.}
}

@article{pyleExperimentalStudyExpectation1909,
  eprinttype = {jstor},
  eprint = {1412973},
  title = {An {{Experimental Study}} of {{Expectation}}},
  volume = {20},
  issn = {0002-9556},
  doi = {10.2307/1412973},
  number = {4},
  journaltitle = {The American Journal of Psychology},
  shortjournal = {The American Journal of Psychology},
  date = {1909},
  pages = {530-569},
  author = {Pyle, William Henry}
}

@article{rajendranTemporalPredictabilityGrouping2013,
  langid = {english},
  title = {Temporal Predictability as a Grouping Cue in the Perception of Auditory Streams},
  volume = {134},
  issn = {1520-8524},
  doi = {10.1121/1.4811161},
  abstract = {This study reports a role of temporal regularity on the perception of auditory streams. Listeners were presented with two-tone sequences in an A-B-A-B rhythm that was either regular or had a controlled amount of temporal jitter added independently to each of the B tones. Subjects were asked to report whether they perceived one or two streams. The percentage of trials in which two streams were reported substantially and significantly increased with increasing amounts of temporal jitter. This suggests that temporal predictability may serve as a binding cue during auditory scene analysis.},
  number = {1},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  date = {2013-07},
  pages = {EL98-104},
  keywords = {Attention,Pitch Discrimination,Cues,time perception,humans,Psychoacoustics,Sound Spectrography,Illusions},
  author = {Rajendran, Vani G. and Harper, Nicol S. and Willmore, Benjamin D. and Hartmann, William M. and Schnupp, Jan W. H.},
  eprinttype = {pmid},
  eprint = {23862914},
  pmcid = {PMC4491984}
}

@article{bendixenRegularityExtractionNonadjacent2012,
  langid = {english},
  title = {Regularity Extraction from Non-Adjacent Sounds},
  volume = {3},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00143},
  abstract = {The regular behavior of sound sources helps us to make sense of the auditory environment. Regular patterns may, for instance, convey information on the identity of a sound source (such as the acoustic signature of a train moving on the rails). Yet typically, this signature overlaps in time with signals emitted from other sound sources. It is generally assumed that auditory regularity extraction cannot operate upon this mixture of signals because it only finds regularities between adjacent sounds. In this view, the auditory environment would be grouped into separate entities by means of readily available acoustic cues such as separation in frequency and location. Regularity extraction processes would then operate upon the resulting groups. Our new experimental evidence challenges this view. We presented two interleaved sound sequences which overlapped in frequency range and shared all acoustic parameters. The sequences only differed in their underlying regular patterns. We inserted deviants into one of the sequences to probe whether the regularity was extracted. In the first experiment, we found that these deviants elicited the mismatch negativity (MMN) component. Thus the auditory system was able to find the regularity between the non-adjacent sounds. Regularity extraction was not influenced by sequence cohesiveness as manipulated by the relative duration of tones and silent inter-tone-intervals. In the second experiment, we showed that a regularity connecting non-adjacent sounds was discovered only when the intervening sequence also contained a regular pattern, but not when the intervening sounds were randomly varying. This suggests that separate regular patterns are available to the auditory system as a cue for identifying signals coming from distinct sound sources. Thus auditory regularity extraction is not necessarily confined to a processing stage after initial sound grouping, but may precede grouping when other acoustic cues are unavailable.},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  date = {2012},
  pages = {143},
  keywords = {Mismatch negativity (MMN),auditory processing,auditory object formation,implicit learning,integration,non-adjacent dependencies,segregation,sound grouping},
  author = {Bendixen, Alexandra and Schröger, Erich and Ritter, Walter and Winkler, István},
  eprinttype = {pmid},
  eprint = {22661959},
  pmcid = {PMC3356878}
}

@article{bendixenRegularPatternsStabilize2010,
  langid = {english},
  title = {Regular Patterns Stabilize Auditory Streams},
  volume = {128},
  issn = {1520-8524},
  doi = {10.1121/1.3500695},
  abstract = {The auditory system continuously parses the acoustic environment into auditory objects, usually representing separate sound sources. Sound sources typically show characteristic emission patterns. These regular temporal sound patterns are possible cues for distinguishing sound sources. The present study was designed to test whether regular patterns are used as cues for source distinction and to specify the role that detecting these regularities may play in the process of auditory stream segregation. Participants were presented with tone sequences, and they were asked to continuously indicate whether they perceived the tones in terms of a single coherent sequence of sounds (integrated) or as two concurrent sound streams (segregated). Unknown to the participant, in some stimulus conditions, regular patterns were present in one or both putative streams. In all stimulus conditions, participants' perception switched back and forth between the two sound organizations. Importantly, regular patterns occurring in either one or both streams prolonged the mean duration of two-stream percepts, whereas the duration of one-stream percepts was unaffected. These results suggest that temporal regularities are utilized in auditory scene analysis. It appears that the role of this cue lies in stabilizing streams once they have been formed on the basis of simpler acoustic cues.},
  number = {6},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  date = {2010-12},
  pages = {3658-3666},
  keywords = {Auditory Perception,Cues,time perception,Acoustic Stimulation,Auditory Pathways,humans,Auditory Threshold,Female,Male,Time Factors,Psychoacoustics,Young Adult,Audiometry,Pattern Recognition; Physiological,Signal Detection; Psychological},
  author = {Bendixen, Alexandra and Denham, Susan L. and Gyimesi, Kinga and Winkler, István},
  eprinttype = {pmid},
  eprint = {21218898}
}

@article{mirandaDoubleDissociationRules2007,
  title = {Double Dissociation between Rules and Memory in Music: {{An}} Event-Related Potential Study},
  volume = {38},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811907006635},
  doi = {10.1016/j.neuroimage.2007.07.034},
  shorttitle = {Double Dissociation between Rules and Memory in Music},
  abstract = {Language and music share a number of characteristics. Crucially, both domains depend on both rules and memorized representations. Double dissociations between the neurocognition of rule-governed and memory-based knowledge have been found in language but not music. Here, the neural bases of both of these aspects of music were examined with an event-related potential (ERP) study of note violations in melodies. Rule-only violations consisted of out-of-key deviant notes that violated tonal harmony rules in novel (unfamiliar) melodies. Memory-only violations consisted of in-key deviant notes in familiar well-known melodies; these notes followed musical rules but deviated from the actual melodies. Finally, out-of-key notes in familiar well-known melodies constituted violations of both rules and memory. All three conditions were presented, within-subjects, to healthy young adults, half musicians and half non-musicians. The results revealed a double dissociation, independent of musical training, between rules and memory: both rule violation conditions, but not the memory-only violations, elicited an early, somewhat right-lateralized anterior-central negativity (ERAN), consistent with previous studies of rule violations in music, and analogous to the early left-lateralized anterior negativities elicited by rule violations in language. In contrast, both memory violation conditions, but not the rule-only violation, elicited a posterior negativity that might be characterized as an N400, an ERP component that depends, at least in part, on the processing of representations stored in long-term memory, both in language and in other domains. The results suggest that the neurocognitive rule/memory dissociation extends from language to music, further strengthening the similarities between the two domains.},
  number = {2},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2016-07-15},
  date = {2007-11-01},
  pages = {331-345},
  author = {Miranda, Robbin A. and Ullman, Michael T.},
  file = {D\:\\Sauve\\Zotero\\storage\\ECURF76G\\Miranda and Ullman - 2007 - Double dissociation between rules and memory in mu.pdf;D\:\\Sauve\\Zotero\\storage\\7QLZ5UNM\\S1053811907006635.html;D\:\\Sauve\\Zotero\\storage\\CQSHKXG6\\S1053811907006635.html;D\:\\Sauve\\Zotero\\storage\\HPANSJ5E\\PMC2186212.html}
}

@article{neuhausProcessingPitchTime2008,
  title = {Processing of Pitch and Time Sequences in Music},
  volume = {441},
  issn = {0304-3940},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394008007829},
  doi = {10.1016/j.neulet.2008.05.101},
  abstract = {Pitch and duration – either as written symbols or in auditory form – are the basic structural properties in tones that form a melodic sequence. From the cognitive perspective, it is still a matter of debate whether, and at which processing stage, these two factors are processed independently or interdependently. The present study addresses this issue from the neuroscientist's point of view by measuring event-related potentials (ERPs) in musicians and non-musicians. Either the pitches or the durations of the tones, or both, were permuted randomly over a set of melodies in order to remove all sequential ordering with respect to these factors. Effects of both, pitch and time order, on the peak amplitudes of the P1-N1-P2 complex were observed. ANOVA revealed that sequential processing may depend on the different levels of skill in analytical hearing. For musicians, strong interaction effects for all three ERP components corroborated the interdependence of pitch and time processing. Musicians also seem to rely on coherent time structure more than non-musicians and showed enlarged P1 and P2 components whenever tone duration, either with or without preserved pitch, was at random. Non-musicians tend to use ordered pitch relations for perceptual orientation, and main effects without any interactions might indicate some kind of independent processing of both dimensions at some processing stages.},
  number = {1},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  urldate = {2016-07-15},
  date = {2008-08-15},
  pages = {11-15},
  keywords = {Time,pitch,event-related potentials,Encoding,Processing effort,Randomisation},
  author = {Neuhaus, Christiane and Knösche, Thomas R.},
  file = {D\:\\Sauve\\Zotero\\storage\\6CAEZTRS\\Neuhaus and Knösche - 2008 - Processing of pitch and time sequences in music.pdf;D\:\\Sauve\\Zotero\\storage\\7EEZXJ53\\S0304394008007829.html}
}

@article{peretzBoundariesSeparabilityMelody1993,
  title = {Boundaries of Separability between Melody and Rhythm in Music Discrimination: {{A}} Neuropsychological Perspective},
  volume = {46},
  issn = {0272-4987},
  url = {http://dx.doi.org/10.1080/14640749308401048},
  doi = {10.1080/14640749308401048},
  shorttitle = {Boundaries of Separability between Melody and Rhythm in Music Discrimination},
  abstract = {The detailed study of a patient who suffered from a severe amelodia without arhythmia as a consequence of bilateral temporal lobe damage revealed that the processing of melodic information is at least partially separable from the processing of rhythmic information. This dissociation was replicated across different sets of material, was supported by the presence of a reversed association, and was maintained in conditions that promote integration in the normal brain. These results argue against the view that melody and rhythm are treated as a unified dimension throughout processing. At the same time, they support the view that integration takes place after early separation of the two dimensions.},
  number = {2},
  journaltitle = {The Quarterly Journal of Experimental Psychology Section A},
  urldate = {2016-07-15},
  date = {1993-05-01},
  pages = {301-325},
  author = {Peretz, Isabelle and Kolinsky, Régine},
  file = {D\:\\Sauve\\Zotero\\storage\\KXUMXJHB\\14640749308401048.html}
}

@article{schmucklerExpectationMusicInvestigation1989,
  langid = {english},
  title = {Expectation in {{Music}}: {{Investigation}} of {{Melodic}} and {{Harmonic Processes}}},
  volume = {7},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/7/2/19},
  doi = {10.2307/40285454},
  shorttitle = {Expectation in {{Music}}},
  abstract = {Expectancy has long been of interest to psychologists and recently has become the focus of research in musical cognition. Four experiments are reported that investigated the formation of expectancies in musically trained listeners and performers. Experiments 1 and 2 examined the factors underlying the formation of melodic and harmonic expectancies, respectively. Both experiments found evidence for the psychological reality of constructs derived from the music-theoretic literature in expectancy formation. Experiment 3 investigated the generation of expectancies for a full musical context (one containing simultaneous melodic and harmonic information) and found that melody and harmony were perceptually independent, such that they combined additively in expectancy formation for a full musical context. Experiment 4 provided a convergent operation for the earlier studies by having skilled pianists perform their expectations for the same passages. These productions strongly correlated with the perceptual expectancies of Experiments 1-3. Taken together, these studies provide evidence for the existence of musical expectancy, as well as delineating some of the factors affecting its formation.},
  number = {2},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-07-15},
  date = {1989-12-01},
  pages = {109-149},
  author = {Schmuckler, Mark A.},
  file = {D\:\\Sauve\\Zotero\\storage\\P888S4Q6\\19.html}
}

@article{krumhanslRhythmPitchMusic2000,
  title = {Rhythm and Pitch in Music Cognition},
  volume = {126},
  issn = {1939-1455(Electronic);0033-2909(Print)},
  doi = {10.1037/0033-2909.126.1.159},
  abstract = {Rhythm and pitch are the 2 primary dimensions of music. They are interesting psychologically because simple, well-defined units combine to form highly complex and varied patterns. This article brings together the major developments in research on how these dimensions are perceived and remembered, beginning with psychophysical results on time and pitch perception. Progressively larger units are considered, moving from basic psychological categories of temporal and frequency ratios, to pulse and scale, to metrical and tonal hierarchies, to the formation of musical rhythms and melodies, and finally to the cognitive representation of large-scale musical form. Interactions between the dimensions are considered, and major theoretical proposals are described. The article identifies various links between musical structure and perceptual and cognitive processes, suggesting psychological influences on how sounds are patterned in music.},
  number = {1},
  journaltitle = {Psychological Bulletin},
  date = {2000},
  pages = {159-179},
  keywords = {Rhythm,*Music Perception,*Cognition,*Pitch Perception},
  author = {Krumhansl, Carol L.}
}

@article{schubertModelingPerceivedEmotion2004,
  langid = {english},
  title = {Modeling {{Perceived Emotion With Continuous Musical Features}}},
  volume = {21},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/21/4/561},
  doi = {10.1525/mp.2004.21.4.561},
  abstract = {The relationship between musical features and perceived emotion was investigated by using continuous response methodology and time-series analysis. Sixty-seven participants responded to four pieces of Romantic music expressing different emotions. Responses were sampled once per second on a two-dimensional emotion space (happy-sad valence and aroused-sleepy). Musical feature variables of loudness, tempo, melodic contour, texture, and spectral centroid (related to perceived timbral sharpness) were coded. Musical feature variables were differenced and used as predictors in two univariate linear regression models of valence and arousal for each of the four pieces. Further adjustments were made to the models to correct for serial correlation. The models explained from 33\% to 73\% of variation in univariate perceived emotion. Changes in loudness and tempo were associated positively with changes in arousal, but loudness was dominant. Melodic contour varied positively with valence, though this finding was not conclusive. Texture and spectral centroid did not produce consistent predictions. This methodology facilitates a more ecologically valid investigation of emotion in music and, importantly in the present study, enabled the approximate identification of the lag between musical features and perceived emotion. Responses were made 1 to 3 s after a change in the causal musical event, with sudden changes in loudness producing response lags from zero (nearly instantaneous) to 1 s. Other findings, interactions, and ramifications of the methodology are also discussed.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-07-15},
  date = {2004-06-01},
  pages = {561-585},
  author = {Schubert, Emery},
  file = {D\:\\Sauve\\Zotero\\storage\\A33S423D\\561.html}
}

@article{steinbeisRoleHarmonicExpectancy2006,
  langid = {english},
  title = {The Role of Harmonic Expectancy Violations in Musical Emotions: Evidence from Subjective, Physiological, and Neural Responses},
  volume = {18},
  issn = {0898-929X},
  doi = {10.1162/jocn.2006.18.8.1380},
  shorttitle = {The Role of Harmonic Expectancy Violations in Musical Emotions},
  abstract = {The purpose of the present study was to investigate the effect of harmonic expectancy violations on emotions. Subjective response measures for tension and emotionality, as well as electrodermal activity (EDA) and heart rate (HR), were recorded from 24 subjects (12 musicians and 12 nonmusicians) to observe the effect of expectancy violations on subjective and physiological measures of emotions. In addition, an electro-encephalogram was recorded to observe the neural correlates for detecting these violations. Stimuli consisted of three matched versions of six Bach chorales, which differed only in terms of one chord (harmonically either expected, unexpected or very unexpected). Musicians' and nonmusicians' responses were also compared. Tension, overall subjective emotionality, and EDA increased with an increase in harmonic unexpectedness. Analysis of the event-related potentials revealed an early negativity (EN) for both the unexpected and the very unexpected harmonies, taken to reflect the detection of the unexpected event. The EN in response to very unexpected chords was significantly larger in amplitude than the EN in response to merely unexpected harmonic events. The ENs did not differ in amplitude between the two groups but peaked earlier for musicians than for nonmusicians. Both groups also showed a P3 component in response to the very unexpected harmonies, which was considerably larger for musicians and may reflect the processing of stylistic violations of Western classical music.},
  number = {8},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {J Cogn Neurosci},
  date = {2006-08},
  pages = {1380-1393},
  keywords = {Evoked Potentials,Auditory Perception,Music,electroencephalography,humans,Adult,Female,Male,Analysis of Variance,emotions,REACTION time,Functional Laterality,Galvanic Skin Response,Signal Detection; Psychological,Heart Rate},
  author = {Steinbeis, Nikolaus and Koelsch, Stefan and Sloboda, John A.},
  eprinttype = {pmid},
  eprint = {16859422}
}

@article{krumhanslMusicalExpectancyInfluence2008,
  title = {Musical Expectancy: {{The}} Influence of Musical Structure on Emotional Response},
  volume = {31},
  issn = {1469-1825},
  url = {http://journals.cambridge.org/article_S0140525X08005384},
  doi = {10.1017/S0140525X08005384},
  shorttitle = {Musical Expectancy},
  abstract = {When examining how emotions are evoked through music, the role of musical expectancy is often surprisingly under-credited. This mechanism, however, is most strongly tied to the actual structure of the music, and thus is important when considering how music elicits emotions. We briefly summarize Leonard Meyer's theoretical framework on musical expectancy and emotion and cite relevant research in the area.},
  number = {05},
  journaltitle = {Behavioral and Brain Sciences},
  urldate = {2016-07-15},
  date = {2008-10},
  pages = {584--585},
  author = {Krumhansl, Carol L. and Agres, Kat R.},
  file = {D\:\\Sauve\\Zotero\\storage\\VRMCA94A\\displayAbstract.html}
}

@article{rentfrowReMiEveryday2003,
  title = {The Do Re Mi's of Everyday Life: {{The}} Structure and Personality Correlates of Music Preferences},
  volume = {84},
  issn = {1939-1315(Electronic);0022-3514(Print)},
  doi = {10.1037/0022-3514.84.6.1236},
  shorttitle = {The Do Re Mi's of Everyday Life},
  abstract = {The present research examined individual differences in music preferences. A series of 6 studies investigated lay beliefs about music, the structure underlying music preferences, and the links between music preferences and personality. The data indicated that people consider music an important aspect of their lives and listening to music an activity they engaged in frequently. Using multiple samples, methods, and geographic regions, analyses of the music preferences of over 3,500 individuals converged to reveal 4 music-preference dimensions: Reflective and Complex, Intense and Rebellious, Upbeat and Conventional, and Energetic and Rhythmic. Preferences for these music dimensions were related to a wide array of personality dimensions (e.g., Openness), self-views (e.g., political orientation), and cognitive abilities (e.g., verbal IQ).},
  number = {6},
  journaltitle = {Journal of Personality and Social Psychology},
  date = {2003},
  pages = {1236-1256},
  keywords = {*Music,*Individual Differences,*Personality Correlates,Preferences},
  author = {Rentfrow, Peter J. and Gosling, Samuel D.},
  file = {D\:\\Sauve\\Zotero\\storage\\D7GF5BR2\\index.html}
}

@article{steinhauerBrainPotentialsIndicate1999,
  langid = {english},
  title = {Brain Potentials Indicate Immediate Use of Prosodic Cues in Natural Speech Processing},
  volume = {2},
  issn = {1097-6256},
  url = {http://www.nature.com/neuro/journal/v2/n2/full/nn0299_191.html},
  doi = {10.1038/5757},
  abstract = {Spoken language, in contrast to written text, provides prosodic information such as rhythm, pauses, accents, amplitude and pitch variations. However, little is known about when and how these features are used by the listener to interpret the speech signal. Here we use event–related brain potentials (ERP) to demonstrate that intonational phrasing guides the initial analysis of sentence structure. Our finding of a positive shift in the ERP at intonational phrase boundaries suggests a specific on–line brain response to prosodic processing. Additional ERP components indicate that a false prosodic boundary is sufficient to mislead the listener's sentence processor. Thus, the application of ERP measures is a promising approach for revealing the time course and neural basis of prosodic information processing.},
  number = {2},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2016-07-15},
  date = {1999-02},
  pages = {191-196},
  author = {Steinhauer, Karsten and Alter, Kai and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\N32FS7KB\\nn0299_191.html}
}

@book{temperleyCognitionBasicMusical2001,
  langid = {english},
  location = {{Cambridge, MA}},
  title = {The {{Cognition}} of {{Basic Musical Structures}}},
  isbn = {978-0-262-70105-1},
  abstract = {Winner of the 2003 Emerging Scholar Award, presented by the Society for Music Theory In this book, David Temperley addresses a fundamental question about music cognition: how do we extract basic kinds of musical information, such as meter, phrase structure, counterpoint, pitch spelling, harmony, and key from music as we hear it? Taking a computational approach, Temperley develops models for generating these aspects of musical structure. The models he proposes are based on preference rules, which are criteria for evaluating a possible structural analysis of a piece of music. A preference rule system evaluates many possible interpretations and chooses the one that best satisfies the rules.After an introductory chapter, Temperley presents preference rule systems for generating six basic kinds of musical structure: meter, phrase structure, contrapuntal structure, harmony, and key, as well as pitch spelling (the labeling of pitch events with spellings such as A flat or G sharp). He suggests that preference rule systems not only show how musical structures are inferred, but also shed light on other aspects of music. He substantiates this claim with discussions of musical ambiguity, retrospective revision, expectation, and music outside the Western canon (rock and traditional African music). He proposes a framework for the description of musical styles based on preference rule systems and explores the relevance of preference rule systems to higher-level aspects of music, such as musical schemata, narrative and drama, and musical tension.},
  pagetotal = {430},
  publisher = {{MIT Press}},
  date = {2001},
  keywords = {Science / Acoustics & Sound,Technology & Engineering / Acoustics & Sound},
  author = {Temperley, David}
}

@article{egermannProbabilisticModelsExpectation2013,
  langid = {english},
  title = {Probabilistic Models of Expectation Violation Predict Psychophysiological Emotional Responses to Live Concert Music},
  volume = {13},
  issn = {1530-7026, 1531-135X},
  url = {http://link.springer.com/article/10.3758/s13415-013-0161-y},
  doi = {10.3758/s13415-013-0161-y},
  abstract = {We present the results of a study testing the often-theorized role of musical expectations in inducing listeners’ emotions in a live flute concert experiment with 50 participants. Using an audience response system developed for this purpose, we measured subjective experience and peripheral psychophysiological changes continuously. To confirm the existence of the link between expectation and emotion, we used a threefold approach. (1) On the basis of an information-theoretic cognitive model, melodic pitch expectations were predicted by analyzing the musical stimuli used (six pieces of solo flute music). (2) A continuous rating scale was used by half of the audience to measure their experience of unexpectedness toward the music heard. (3) Emotional reactions were measured using a multicomponent approach: subjective feeling (valence and arousal rated continuously by the other half of the audience members), expressive behavior (facial EMG), and peripheral arousal (the latter two being measured in all 50 participants). Results confirmed the predicted relationship between high-information-content musical events, the violation of musical expectations (in corresponding ratings), and emotional reactions (psychologically and physiologically). Musical structures leading to expectation reactions were manifested in emotional reactions at different emotion component levels (increases in subjective arousal and autonomic nervous system activations). These results emphasize the role of musical structure in emotion induction, leading to a further understanding of the frequently experienced emotional effects of music.},
  number = {3},
  journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
  shortjournal = {Cogn Affect Behav Neurosci},
  urldate = {2016-07-14},
  date = {2013-04-20},
  pages = {533-553},
  author = {Egermann, Hauke and Pearce, Marcus T. and Wiggins, Geraint A. and McAdams, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\EF6JPZZW\\Egermann et al. - 2013 - Probabilistic models of expectation violation pred.pdf;D\:\\Sauve\\Zotero\\storage\\72KC8RMC\\s13415-013-0161-y.html}
}

@thesis{pearceConstructionEvaluationStatistical2005,
  langid = {english},
  location = {{London, UK}},
  title = {The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition},
  url = {http://openaccess.city.ac.uk/8459/},
  abstract = {The prevalent approach to developing cognitive models of music perception and composition is to construct systems of symbolic rules and constraints on the basis of extensive music-theoretic and music-analytic knowledge. The thesis proposed in this dissertation is that statistical models which acquire knowledge through the induction of regularities in corpora of existing music can, if examined with appropriate methodologies, provide significant insights into the cognitive processing involved in music perception and composition. This claim is examined in three stages. First, a number of statistical modelling techniques drawn from the fields of data compression, statistical language modelling and machine learning are subjected to empirical evaluation in the context of sequential prediction of pitch structure in unseen melodies. This investigation results in a collection of modelling strategies which together yield significant performance improvements over existing methods. In the second stage, these statistical systems are used to examine observed patterns of expectation collected in previous psychological research on melody perception. In contrast to previous accounts of this data, the results demonstrate that these patterns of expectation can be accounted for in terms of the induction of statistical regularities acquired through exposure to music. In the final stage of the present research, the statistical systems developed in the first stage are used to examine the intrinsic computational demands of the task of composing a stylistically successful melody The results suggest that the systems lack the degree of expressive power needed to consistently meet the demands of the task. In contrast to previous research, however, the methodological framework developed for the evaluation of computational models of composition enables a detailed empirical examination and comparison of such models which facilitates the identification and resolution of their weaknesses.},
  institution = {{City University London}},
  type = {doctoral},
  urldate = {2016-06-27},
  date = {2005-12},
  author = {Pearce, M. T.},
  file = {D\:\\Sauve\\Zotero\\storage\\IFC2TG5P\\Pearce - 2005 - The construction and evaluation of statistical mod.pdf;D\:\\Sauve\\Zotero\\storage\\E5BHVD65\\8459.html}
}

@book{manningFoundationsStatisticalNatural1999,
  langid = {english},
  title = {Foundations of {{Statistical Natural Language Processing}}},
  isbn = {978-0-262-13360-9},
  abstract = {Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.},
  pagetotal = {722},
  publisher = {{MIT Press}},
  date = {1999},
  keywords = {Language Arts & Disciplines / Linguistics / General},
  author = {Manning, Christopher D. and Schütze, Hinrich}
}

@inproceedings{clearyUnboundedLengthContexts1995,
  title = {Unbounded Length Contexts for {{PPM}}},
  doi = {10.1109/DCC.1995.515495},
  abstract = {The prediction by partial matching (PPM) data compression scheme has set the performance standard in lossless compression of text throughout the past decade. The original algorithm was first published in 1984 by Cleary and Witten, and a series of improvements was described by Moffat (1990), culminating in a careful implementation, called PPMC, which has become the benchmark version. This still achieves results superior to virtually all other compression methods, despite many attempts to better it. PPM, is a finite-context statistical modeling technique that can be viewed as blending together several fixed-order context models to predict the next character in the input sequence. Prediction probabilities for each context in the model are calculated from frequency counts which are updated adaptively; and the symbol that actually occurs is encoded relative to its predicted distribution using arithmetic coding. The paper describes a new algorithm, PPM*, which exploits contexts of unbounded length. It reliably achieves compression superior to PPMC, although our current implementation uses considerably greater computational resources (both time and space). The basic PPM compression scheme is described, showing the use of contexts of unbounded length, and how it can be implemented using a tree data structure. Some results are given that demonstrate an improvement of about 6\% over the old method},
  eventtitle = {Data {{Compression Conference}}, 1995. {{DCC}} '95. {{Proceedings}}},
  booktitle = {Data {{Compression Conference}}, 1995. {{DCC}} '95. {{Proceedings}}},
  date = {1995-03},
  pages = {52-61},
  keywords = {statistical analysis,Probability,Context modeling,Predictive models,Arithmetic,arithmetic codes,arithmetic coding,Benchmark testing,Computer science,data compression,Data structures,estimation theory,finite-context statistical modeling,fixed-order context models,Frequency,frequency counts,input sequence,lossless text compression,Performance loss,performance standard,PPM*,PPMC,predicted distribution,prediction by partial matching,prediction probabilities,prediction theory,tree data structure,tree data structures,unbounded length contexts,word processing},
  author = {Cleary, J. G. and Teahan, W. J. and Witten, I. H.},
  file = {D\:\\Sauve\\Zotero\\storage\\WNPBUXEC\\login.html}
}

@article{pearceImprovedMethodsStatistical2004,
  title = {Improved {{Methods}} for {{Statistical Modelling}} of {{Monophonic Music}}},
  volume = {33},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/0929821052000343840},
  doi = {10.1080/0929821052000343840},
  abstract = {N-Gram based models have been used for a variety of musical tasks including computer-assisted composition, machine improvisation, music information retrieval, stylistic analysis and cognitive modelling. We present an application-independent evaluation of some recent techniques for improving the performance of a subclass of n-gram models on a range of monophonic music data. We have applied these techniques incrementally to eight melodic datasets using cross entropy computed by 10-fold cross-validation on each dataset as our performance metric. The results demonstrate that significant and consistent improvements in performance are afforded by several of the evaluated techniques. We discuss the results in terms of previous research carried out in the field of data compression and with natural language and music corpora and conclude by presenting some important directions for future research.},
  number = {4},
  journaltitle = {Journal of New Music Research},
  urldate = {2016-06-26},
  date = {2004-12-01},
  pages = {367-385},
  author = {Pearce, Marcus and Wiggins, Geraint},
  file = {D\:\\Sauve\\Zotero\\storage\\CM56GM57\\0929821052000343840.html}
}

@article{conklinMultipleViewpointSystems1995,
  title = {Multiple Viewpoint Systems for Music Prediction},
  volume = {24},
  issn = {0929-8215},
  url = {http://dx.doi.org/10.1080/09298219508570672},
  doi = {10.1080/09298219508570672},
  abstract = {This paper examines the prediction and generation of music using a multiple viewpoint system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena. Both the general style and a particular piece are modeled using dual short‐term and long‐term theories, and the model is created using machine learning techniques on a corpus of musical examples. The models are used for analysis and prediction, and we conjecture that highly predictive theories will also generate original, acceptable, works. Although the quality of the works generated is hard to quantify objectively, the predictive power of models can be measured by the notion of entropy, or unpredictability. Highly predictive theories will produce low‐entropy estimates of a musical language. The methods developed are applied to the Bach chorale melodies. Multiple‐viewpoint systems are learned from a sample of 95 chorales, estimates of entropy are produced, and a predictive theory is used to generate new, unseen pieces.},
  number = {1},
  journaltitle = {Journal of New Music Research},
  urldate = {2016-06-26},
  date = {1995-03-01},
  pages = {51-73},
  author = {Conklin, Darrell and Witten, Ian H.},
  file = {D\:\\Sauve\\Zotero\\storage\\I6TNIMRQ\\09298219508570672.html}
}

@book{narmourAnalysisCognitionBasic1990,
  location = {{Chicago}},
  title = {The Analysis and Cognition of Basic Melodic Structures: The Implication-Realization Model.},
  publisher = {{University of Chicago Press}},
  date = {1990},
  author = {Narmour, Eugene}
}

@book{narmourAnalysisCognitionMelodic1992,
  langid = {english},
  location = {{Chicago}},
  title = {The {{Analysis}} and {{Cognition}} of {{Melodic Complexity}}: {{The Implication}}-Realization {{Model}}},
  isbn = {978-0-226-56842-3},
  shorttitle = {The {{Analysis}} and {{Cognition}} of {{Melodic Complexity}}},
  abstract = {In this work, Eugene Narmour extends the unique theories of musical perception presented in The Analysis and Cognition of Basic Melodic Structures. The two books together constitute the first comprehensive theory of melody founded on psychological research. Narmour's earlier study dealt with cognitive relations between melodic tones at their most basic level. After summarizing the formalized methodology of the theory described in that work, Narmour develops an elaborate and original symbology to show how sixteen archetypes can combine to form some 200 complex structures that, in turn, can chain together in a theoretically infinite number of ways. He then explains and speculates on the cognitive operations by which listeners assimilate and ultimately encode these complex melodic structures. More than 250 musical examples from different historical periods and non-Western cultures demonstrate the panstylistic scope of Narmour's model. Of particular importance to music theorists and music historians is Narmour's argument that melodic analysis and formal analysis, though often treated separately, are in fact indissolubly linked. The Analysis and Cognition of Melodic Complexity will also appeal to ethnomusicologists, psychologists, and cognitive scientists.},
  pagetotal = {460},
  publisher = {{University of Chicago Press}},
  date = {1992-01},
  keywords = {Technology & Engineering / Acoustics & Sound,Music / Instruction & Study / General},
  author = {Narmour, Eugene}
}

@article{narmourTopdownBottomupSystems1991,
  langid = {english},
  title = {The {{Top}}-down and {{Bottom}}-up {{Systems}} of {{Musical Implication}}: {{Building}} on {{Meyer}}'s {{Theory}} of {{Emotional Syntax}}},
  volume = {9},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/9/1/1},
  doi = {10.2307/40286156},
  shorttitle = {The {{Top}}-down and {{Bottom}}-up {{Systems}} of {{Musical Implication}}},
  abstract = {The implication-realization model hypothesizes that emotional syntax in music is a product of two expectation systems—one top down, the other bottom up. Syntactic mismatch or conflict in realizations can occur either within each system or between them. The theory argues that interruption or suppression of parametric expectations generated separately by the two systems explains certain types of recurrent aesthetic strategies in melodic composition and accounts for the most common kinds of musical forms (AAA, AAB, ABB, ABC, and ABA).},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-06-22},
  date = {1991-10-01},
  pages = {1-26},
  author = {Narmour, Eugene},
  file = {D\:\\Sauve\\Zotero\\storage\\46CPZQHE\\1.html}
}

@article{schellenbergExpectancyMelodyTests1996,
  title = {Expectancy in Melody: Tests of the Implication-Realization Model},
  volume = {58},
  issn = {0010-0277},
  url = {http://www.sciencedirect.com/science/article/pii/0010027795006656},
  doi = {10.1016/0010-0277(95)00665-6},
  shorttitle = {Expectancy in Melody},
  abstract = {The implication-realization model's description of tone-to-tone expectancies for continuations of melodies was examined. The model's predictions for expectancies are described with a small number of principles specified precisely in terms of interval size and direction of pitch. These principles were quantified and used to predict the data from three experiments in which listeners were required to judge how well individual test tones continued melodic fragments. The model successfully predicted listeners' judgments across different musical styles (British and Chinese folk songs and Webern Lieder), regardless of the extent of listeners' musical training (Experiments 1 and 2) or whether they were born and raised in China or the U.S.A. (Experiment 3). For each experiment, however, the collinearity of the model's predictors indicated that a simplified version of the model might predict the data equally well. Indeed, a revised and simplified model did not result in a loss of predictive power for any of the three experiments. Convergent evidence was provided in a reanalysis of data reported by Carlsen (1981) and Unyk and Carlsen (1987), whose listeners were required to sing continuations to two-tone stimuli. Thus, these findings indicate that the implication-realization model is over-specified. The consistency that was found across experimental tasks, musical styles, and listeners raises the possibility, however, that the revised version of the model may withstand the original model's claims of universality.},
  number = {1},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  urldate = {2016-06-22},
  date = {1996-01},
  pages = {75-125},
  author = {Schellenberg, E. Glenn},
  file = {D\:\\Sauve\\Zotero\\storage\\2H3MN3VF\\Schellenberg - 1996 - Expectancy in melody tests of the implication-rea.pdf;D\:\\Sauve\\Zotero\\storage\\2RDVTGDE\\0010027795006656.html}
}

@article{russellCoreAffectPsychological2003,
  langid = {english},
  title = {Core Affect and the Psychological Construction of Emotion},
  volume = {110},
  issn = {0033-295X},
  abstract = {At the heart of emotion, mood, and any other emotionally charged event are states experienced as simply feeling good or bad, energized or enervated. These states--called core affect--influence reflexes, perception, cognition, and behavior and are influenced by many causes internal and external, but people have no direct access to these causal connections. Core affect can therefore be experienced as free-floating (mood) or can be attributed to some cause (and thereby begin an emotional episode). These basic processes spawn a broad framework that includes perception of the core-affect-altering properties of stimuli, motives, empathy, emotional meta-experience, and affect versus emotion regulation; it accounts for prototypical emotional episodes, such as fear and anger, as core affect attributed to something plus various nonemotional processes.},
  number = {1},
  journaltitle = {Psychological Review},
  shortjournal = {Psychol Rev},
  date = {2003-01},
  pages = {145-172},
  keywords = {cognition,humans,Psychology,affect,Behavior,Reflex},
  author = {Russell, James A.},
  eprinttype = {pmid},
  eprint = {12529060}
}

@article{deanSharedDistinctMechanisms2014,
  title = {Shared and Distinct Mechanisms of Individual and Expertise-Group Perception of Expressed Arousal in Four Works},
  volume = {8},
  issn = {1745-9737},
  url = {http://dx.doi.org/10.1080/17459737.2014.928753},
  doi = {10.1080/17459737.2014.928753},
  abstract = {We compare the continuously perceived expressed arousal of four three-minute musical extracts (three electroacoustic, one acoustic – for piano), for inter-individual and inter-expertise-group differences. Participants are electroacoustic musicians, generalist musicians, and non-musicians. Time series analysis methods are used, including cross-sectional time series analysis. The prevalence of stasis in individual responses is described. Then we determine whether group average responses differ between groups. Finally we define the inter-individual variation in responses, and how this impacts the detection of group differences. We find that inter-individual variations in each expertise group are large, and can overwhelm inter-group differences. There are substantial variations between the pieces, related to the extent of acoustic intensity changes, and distinguishing the instrumental and electroacoustic works. Nevertheless, a single optimised “parent” model of the influence of acoustic profiles on perceived arousal has predictive power for all four pieces. This includes acoustic intensity, spectral flatness, and autoregression. The pieces share mechanisms, and show differences, in how they evoke perceptions of expressed arousal.},
  number = {3},
  journaltitle = {Journal of Mathematics and Music},
  urldate = {2016-06-22},
  date = {2014-09-02},
  pages = {207-223},
  author = {Dean, Roger T. and Bailes, Freya and Dunsmuir, William T. M.},
  file = {D\:\\Sauve\\Zotero\\storage\\KSW2KSU9\\17459737.2014.html}
}

@book{meyerEmotionMeaningMusic1956,
  title = {Emotion and {{Meaning}} in {{Music}}},
  url = {http://www.press.uchicago.edu/ucp/books/book/chicago/E/bo3643659.html},
  abstract = {"Altogether it is a book that should be required reading for any student of music, be he composer, performer, or theorist. It clears the air of many confused notions . . . and lays the groundwork for exhaustive study of the basic problem of music theory and aesthetics, the relationship between pattern and meaning."—David Kraehenbuehl, Journal of Music Theory ~"This is the best study of its kind to have come to the attention of this reviewer."—Jules Wolffers, The Christian Science Monitor "It is not too much to say that his approach provides a basis for the meaningful discussion of emotion and meaning in all art."—David P. McAllester, American Anthropologist "A book which should be read by all who want deeper insights into music listening, performing, and composing."—Marcus G. Raskin, Chicago Review},
  publisher = {{Unviersity of Chicago Press}},
  urldate = {2016-06-22},
  date = {1956},
  author = {Meyer, Leonard},
  file = {D\:\\Sauve\\Zotero\\storage\\FPCQR6TR\\bo3643659.html}
}

@article{juslinEmotionalResponsesMusic2008,
  langid = {english},
  title = {Emotional Responses to Music: The Need to Consider Underlying Mechanisms},
  volume = {31},
  issn = {1469-1825},
  doi = {10.1017/S0140525X08005293},
  shorttitle = {Emotional Responses to Music},
  abstract = {Research indicates that people value music primarily because of the emotions it evokes. Yet, the notion of musical emotions remains controversial, and researchers have so far been unable to offer a satisfactory account of such emotions. We argue that the study of musical emotions has suffered from a neglect of underlying mechanisms. Specifically, researchers have studied musical emotions without regard to how they were evoked, or have assumed that the emotions must be based on the "default" mechanism for emotion induction, a cognitive appraisal. Here, we present a novel theoretical framework featuring six additional mechanisms through which music listening may induce emotions: (1) brain stem reflexes, (2) evaluative conditioning, (3) emotional contagion, (4) visual imagery, (5) episodic memory, and (6) musical expectancy. We propose that these mechanisms differ regarding such characteristics as their information focus, ontogenetic development, key brain regions, cultural impact, induction speed, degree of volitional influence, modularity, and dependence on musical structure. By synthesizing theory and findings from different domains, we are able to provide the first set of hypotheses that can help researchers to distinguish among the mechanisms. We show that failure to control for the underlying mechanism may lead to inconsistent or non-interpretable findings. Thus, we argue that the new framework may guide future research and help to resolve previous disagreements in the field. We conclude that music evokes emotions through mechanisms that are not unique to music, and that the study of musical emotions could benefit the emotion field as a whole by providing novel paradigms for emotion induction.},
  number = {5},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  date = {2008-10},
  pages = {559-575; discussion 575-621},
  keywords = {Attention,brain,Auditory Perception,Music,humans,Psychoacoustics,emotions,Imagination,amygdala,Nerve Net,Mental Recall,Brain Stem,Set (Psychology),Arousal,Reflex,Conditioning (Psychology),Frontal Lobe,Hypothalamus,Psychological Theory},
  author = {Juslin, Patrik N. and Västfjäll, Daniel},
  eprinttype = {pmid},
  eprint = {18826699}
}

@article{barrRandomEffectsStructure2013,
  langid = {english},
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  volume = {68},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.11.001},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F 1 and F 2 tests, and in many cases, even worse than F 1 alone. Maximal LMEMs should be the 'gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  number = {3},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {J Mem Lang},
  date = {2013-04},
  pages = {255-278},
  keywords = {Generalization,Statistics,linear mixed-effects models,Monte Carlo simulation},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  eprinttype = {pmid},
  eprint = {24403724},
  pmcid = {PMC3881361}
}

@article{clarkWhateverNextPredictive2013,
  langid = {english},
  title = {Whatever next? {{Predictive}} Brains, Situated Agents, and the Future of Cognitive Science},
  volume = {36},
  issn = {1469-1825},
  doi = {10.1017/S0140525X12000477},
  shorttitle = {Whatever Next?},
  abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this "hierarchical prediction machine" approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
  number = {3},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  date = {2013-06},
  pages = {181-204},
  keywords = {Attention,brain,cognition,humans,Learning,Perception,Models; Neurological,Cognitive Science},
  author = {Clark, Andy},
  eprinttype = {pmid},
  eprint = {23663408}
}

@article{bernikerEstimatingSourcesMotor2008,
  langid = {english},
  title = {Estimating the Sources of Motor Errors for Adaptation and Generalization},
  volume = {11},
  issn = {1546-1726},
  doi = {10.1038/nn.2229},
  abstract = {Motor adaptation is usually defined as the process by which our nervous system produces accurate movements while the properties of our bodies and our environment continuously change. Many experimental and theoretical studies have characterized this process by assuming that the nervous system uses internal models to compensate for motor errors. Here we extend these approaches and construct a probabilistic model that not only compensates for motor errors but estimates the sources of these errors. These estimates dictate how the nervous system should generalize. For example, estimated changes of limb properties will affect movements across the workspace but not movements with the other limb. We provide evidence that many movement-generalization phenomena emerge from a strategy by which the nervous system estimates the sources of our motor errors.},
  number = {12},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat. Neurosci.},
  date = {2008-12},
  pages = {1454-1461},
  keywords = {humans,Models; Biological,Computer Simulation,Feedback,Bayes Theorem,Movement,Psychomotor Performance,Adaptation; Physiological,Biomechanical Phenomena,Generalization (Psychology),Hand,Kinesthesis,Nonlinear Dynamics,Postural Balance,Upper Extremity},
  author = {Berniker, Max and Kording, Konrad},
  eprinttype = {pmid},
  eprint = {19011624},
  pmcid = {PMC2707921}
}

@article{yuAdaptiveBehaviorHumans2007,
  langid = {english},
  title = {Adaptive Behavior: Humans Act as {{Bayesian}} Learners},
  volume = {17},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2007.09.007},
  shorttitle = {Adaptive Behavior},
  abstract = {Subjects adapt to environmental changes differently depending on the perceived frequency of the changes--the environmental volatility--similar to an ideal Bayesian learner. This volatility information correlates with the fMRI BOLD signal in the anterior cingulate cortex.},
  number = {22},
  journaltitle = {Current biology: CB},
  shortjournal = {Curr. Biol.},
  date = {2007-11-20},
  pages = {R977-980},
  keywords = {animals,humans,Learning,Bayes Theorem,Cerebral Cortex,Adaptation; Psychological},
  author = {Yu, Angela J.},
  eprinttype = {pmid},
  eprint = {18029257}
}

@article{kordingCausalInferenceMultisensory2007,
  langid = {english},
  title = {Causal Inference in Multisensory Perception},
  volume = {2},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0000943},
  abstract = {Perceptual events derive their significance to an animal from their meaning about the world, that is from the information they carry about their causes. The brain should thus be able to efficiently infer the causes underlying our sensory events. Here we use multisensory cue combination to study causal inference in perception. We formulate an ideal-observer model that infers whether two sensory cues originate from the same location and that also estimates their location(s). This model accurately predicts the nonlinear integration of cues by human subjects in two auditory-visual localization tasks. The results show that indeed humans can efficiently infer the causal structure as well as the location of causes. By combining insights from the study of causal inference with the ideal-observer approach to sensory cue combination, we show that the capacity to infer causal structure is not limited to conscious, high-level cognition; it is also performed continually and effortlessly in perception.},
  number = {9},
  journaltitle = {PloS One},
  shortjournal = {PLoS ONE},
  date = {2007},
  pages = {e943},
  keywords = {brain,Auditory Perception,Cues,VISUAL perception,humans,Bayes Theorem,Neurons; Afferent,Space Perception,Task Performance and Analysis},
  author = {Körding, Konrad P. and Beierholm, Ulrik and Ma, Wei Ji and Quartz, Steven and Tenenbaum, Joshua B. and Shams, Ladan},
  eprinttype = {pmid},
  eprint = {17895984},
  pmcid = {PMC1978520}
}

@article{mehtaNeuronalDynamicsPredictive2001,
  langid = {english},
  title = {Neuronal {{Dynamics}} of {{Predictive Coding}}},
  volume = {7},
  issn = {1073-8584, 1089-4098},
  url = {http://nro.sagepub.com/content/7/6/490},
  doi = {10.1177/107385840100700605},
  abstract = {A critical task of the central nervous system is to learn causal relationships between stimuli to anticipate events in the future, such as the position of a moving prey or predator. What are the neuronal phenomena underlying anticipation? In this article, I review recent results in hippocampal electrophysiology that shed light on this issue. It is shown that hippocampal spatial receptive fields show large and rapid anticipatory changes in their firing characteristics. These changes are experience- and environment-dependent and can be explained by a computational model based on NMDA-dependent synaptic plasticity during behavior. Striking similarities between the anticipatory network dynamics of widely different neural circuits, such as the hippocampus and primary visual cortex, are discussed. These experimental and theoretical results indicate how the microscopic laws of synaptic plasticity give rise to emergent anticipatory properties of receptive fields and behavior.},
  number = {6},
  journaltitle = {The Neuroscientist},
  shortjournal = {Neuroscientist},
  urldate = {2016-06-14},
  date = {2001-01-12},
  pages = {490-495},
  keywords = {Learning,Direction selectivity,Hippocampus,LTP,Place fields,Tetrodes},
  author = {Mehta, Mayank R.},
  file = {D\:\\Sauve\\Zotero\\storage\\98HHX7G4\\490.html},
  eprinttype = {pmid},
  eprint = {11765126}
}

@article{hohwyPredictiveCodingExplains2008,
  title = {Predictive Coding Explains Binocular Rivalry: {{An}} Epistemological Review},
  volume = {108},
  issn = {0010-0277},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027708001327},
  doi = {10.1016/j.cognition.2008.05.010},
  shorttitle = {Predictive Coding Explains Binocular Rivalry},
  abstract = {Binocular rivalry occurs when the eyes are presented with different stimuli and subjective perception alternates between them. Though recent years have seen a number of models of this phenomenon, the mechanisms behind binocular rivalry are still debated and we still lack a principled understanding of why a cognitive system such as the brain should exhibit this striking kind of behaviour. Furthermore, psychophysical and neurophysiological (single cell and imaging) studies of rivalry are not unequivocal and have proven difficult to reconcile within one framework. This review takes an epistemological approach to rivalry that considers the brain as engaged in probabilistic unconscious perceptual inference about the causes of its sensory input. We describe a simple empirical Bayesian framework, implemented with predictive coding, which seems capable of explaining binocular rivalry and reconciling many findings. The core of the explanation is that selection of one stimulus, and subsequent alternation between stimuli in rivalry occur when: (i) there is no single model or hypothesis about the causes in the environment that enjoys both high likelihood and high prior probability and (ii) when one stimulus dominates, the bottom–up, driving signal for that stimulus is explained away while, crucially, the bottom–up signal for the suppressed stimulus is not, and remains as an unexplained but explainable prediction error signal. This induces instability in perceptual dynamics that can give rise to perceptual transitions or alternations during rivalry.},
  number = {3},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  urldate = {2016-06-14},
  date = {2008-09},
  pages = {687-701},
  keywords = {predictive coding,Neurophysiology,Learning,Binocular rivalry,Consciousness,Empirical Bayes,Free-energy,Perceptual inference,Psychophysics},
  author = {Hohwy, Jakob and Roepstorff, Andreas and Friston, Karl},
  file = {D\:\\Sauve\\Zotero\\storage\\H29UPUBI\\S0010027708001327.html}
}

@article{wacongneNeuronalModelPredictive2012,
  langid = {english},
  title = {A {{Neuronal Model}} of {{Predictive Coding Accounting}} for the {{Mismatch Negativity}}},
  volume = {32},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/32/11/3665},
  doi = {10.1523/JNEUROSCI.5003-11.2012},
  abstract = {The mismatch negativity (MMN) is thought to index the activation of specialized neural networks for active prediction and deviance detection. However, a detailed neuronal model of the neurobiological mechanisms underlying the MMN is still lacking, and its computational foundations remain debated. We propose here a detailed neuronal model of auditory cortex, based on predictive coding, that accounts for the critical features of MMN. The model is entirely composed of spiking excitatory and inhibitory neurons interconnected in a layered cortical architecture with distinct input, predictive, and prediction error units. A spike-timing dependent learning rule, relying upon NMDA receptor synaptic transmission, allows the network to adjust its internal predictions and use a memory of the recent past inputs to anticipate on future stimuli based on transition statistics. We demonstrate that this simple architecture can account for the major empirical properties of the MMN. These include a frequency-dependent response to rare deviants, a response to unexpected repeats in alternating sequences (ABABAA…), a lack of consideration of the global sequence context, a response to sound omission, and a sensitivity of the MMN to NMDA receptor antagonists. Novel predictions are presented, and a new magnetoencephalography experiment in healthy human subjects is presented that validates our key hypothesis: the MMN results from active cortical prediction rather than passive synaptic habituation.},
  number = {11},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-06-14},
  date = {2012-03-14},
  pages = {3665-3678},
  author = {Wacongne, Catherine and Changeux, Jean-Pierre and Dehaene, Stanislas},
  file = {D\:\\Sauve\\Zotero\\storage\\PZJG63ZB\\Wacongne et al. - 2012 - A Neuronal Model of Predictive Coding Accounting f.pdf;D\:\\Sauve\\Zotero\\storage\\FHGNCNZD\\3665.html},
  eprinttype = {pmid},
  eprint = {22423089}
}

@article{vuustPredictiveCodingMusic2009,
  title = {Predictive Coding of Music – {{Brain}} Responses to Rhythmic Incongruity},
  volume = {45},
  issn = {0010-9452},
  url = {http://www.sciencedirect.com/science/article/pii/S0010945208002499},
  doi = {10.1016/j.cortex.2008.05.014},
  abstract = {During the last decades, models of music processing in the brain have mainly discussed the specificity of brain modules involved in processing different musical components. We argue that predictive coding offers an explanatory framework for functional integration in musical processing. Further, we provide empirical evidence for such a network in the analysis of event-related MEG-components to rhythmic incongruence in the context of strong metric anticipation. This is seen in a mismatch negativity (MMNm) and a subsequent P3am component, which have the properties of an error term and a subsequent evaluation in a predictive coding framework. There were both quantitative and qualitative differences in the evoked responses in expert jazz musicians compared with rhythmically unskilled non-musicians. We propose that these differences trace a functional adaptation and/or a genetic pre-disposition in experts which allows for a more precise rhythmic prediction.},
  number = {1},
  journaltitle = {Cortex},
  shortjournal = {Cortex},
  series = {Special {{Issue}} on "{{The Rhythmic Brain}}"},
  urldate = {2016-06-14},
  date = {2009-01},
  pages = {80-92},
  keywords = {predictive coding,Music,MEG,meter,MMN},
  author = {Vuust, Peter and Ostergaard, Leif and Pallesen, Karen Johanne and Bailey, Christopher and Roepstorff, Andreas},
  file = {D\:\\Sauve\\Zotero\\storage\\8I2Z38VS\\S0010945208002499.html}
}

@article{largeResonancePerceptionMusical1994,
  title = {Resonance and the {{Perception}} of {{Musical Meter}}},
  volume = {6},
  issn = {0954-0091},
  url = {http://dx.doi.org/10.1080/09540099408915723},
  doi = {10.1080/09540099408915723},
  abstract = {Many connectionist approaches to musical expectancy and music composition let the question of ‘What next?’ overshadow the equally important question of ‘When next?’. One cannot escape the latter question, one of temporal structure, when considering the perception of musical meter. We view the perception of metrical structure as a dynamic process where the temporal organization of external musical events synchronizes, or entrains, a listener's internal processing mechanisms. This article introduces a novel connectionist unit, based upon a mathematical model of entrainment, capable of phase- and frequency-locking to periodic components of incoming rhythmic patterns. Networks of these units can self-organize temporally structured responses to rhythmic patterns. The resulting network behavior embodies the perception of metrical structure. The article concludes with a discussion of the implications of our approach for theories of metrical structure and musical expectancy.},
  number = {2-3},
  journaltitle = {Connection Science},
  urldate = {2016-06-14},
  date = {1994-01-01},
  pages = {177-208},
  author = {Large, Edward and Kolen, John},
  file = {D\:\\Sauve\\Zotero\\storage\\ZXSQZ88V\\09540099408915723.html}
}

@article{desainComposableTheoryRhythm1992,
  langid = {english},
  title = {A ({{De}}){{Composable Theory}} of {{Rhythm Perception}}},
  volume = {9},
  issn = {0730-7829, 1533-8312},
  url = {http://mp.ucpress.edu/content/9/4/439},
  doi = {10.2307/40285564},
  abstract = {A definition is given of expectancy of events projected into the future by a complex temporal sequence. The definition can be decomposed into basic expectancy components projected by each time interval implicit in the sequence. A preliminary formulation of these basic curves is proposed and the (de) composition method is stated in a formalized, mathematical way. The resulting expectancy of complex temporal patterns can be used to model such diverse topics as categorical rhythm perception, clock and meter inducement, rhythmicity, and the similarity of temporal sequences. Besides expectancy projected into the future, the proposed measure can be projected back into the past as well, generating reinforcement of past events by new data. The consistency of the predictions of the theory with some findings in categorical rhythm perception is shown.},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  urldate = {2016-06-13},
  date = {1992-07-01},
  pages = {439-454},
  author = {Desain, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\JMGS77W3\\439.html}
}

@article{zantoNeuralCorrelatesRhythmic2006,
  title = {Neural Correlates of Rhythmic Expectancy},
  volume = {2},
  issn = {1895-1171},
  url = {http://newacp.vizja.pl/en/download-pdf/volume/2/issue/2/id/20},
  doi = {10.2478/v10053-008-0057-5},
  number = {2},
  journaltitle = {Advances in Cognitive Psychology},
  urldate = {2016-06-13},
  date = {2006-01-01},
  pages = {221-231},
  author = {Zanto, Theodore P. and Snyder, Joel S. and Large, Edward W.}
}

@article{henryLowFrequencyNeuralOscillations2014,
  title = {Low-{{Frequency Neural Oscillations Support Dynamic Attending}} in {{Temporal Context}}},
  volume = {2},
  issn = {2213-4468},
  url = {http://booksandjournals.brillonline.com/content/journals/10.1163/22134468-00002011},
  doi = {10.1163/22134468-00002011},
  abstract = {Behaviorally relevant environmental stimuli are often characterized by some degree of temporal regularity. Dynamic attending theory provides a framework for explaining how perception of stimulus events is affected by the temporal context within which they occur. However, the precise neural implementation of dynamic attending remains unclear. Here, we provide a suggestion for a potential neural implementation of dynamic attending by appealing to low-frequency neural oscillations. The current review will familiarize the reader with the basic theoretical tenets of dynamic attending theory, and review empirical work supporting predictions derived from the theory. The potential neural implementation of dynamic attending theory with respect to low-frequency neural oscillations will be outlined, covering stimulus processing in regular and irregular contexts. Finally, we will provide some more speculative connections between dynamic attending and neural oscillations, and suggest further avenues for future research.},
  number = {1},
  journaltitle = {Timing \&amp; Time Perception},
  urldate = {2016-06-13},
  date = {2014-01-01},
  pages = {62-86},
  keywords = {continuous processing,Dynamic attending theory,nested oscillations,neural oscillations,rhythmic processing},
  author = {Henry, Molly J. and Herrmann, Björn},
  file = {D\:\\Sauve\\Zotero\\storage\\D8WG5EZ5\\Henry and Herrmann - 2014 - Low-Frequency Neural Oscillations Support Dynamic .pdf;D\:\\Sauve\\Zotero\\storage\\AU2TEFU9\\22134468-00002011.html}
}

@article{jonesTimeOurLost1976,
  title = {Time, Our Lost Dimension: {{Toward}} a New Theory of Perception, Attention, and Memory},
  volume = {83},
  issn = {1939-1471(Electronic);0033-295X(Print)},
  doi = {10.1037/0033-295X.83.5.323},
  shorttitle = {Time, Our Lost Dimension},
  abstract = {Presents a theory of perception and attention that emphasizes the relational nature of perceptual invariants and has been developed within the context of auditory pattern research. Part 1 of the theory examines world pattern structure; Part 2 describes interaction of organisms with pattern structure. In Part 1, world patterns are subjectively represented as nested relations within a multidimensional space defined by pitch, loudness, and time. But dependency of these defining dimensions means that a pattern's time scale determines the serial integrity of its pitch/loudness structure. Part 2 proposes a time scale for living things that is manifest in graded perceptual rhythms. These rhythms can be synchronized to corresponding nested time zones within world pattern structure. Related assumptions about the deployment of physical energy across time zones and cognitive locations of perceptual rhythms lead to a simple, but general, attentional theory. Theoretical support, found in research with tone patterns, speech, and sequences of noise, is cited. Beyond this focal research, the theory offers a general framework for understanding diverse phenomena that range from speech perception and aphasia to sleep, growth, and time estimation. (11/2 p ref)},
  number = {5},
  journaltitle = {Psychological Review},
  date = {1976},
  pages = {323-355},
  keywords = {time perception,*Attention,*Auditory Perception,*Memory,*Speech Perception,*Stimulus Variability},
  author = {Jones, Mari R.}
}

@article{jonesAttentionTiming2004,
  title = {Attention and Timing},
  journaltitle = {Ecological psychoacoustics},
  date = {2004},
  pages = {49-85},
  author = {Jones, M. R.}
}

@article{grahnFindingFeelingMusical2013,
  langid = {english},
  title = {Finding and {{Feeling}} the {{Musical Beat}}: {{Striatal Dissociations}} between {{Detection}} and {{Prediction}} of {{Regularity}}},
  volume = {23},
  issn = {1047-3211, 1460-2199},
  url = {http://cercor.oxfordjournals.org/content/23/4/913},
  doi = {10.1093/cercor/bhs083},
  shorttitle = {Finding and {{Feeling}} the {{Musical Beat}}},
  abstract = {Perception of temporal patterns is critical for speech, movement, and music. In the auditory domain, perception of a regular pulse, or beat, within a sequence of temporal intervals is associated with basal ganglia activity. Two alternative accounts of this striatal activity are possible: “searching” for temporal regularity in early stimulus processing stages or “prediction’ of the timing of future tones after the beat is found (relying on continuation of an internally generated beat). To resolve between these accounts, we used functional magnetic resonance imaging (fMRI) to investigate different stages of beat perception. Participants heard a series of beat and nonbeat (irregular) monotone sequences. For each sequence, the preceding sequence provided a temporal beat context for the following sequence. Beat sequences were preceded by nonbeat sequences, requiring the beat to be found anew (“beat finding” condition), or by beat sequences with the same beat rate (“beat continuation”), or a different rate (“beat adjustment”). Detection of regularity is highest during beat finding, whereas generation and prediction are highest during beat continuation. We found the greatest striatal activity for beat continuation, less for beat adjustment, and the least for beat finding. Thus, the basal ganglia's response profile suggests a role in beat prediction, not in beat finding.},
  number = {4},
  journaltitle = {Cerebral Cortex},
  shortjournal = {Cereb. Cortex},
  urldate = {2016-06-13},
  date = {2013-01-04},
  pages = {913-921},
  keywords = {Prediction,Auditory Perception,Music,Rhythm,timing,fMRI,basal ganglia,beat perception},
  author = {Grahn, Jessica A. and Rowe, James B.},
  file = {D\:\\Sauve\\Zotero\\storage\\7K9273XS\\Grahn and Rowe - 2013 - Finding and Feeling the Musical Beat Striatal Dis.pdf;D\:\\Sauve\\Zotero\\storage\\PA3FHNBW\\913.html},
  eprinttype = {pmid},
  eprint = {22499797}
}

@article{rohenkohlCombiningSpatialTemporal2014,
  langid = {english},
  title = {Combining Spatial and Temporal Expectations to Improve Visual Perception},
  volume = {14},
  issn = {1534-7362},
  url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/14.4.8},
  doi = {10.1167/14.4.8},
  number = {4},
  journaltitle = {Journal of Vision},
  urldate = {2016-06-13},
  date = {2014-04-10},
  pages = {8-8},
  author = {Rohenkohl, G. and Gould, I. C. and Pessoa, J. and Nobre, A. C.}
}

@article{fujiokaBetaBandOscillationsRepresent2015,
  langid = {english},
  title = {Beta-{{Band Oscillations Represent Auditory Beat}} and {{Its Metrical Hierarchy}} in {{Perception}} and {{Imagery}}},
  volume = {35},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/35/45/15187},
  doi = {10.1523/JNEUROSCI.2397-15.2015},
  abstract = {Dancing to music involves synchronized movements, which can be at the basic beat level or higher hierarchical metrical levels, as in a march (groups of two basic beats, one–two–one–two …) or waltz (groups of three basic beats, one–two–three–one–two–three …). Our previous human magnetoencephalography studies revealed that the subjective sense of meter influences auditory evoked responses phase locked to the stimulus. Moreover, the timing of metronome clicks was represented in periodic modulation of induced (non-phase locked) β-band (13–30 Hz) oscillation in bilateral auditory and sensorimotor cortices. Here, we further examine whether acoustically accented and subjectively imagined metric processing in march and waltz contexts during listening to isochronous beats were reflected in neuromagnetic β-band activity recorded from young adult musicians. First, we replicated previous findings of beat-related β-power decrease at 200 ms after the beat followed by a predictive increase toward the onset of the next beat. Second, we showed that the β decrease was significantly influenced by the metrical structure, as reflected by differences across beat type for both perception and imagery conditions. Specifically, the β-power decrease associated with imagined downbeats (the count “one”) was larger than that for both the upbeat (preceding the count “one”) in the march, and for the middle beat in the waltz. Moreover, beamformer source analysis for the whole brain volume revealed that the metric contrasts involved auditory and sensorimotor cortices; frontal, parietal, and inferior temporal lobes; and cerebellum. We suggest that the observed β-band activities reflect a translation of timing information to auditory–motor coordination.
SIGNIFICANCE STATEMENT With magnetoencephalography, we examined β-band oscillatory activities around 20 Hz while participants listened to metronome beats and imagined musical meters such as a march and waltz. We demonstrated that β-band event-related desynchronization in the auditory cortex differentiates between beat positions, specifically between downbeats and the following beat. This is the first demonstration of β-band oscillations related to hierarchical and internalized timing information. Moreover, the meter representation in the β oscillations was widespread across the brain, including sensorimotor and premotor cortices, parietal lobe, and cerebellum. The results extend current understanding of the role of β oscillations in neural processing of predictive timing.},
  number = {45},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2016-06-13},
  date = {2015-11-11},
  pages = {15187-15198},
  keywords = {predictive coding,magnetoencephalography,ERD,event-related desynchronization,timing processing},
  author = {Fujioka, Takako and Ross, Bernhard and Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\AVD7QQMN\\Fujioka et al. - 2015 - Beta-Band Oscillations Represent Auditory Beat and.pdf;D\:\\Sauve\\Zotero\\storage\\TW2PMUVJ\\15187.html},
  eprinttype = {pmid},
  eprint = {26558788}
}

@article{jonesDynamicPatternStructure1987,
  langid = {english},
  title = {Dynamic Pattern Structure in Music: {{Recent}} Theory and Research},
  volume = {41},
  issn = {0031-5117, 1532-5962},
  url = {http://link.springer.com/article/10.3758/BF03210494},
  doi = {10.3758/BF03210494},
  shorttitle = {Dynamic Pattern Structure in Music},
  abstract = {Recent theory and research addressed to dynamic pattern structure in music is reported. Dynamic pattern structure refers to the ways in which rhythm and melody combine; it is summarized here in terms of the constructof joint accent structure. Properties of joint accent structures involving accent couplings and time symmetries are used to address standard psychological issues of pattern similarity and pattern simplicity as they are realized in musical tasks. In particular, experimental work on melody recognition reveals the importance of formalizations concerning dynamic pattern similarities, and other research on reproductions of music-like patterns reveals the potential of a time-based approach to dynamic pattern simplicity.},
  number = {6},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  urldate = {2016-06-13},
  date = {1987-11},
  pages = {621-634},
  keywords = {cognitive psychology},
  author = {Jones, Mari Riess},
  file = {D\:\\Sauve\\Zotero\\storage\\VRB29M52\\Jones - 1987 - Dynamic pattern structure in music Recent theory .pdf;D\:\\Sauve\\Zotero\\storage\\HB94METI\\BF03210494.html}
}

@incollection{HumanBrainElectrophysiology,
  title = {Human {{Brain Electrophysiology}}: {{Evoked Potentials}} and {{Evoked Magnetic Fields}} in {{Science}} and {{Medicine}}}
}

@article{nozaradanIndividualDifferencesRhythmic2016,
  title = {Individual Differences in Rhythmic Cortical Entrainment Correlate with Predictive Behavior in Sensorimotor Synchronization},
  volume = {6},
  journaltitle = {Scientific reports},
  date = {2016},
  pages = {20612},
  author = {Nozaradan, Sylvie and Peretz, Isabelle and Keller, Peter E.},
  file = {D\:\\Sauve\\Zotero\\storage\\8F9IT54T\\srep20612.html;D\:\\Sauve\\Zotero\\storage\\DFQAA6ZR\\srep20612.html}
}

@article{salthouseWhatWhenCognitive2004,
  title = {What and When of Cognitive Aging},
  volume = {13},
  number = {4},
  journaltitle = {Current directions in psychological science},
  date = {2004},
  pages = {140--144},
  author = {Salthouse, Timothy A.},
  file = {D\:\\Sauve\\Zotero\\storage\\EZBQ6V67\\Salthouse - 2004 - What and when of cognitive aging.pdf;D\:\\Sauve\\Zotero\\storage\\C5EC45RX\\j.0963-7214.2004.00293.html}
}

@article{rossPhysicalNeuralEntrainment2014,
  langid = {english},
  title = {Physical and Neural Entrainment to Rhythm: Human Sensorimotor Coordination across Tasks and Effector Systems},
  volume = {8},
  issn = {1662-5161},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00576/full},
  doi = {10.3389/fnhum.2014.00576},
  shorttitle = {Physical and Neural Entrainment to Rhythm},
  abstract = {The human sensorimotor system can be readily entrained to environmental rhythms, through multiple sensory modalities. In this review, we provide an overview of theories of timekeeping that make this neuroentrainment possible. First, we present recent evidence that contests the assumptions made in classic timekeeper models. The role of state estimation, sensory feedback and movement parameters on the organization of sensorimotor timing are discussed in the context of recent experiments that examined simultaneous timing and force control. This discussion is extended to the study of coordinated multi-effector movements and how they may be entrained.},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  urldate = {2018-10-02},
  date = {2014},
  keywords = {Rhythm,force control,neuroentrainment,sensorimotor coordination,state estimation,timekeeping},
  author = {Ross, Jessica Marie and Balasubramaniam, Ramesh},
  file = {D\:\\Sauve\\Zotero\\storage\\SQYG4D2F\\Ross and Balasubramaniam - 2014 - Physical and neural entrainment to rhythm human s.pdf}
}

@article{largeResonatingMusicalRhythm2008,
  title = {Resonating to Musical Rhythm: Theory and Experiment},
  shorttitle = {Resonating to Musical Rhythm},
  journaltitle = {The psychology of time},
  date = {2008},
  pages = {189--232},
  author = {Large, Edward W.},
  file = {D\:\\Sauve\\Zotero\\storage\\TL2HF9LD\\Large - 2008 - Resonating to musical rhythm theory and experimen.pdf;D\:\\Sauve\\Zotero\\storage\\RF3786SF\\books.html}
}

@article{grahnRhythmBeatPerception2007,
  title = {Rhythm and Beat Perception in Motor Areas of the Brain},
  volume = {19},
  number = {5},
  journaltitle = {Journal of cognitive neuroscience},
  date = {2007},
  pages = {893--906},
  author = {Grahn, Jessica A. and Brett, Matthew},
  file = {D\:\\Sauve\\Zotero\\storage\\N8NBHJ4V\\Grahn and Brett - 2007 - Rhythm and beat perception in motor areas of the b.pdf;D\:\\Sauve\\Zotero\\storage\\YQ2RCDH9\\jocn.2007.19.5.html}
}

@article{kellerMultilevelCoordinationStability2008,
  title = {Multilevel Coordination Stability: {{Integrated}} Goal Representations in Simultaneous Intra-Personal and Inter-Agent Coordination},
  volume = {128},
  shorttitle = {Multilevel Coordination Stability},
  number = {2},
  journaltitle = {Acta psychologica},
  date = {2008},
  pages = {378--386},
  author = {Keller, Peter E. and Repp, Bruno H.},
  file = {D\:\\Sauve\\Zotero\\storage\\276NQVSV\\S0001691808000498.html;D\:\\Sauve\\Zotero\\storage\\D3WMMY9N\\PMC2570261.html}
}

@article{daikokuConcurrentStatisticalLearning2019,
  title = {Concurrent Statistical Learning of Ignored and Attended Sound Sequences: {{An MEG}} Study},
  volume = {13},
  shorttitle = {Concurrent Statistical Learning of Ignored and Attended Sound Sequences},
  journaltitle = {Frontiers in Human Neuroscience},
  date = {2019},
  author = {Daikoku, Tatsuya and Yumoto, Masato},
  file = {D\:\\Sauve\\Zotero\\storage\\7XECUIAF\\Daikoku and Yumoto - 2019 - Concurrent statistical learning of ignored and att.pdf;D\:\\Sauve\\Zotero\\storage\\Y3LFD9NJ\\ViewItemOverviewPage.html}
}

@article{phillips-silverEcologyEntrainmentFoundations2010,
  title = {The Ecology of Entrainment: {{Foundations}} of Coordinated Rhythmic Movement},
  volume = {28},
  shorttitle = {The Ecology of Entrainment},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2010},
  pages = {3--14},
  author = {Phillips-Silver, Jessica and Aktipis, C. Athena and Bryant, Gregory A.},
  file = {D\:\\Sauve\\Zotero\\storage\\7TXTRUNB\\Phillips-Silver et al. - 2010 - The ecology of entrainment Foundations of coordin.pdf;D\:\\Sauve\\Zotero\\storage\\KFZHQVCW\\3.html}
}

@incollection{balasubramaniamTrajectoryFormationTimed2006,
  title = {Trajectory Formation in Timed Repetitive Movements},
  booktitle = {Motor Control and Learning},
  publisher = {{Springer}},
  date = {2006},
  pages = {47--54},
  author = {Balasubramaniam, Ramesh},
  file = {D\:\\Sauve\\Zotero\\storage\\DXKHA72S\\Balasubramaniam - 2006 - Trajectory formation in timed repetitive movements.pdf;D\:\\Sauve\\Zotero\\storage\\FWFBAGXJ\\0-387-28287-4_4.html}
}

@software{toiviainenMIDIToolbox2016,
  title = {{{MIDI Toolbox}} 1.1},
  url = {https://github.com/miditoolbox/1.1},
  date = {2016},
  author = {Toiviainen, Petri and Eerola, Tuomas}
}

@thesis{sauvePredictionPolyphonyModelling2018,
  title = {Prediction in Polyphony: Modelling Musical Auditory Scene Analysis},
  institution = {{Queen Mary University of London}},
  date = {2018-09-10},
  author = {Sauvé, Sarah}
}

@article{nozaradanSelectiveNeuronalEntrainment2012,
  title = {Selective Neuronal Entrainment to the Beat and Meter Embedded in a Musical Rhythm},
  volume = {32},
  number = {49},
  journaltitle = {Journal of Neuroscience},
  date = {2012},
  pages = {17572--17581},
  author = {Nozaradan, Sylvie and Peretz, Isabelle and Mouraux, André},
  file = {D\:\\Sauve\\Zotero\\storage\\MLLYI8FY\\Nozaradan et al. - 2012 - Selective neuronal entrainment to the beat and met.pdf;D\:\\Sauve\\Zotero\\storage\\MLYRD7U7\\17572.html}
}

@article{nozaradanTaggingNeuronalEntrainment2011,
  title = {Tagging the Neuronal Entrainment to Beat and Meter},
  volume = {31},
  number = {28},
  journaltitle = {Journal of Neuroscience},
  date = {2011},
  pages = {10234--10240},
  author = {Nozaradan, Sylvie and Peretz, Isabelle and Missal, Marcus and Mouraux, André},
  file = {D\:\\Sauve\\Zotero\\storage\\5FJTZE5E\\Nozaradan et al. - 2011 - Tagging the neuronal entrainment to beat and meter.pdf;D\:\\Sauve\\Zotero\\storage\\CMBPRNQC\\10234.html}
}

@article{gillinghamOlderAdultsHearing2018,
  title = {Older {{Adults}} with Hearing Loss Have Reductions in Visual, Motor, and Attentional Functioning},
  volume = {10},
  journaltitle = {Frontiers in Aging Neuroscience},
  date = {2018},
  pages = {351},
  author = {Gillingham, Susan and Vallesi, Antonino and Pichora-Fuller, M. Kathleen and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\QL69V436\\full.html}
}

@article{hodgesHowWhyDoes2015,
  title = {How and Why Does Music Move Us? {{Answers}} from Psychology and Neuroscience},
  volume = {101},
  shorttitle = {How and Why Does Music Move Us?},
  number = {4},
  journaltitle = {Music Educators Journal},
  date = {2015},
  pages = {41--47},
  author = {Hodges, Donald A. and Wilkins, Robin W.},
  file = {D\:\\Sauve\\Zotero\\storage\\AJE57NYB\\Hodges and Wilkins - 2015 - How and why does music move us Answers from psych.pdf;D\:\\Sauve\\Zotero\\storage\\CICN7MPU\\0027432115575755.html}
}

@article{dedreuRehabilitationExerciseTherapy2012,
  title = {Rehabilitation, Exercise Therapy and Music in Patients with {{Parkinson}}'s Disease: A Meta-Analysis of the Effects of Music-Based Movement Therapy on Walking Ability, Balance and Quality of Life},
  volume = {18},
  shorttitle = {Rehabilitation, Exercise Therapy and Music in Patients with {{Parkinson}}'s Disease},
  journaltitle = {Parkinsonism \& related disorders},
  date = {2012},
  pages = {S114--S119},
  author = {de Dreu, Miek J. and Van Der Wilk, A. S. D. and Poppe, E. and Kwakkel, Gert and van Wegen, Erwin EH},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\FLAC7JMF\\de Dreu et al. - 2012 - Rehabilitation, exercise therapy and music in pati.pdf;D\:\\Sauve\\Zotero\\storage\\CFSDEFMY\\abstract.html}
}

@article{kneafseyTherapeuticUseMusic1997,
  title = {The Therapeutic Use of Music in a Care of the Elderly Setting: A Literature Review},
  volume = {6},
  shorttitle = {The Therapeutic Use of Music in a Care of the Elderly Setting},
  number = {5},
  journaltitle = {Journal of clinical nursing},
  date = {1997},
  pages = {341--346},
  author = {Kneafsey, Rosaleen},
  file = {D\:\\Sauve\\Zotero\\storage\\M8BFD63P\\Kneafsey - 1997 - The therapeutic use of music in a care of the elde.pdf;D\:\\Sauve\\Zotero\\storage\\HGNFKZ5H\\j.1365-2702.1997.tb00326.html}
}

@article{bernatzkyStimulatingMusicIncreases2004,
  title = {Stimulating Music Increases Motor Coordination in Patients Afflicted with {{Morbus Parkinson}}},
  volume = {361},
  number = {1-3},
  journaltitle = {Neuroscience letters},
  date = {2004},
  pages = {4--8},
  author = {Bernatzky, Günther and Bernatzky, Patrick and Hesse, Horst-Peter and Staffen, Wolfgang and Ladurner, Gunther},
  file = {D\:\\Sauve\\Zotero\\storage\\86PDG2WM\\Bernatzky et al. - 2004 - Stimulating music increases motor coordination in .pdf;D\:\\Sauve\\Zotero\\storage\\AEYWCBST\\S0304394003014046.html}
}

@article{thompsonMusicEnhancesCategory2005,
  title = {Music Enhances Category Fluency in Healthy Older Adults and {{Alzheimer}}'s Disease Patients},
  volume = {31},
  number = {1},
  journaltitle = {Experimental aging research},
  date = {2005},
  pages = {91--99},
  author = {Thompson, Rebecca G. and Moulin, C. J. A. and Hayre, S. and Jones, R. W.},
  file = {D\:\\Sauve\\Zotero\\storage\\CI5QAKBW\\03610730590882819.html;D\:\\Sauve\\Zotero\\storage\\LRYF7PMC\\03610730590882819.html}
}

@article{sherrattMusicInterventionsPeople2004,
  title = {Music Interventions for People with Dementia: A Review of the Literature},
  volume = {8},
  shorttitle = {Music Interventions for People with Dementia},
  number = {1},
  journaltitle = {Aging \& Mental Health},
  date = {2004},
  pages = {3--12},
  author = {Sherratt, Kirsty and Thornton, Amanda and Hatton, Chris},
  file = {D\:\\Sauve\\Zotero\\storage\\ZMB2ITVR\\Sherratt et al. - 2004 - Music interventions for people with dementia a re.pdf;D\:\\Sauve\\Zotero\\storage\\N5QM74IS\\13607860310001613275.html}
}

@article{irishInvestigatingEnhancingEffect2006,
  title = {Investigating the Enhancing Effect of Music on Autobiographical Memory in Mild {{Alzheimer}}’s Disease},
  volume = {22},
  number = {1},
  journaltitle = {Dementia and geriatric cognitive disorders},
  date = {2006},
  pages = {108--120},
  author = {Irish, Muireann and Cunningham, Conal J. and Walsh, J. Bernard and Coakley, Davis and Lawlor, Brian A. and Robertson, Ian H. and Coen, Robert F.},
  file = {D\:\\Sauve\\Zotero\\storage\\JRXMGIPH\\Irish et al. - 2006 - Investigating the enhancing effect of music on aut.pdf;D\:\\Sauve\\Zotero\\storage\\G82INHNY\\93487.html}
}

@article{andersonTrainingChangesProcessing2013,
  title = {Training Changes Processing of Speech Cues in Older Adults with Hearing Loss},
  volume = {7},
  journaltitle = {Frontiers in systems neuroscience},
  date = {2013},
  pages = {97},
  author = {Anderson, Samira and White-Schwoch, Travis and Choi, Hee Jae and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\7HMXQ9G6\\full.html;D\:\\Sauve\\Zotero\\storage\\FWGEF2PA\\full.html}
}

@article{andersonPartialMaintenanceAuditorybased2014,
  title = {Partial Maintenance of Auditory-Based Cognitive Training Benefits in Older Adults},
  volume = {62},
  journaltitle = {Neuropsychologia},
  date = {2014},
  pages = {286--296},
  author = {Anderson, Samira and White-Schwoch, Travis and Choi, Hee Jae and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\26DHKHRE\\S0028393214002620.html;D\:\\Sauve\\Zotero\\storage\\ALDXCUMC\\S0028393214002620.html}
}

@article{parbery-clarkMusicalExperienceOffsets2012,
  title = {Musical Experience Offsets Age-Related Delays in Neural Timing},
  volume = {33},
  number = {7},
  journaltitle = {Neurobiology of aging},
  date = {2012},
  pages = {1483--e1},
  author = {Parbery-Clark, Alexandra and Anderson, Samira and Hittner, Emily and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\JDZ3QBFM\\Parbery-Clark et al. - 2012 - Musical experience offsets age-related delays in n.pdf;D\:\\Sauve\\Zotero\\storage\\KRMLJTCZ\\S0197458011005471.html}
}

@article{white-schwochOlderAdultsBenefit2013,
  title = {Older Adults Benefit from Music Training Early in Life: Biological Evidence for Long-Term Training-Driven Plasticity},
  volume = {33},
  shorttitle = {Older Adults Benefit from Music Training Early in Life},
  number = {45},
  journaltitle = {Journal of Neuroscience},
  date = {2013},
  pages = {17667--17674},
  author = {White-Schwoch, Travis and Carr, Kali Woodruff and Anderson, Samira and Strait, Dana L. and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\R6739C7G\\White-Schwoch et al. - 2013 - Older adults benefit from music training early in .pdf;D\:\\Sauve\\Zotero\\storage\\TNW7FFFI\\17667.html}
}

@article{bidelmanMusicalTrainingOrchestrates2015,
  title = {Musical Training Orchestrates Coordinated Neuroplasticity in Auditory Brainstem and Cortex to Counteract Age-Related Declines in Categorical Vowel Perception},
  volume = {35},
  number = {3},
  journaltitle = {Journal of Neuroscience},
  date = {2015},
  pages = {1240--1249},
  author = {Bidelman, Gavin M. and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\NWF8X7MF\\Bidelman and Alain - 2015 - Musical training orchestrates coordinated neuropla.pdf;D\:\\Sauve\\Zotero\\storage\\WHYSLKPW\\1240.html}
}

@article{parbery-clarkMusiciansChangeTheir2013,
  title = {Musicians Change Their Tune: How Hearing Loss Alters the Neural Code},
  volume = {302},
  shorttitle = {Musicians Change Their Tune},
  journaltitle = {Hearing research},
  date = {2013},
  pages = {121--131},
  author = {Parbery-Clark, Alexandra and Anderson, Samira and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\CKHWNVLK\\Parbery-Clark et al. - 2013 - Musicians change their tune how hearing loss alte.pdf;D\:\\Sauve\\Zotero\\storage\\85M4UEYV\\S0378595513000774.html}
}

@article{parbery-clarkMusicalExperienceStrengthens2012,
  title = {Musical Experience Strengthens the Neural Representation of Sounds Important for Communication in Middle-Aged Adults},
  volume = {4},
  journaltitle = {Frontiers in Aging Neuroscience},
  date = {2012},
  pages = {30},
  author = {Parbery-Clark, Alexandra and Anderson, Samira and Hittner, Emily and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\5XXETBMF\\full.html;D\:\\Sauve\\Zotero\\storage\\GEMPCTAA\\full.html}
}

@article{amerOlderProfessionalMusicians2013,
  title = {Do Older Professional Musicians Have Cognitive Advantages?},
  volume = {8},
  number = {8},
  journaltitle = {PloS one},
  date = {2013},
  pages = {e71630},
  author = {Amer, Tarek and Kalender, Beste and Hasher, Lynn and Trehub, Sandra E. and Wong, Yukwal},
  file = {D\:\\Sauve\\Zotero\\storage\\4EJIFNPF\\article.html;D\:\\Sauve\\Zotero\\storage\\T9ACL3GC\\article.html}
}

@article{skoeLittleGoesLong2012,
  title = {A Little Goes a Long Way: How the Adult Brain Is Shaped by Musical Training in Childhood},
  volume = {32},
  shorttitle = {A Little Goes a Long Way},
  number = {34},
  journaltitle = {Journal of Neuroscience},
  date = {2012},
  pages = {11507--11510},
  author = {Skoe, Erika and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\WNC7D78Q\\Skoe and Kraus - 2012 - A little goes a long way how the adult brain is s.pdf;D\:\\Sauve\\Zotero\\storage\\NFH4I3GM\\11507.html}
}

@article{herholzMusicalTrainingFramework2012,
  title = {Musical Training as a Framework for Brain Plasticity: Behavior, Function, and Structure},
  volume = {76},
  shorttitle = {Musical Training as a Framework for Brain Plasticity},
  number = {3},
  journaltitle = {Neuron},
  date = {2012},
  pages = {486--502},
  author = {Herholz, Sibylle C. and Zatorre, Robert J.},
  file = {D\:\\Sauve\\Zotero\\storage\\7BS363E8\\S0896627312009312.html;D\:\\Sauve\\Zotero\\storage\\KPNEMH3Q\\S0896627312009312.html}
}

@article{slumingVoxelbasedMorphometryReveals2002,
  title = {Voxel-Based Morphometry Reveals Increased Gray Matter Density in {{Broca}}'s Area in Male Symphony Orchestra Musicians.},
  volume = {17},
  doi = {https://doi.org/10.1006/nimg.2002.1288},
  number = {3},
  journaltitle = {NeuroImage},
  date = {2002},
  pages = {1613-1622},
  author = {Sluming, V. and Barrick, T. and Howard, M. and Cezayirli, E. and Mayes, A. and Roberts, N.}
}

@article{hanna-pladdyRecentMusicalActivity2012,
  title = {Recent and Past Musical Activity Predicts Cognitive Aging Variability: Direct Comparison with General Lifestyle Activities},
  volume = {6},
  shorttitle = {Recent and Past Musical Activity Predicts Cognitive Aging Variability},
  journaltitle = {Frontiers in Human Neuroscience},
  date = {2012},
  pages = {198},
  author = {Hanna-Pladdy, Brenda and Gajewski, Byron},
  file = {D\:\\Sauve\\Zotero\\storage\\SING8UIL\\full.html;D\:\\Sauve\\Zotero\\storage\\TMEM65GY\\full.html}
}

@article{hanna-pladdyRelationInstrumentalMusical2011,
  title = {The Relation between Instrumental Musical Activity and Cognitive Aging.},
  volume = {25},
  number = {3},
  journaltitle = {Neuropsychology},
  date = {2011},
  pages = {378},
  author = {Hanna-Pladdy, Brenda and MacKay, Alicia},
  file = {D\:\\Sauve\\Zotero\\storage\\CLBELXST\\PMC4354683.html;D\:\\Sauve\\Zotero\\storage\\NKVTVXAB\\2011-06927-001.html}
}

@article{daikokuTimecourseVariationStatistics2018,
  title = {Time-Course Variation of Statistics Embedded in Music: {{Corpus}} Study on Implicit Learning and Knowledge},
  volume = {13},
  shorttitle = {Time-Course Variation of Statistics Embedded in Music},
  number = {5},
  journaltitle = {PloS one},
  date = {2018},
  pages = {e0196493},
  author = {Daikoku, Tatsuya},
  file = {D\:\\Sauve\\Zotero\\storage\\7P955LX3\\article.html;D\:\\Sauve\\Zotero\\storage\\Z9I57DR4\\article.html}
}

@article{nourskiProcessingAuditoryNovelty2018,
  title = {Processing of {{Auditory Novelty Across}} the {{Cortical Hierarchy}}: {{An Intracranial Electrophysiology Study}}},
  shorttitle = {Processing of {{Auditory Novelty Across}} the {{Cortical Hierarchy}}},
  journaltitle = {bioRxiv},
  date = {2018},
  pages = {290106},
  author = {Nourski, Kirill V. and Steinschneider, Mitchell and Rhone, Ariane E. and Kawasaki, Hiroto and Howard, Matthew A. and Banks, Matthew I.},
  file = {D\:\\Sauve\\Zotero\\storage\\4HT8X4US\\Nourski et al. - 2018 - Processing of Auditory Novelty Across the Cortical.pdf;D\:\\Sauve\\Zotero\\storage\\7VVAB43F\\290106.html}
}

@article{kondoSeparabilityCommonalityAuditory2011,
  title = {Separability and Commonality of Auditory and Visual Bistable Perception},
  volume = {22},
  number = {8},
  journaltitle = {Cerebral Cortex},
  date = {2011},
  pages = {1915--1922},
  author = {Kondo, Hirohito M. and Kitagawa, Norimichi and Kitamura, Miho S. and Koizumi, Ai and Nomura, Michio and Kashino, Makio},
  file = {D\:\\Sauve\\Zotero\\storage\\MNJJIJ88\\321099.html}
}

@article{shinn-cunninghamObjectbasedAuditoryVisual2008,
  langid = {english},
  title = {Object-Based Auditory and Visual Attention},
  volume = {12},
  issn = {1364-6613, 1879-307X},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(08)00060-0},
  doi = {10.1016/j.tics.2008.02.003},
  number = {5},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2018-11-05},
  date = {2008-05-01},
  pages = {182-186},
  author = {Shinn-Cunningham, Barbara G.},
  file = {D\:\\Sauve\\Zotero\\storage\\AINP2KAI\\Shinn-Cunningham - 2008 - Object-based auditory and visual attention.pdf;D\:\\Sauve\\Zotero\\storage\\8HGHMUEK\\S1364-6613(08)00060-0.html},
  eprinttype = {pmid},
  eprint = {18396091}
}

@article{mehtaAuditoryIllusionReveals2017,
  langid = {english},
  title = {An Auditory Illusion Reveals the Role of Streaming in the Temporal Misallocation of Perceptual Objects},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/content/372/1714/20160114},
  doi = {10.1098/rstb.2016.0114},
  abstract = {This study investigates the neural correlates and processes underlying the ambiguous percept produced by a stimulus similar to Deutsch's ‘octave illusion’, in which each ear is presented with a sequence of alternating pure tones of low and high frequencies. The same sequence is presented to each ear, but in opposite phase, such that the left and right ears receive a high–low–high … and a low–high–low … pattern, respectively. Listeners generally report hearing the illusion of an alternating pattern of low and high tones, with all the low tones lateralized to one side and all the high tones lateralized to the other side. The current explanation of the illusion is that it reflects an illusory feature conjunction of pitch and perceived location. Using psychophysics and electroencephalogram measures, we test this and an alternative hypothesis involving synchronous and sequential stream segregation, and investigate potential neural correlates of the illusion. We find that the illusion of alternating tones arises from the synchronous tone pairs across ears rather than sequential tones in one ear, suggesting that the illusion involves a misattribution of time across perceptual streams, rather than a misattribution of location within a stream. The results provide new insights into the mechanisms of binaural streaming and synchronous sound segregation.
This article is part of the themed issue ‘Auditory and visual scene analysis’.},
  number = {1714},
  journaltitle = {Phil. Trans. R. Soc. B},
  shortjournal = {Phil. Trans. R. Soc. B},
  urldate = {2018-11-05},
  date = {2017-02-19},
  pages = {20160114},
  author = {Mehta, Anahita H. and Jacoby, Nori and Yasin, Ifat and Oxenham, Andrew J. and Shamma, Shihab A.},
  file = {D\:\\Sauve\\Zotero\\storage\\6QCLL3KU\\Mehta et al. - 2017 - An auditory illusion reveals the role of streaming.pdf;D\:\\Sauve\\Zotero\\storage\\E4SS5ZN8\\20160114.html},
  eprinttype = {pmid},
  eprint = {28044024}
}

@article{vealeHowVisualSalience2017,
  langid = {english},
  title = {How Is Visual Salience Computed in the Brain? {{Insights}} from Behaviour, Neurobiology and Modelling},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/content/372/1714/20160113},
  doi = {10.1098/rstb.2016.0113},
  shorttitle = {How Is Visual Salience Computed in the Brain?},
  abstract = {Inherent in visual scene analysis is a bottleneck associated with the need to sequentially sample locations with foveating eye movements. The concept of a ‘saliency map’ topographically encoding stimulus conspicuity over the visual scene has proven to be an efficient predictor of eye movements. Our work reviews insights into the neurobiological implementation of visual salience computation. We start by summarizing the role that different visual brain areas play in salience computation, whether at the level of feature analysis for bottom-up salience or at the level of goal-directed priority maps for output behaviour. We then delve into how a subcortical structure, the superior colliculus (SC), participates in salience computation. The SC represents a visual saliency map via a centre-surround inhibition mechanism in the superficial layers, which feeds into priority selection mechanisms in the deeper layers, thereby affecting saccadic and microsaccadic eye movements. Lateral interactions in the local SC circuit are particularly important for controlling active populations of neurons. This, in turn, might help explain long-range effects, such as those of peripheral cues on tiny microsaccades. Finally, we show how a combination of in vitro neurophysiology and large-scale computational modelling is able to clarify how salience computation is implemented in the local circuit of the SC.
This article is part of the themed issue ‘Auditory and visual scene analysis’.},
  number = {1714},
  journaltitle = {Phil. Trans. R. Soc. B},
  shortjournal = {Phil. Trans. R. Soc. B},
  urldate = {2018-11-05},
  date = {2017-02-19},
  pages = {20160113},
  author = {Veale, Richard and Hafed, Ziad M. and Yoshida, Masatoshi},
  file = {D\:\\Sauve\\Zotero\\storage\\4A6MFRFU\\Veale et al. - 2017 - How is visual salience computed in the brain Insi.pdf;D\:\\Sauve\\Zotero\\storage\\T6S9TCSZ\\20160113.html},
  eprinttype = {pmid},
  eprint = {28044023}
}

@article{kondoAuditoryMultistabilityNeurotransmitter2017,
  langid = {english},
  title = {Auditory Multistability and Neurotransmitter Concentrations in the Human Brain},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/content/372/1714/20160110},
  doi = {10.1098/rstb.2016.0110},
  abstract = {Multistability in perception is a powerful tool for investigating sensory–perceptual transformations, because it produces dissociations between sensory inputs and subjective experience. Spontaneous switching between different perceptual objects occurs during prolonged listening to a sound sequence of tone triplets or repeated words (termed auditory streaming and verbal transformations, respectively). We used these examples of auditory multistability to examine to what extent neurochemical and cognitive factors influence the observed idiosyncratic patterns of switching between perceptual objects. The concentrations of glutamate–glutamine (Glx) and γ-aminobutyric acid (GABA) in brain regions were measured by magnetic resonance spectroscopy, while personality traits and executive functions were assessed using questionnaires and response inhibition tasks. Idiosyncratic patterns of perceptual switching in the two multistable stimulus configurations were identified using a multidimensional scaling (MDS) analysis. Intriguingly, although switching patterns within each individual differed between auditory streaming and verbal transformations, similar MDS dimensions were extracted separately from the two datasets. Individual switching patterns were significantly correlated with Glx and GABA concentrations in auditory cortex and inferior frontal cortex but not with the personality traits and executive functions. Our results suggest that auditory perceptual organization depends on the balance between neural excitation and inhibition in different brain regions.
This article is part of the themed issue ‘Auditory and visual scene analysis'.},
  number = {1714},
  journaltitle = {Phil. Trans. R. Soc. B},
  shortjournal = {Phil. Trans. R. Soc. B},
  urldate = {2018-11-05},
  date = {2017-02-19},
  pages = {20160110},
  author = {Kondo, Hirohito M. and Farkas, Dávid and Denham, Susan L. and Asai, Tomohisa and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\P67K7L8W\\Kondo et al. - 2017 - Auditory multistability and neurotransmitter conce.pdf;D\:\\Sauve\\Zotero\\storage\\XX2QA922\\20160110.html},
  eprinttype = {pmid},
  eprint = {28044020}
}

@article{linSingularNatureAuditory2017,
  langid = {english},
  title = {The Singular Nature of Auditory and Visual Scene Analysis in Autism},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  url = {http://rstb.royalsocietypublishing.org/content/372/1714/20160115},
  doi = {10.1098/rstb.2016.0115},
  abstract = {Individuals with autism spectrum disorder often have difficulty acquiring relevant auditory and visual information in daily environments, despite not being diagnosed as hearing impaired or having low vision. Resent psychophysical and neurophysiological studies have shown that autistic individuals have highly specific individual differences at various levels of information processing, including feature extraction, automatic grouping and top-down modulation in auditory and visual scene analysis. Comparison of the characteristics of scene analysis between auditory and visual modalities reveals some essential commonalities, which could provide clues about the underlying neural mechanisms. Further progress in this line of research may suggest effective methods for diagnosing and supporting autistic individuals.
This article is part of the themed issue ‘Auditory and visual scene analysis'.},
  number = {1714},
  journaltitle = {Phil. Trans. R. Soc. B},
  shortjournal = {Phil. Trans. R. Soc. B},
  urldate = {2018-11-05},
  date = {2017-02-19},
  pages = {20160115},
  author = {Lin, I.-Fan and Shirama, Aya and Kato, Nobumasa and Kashino, Makio},
  file = {D\:\\Sauve\\Zotero\\storage\\TXP6EX7Y\\Lin et al. - 2017 - The singular nature of auditory and visual scene a.pdf;D\:\\Sauve\\Zotero\\storage\\M5D38YH4\\20160115.html},
  eprinttype = {pmid},
  eprint = {28044025}
}

@article{osullivanAttentionalSelectionCocktail2014,
  title = {Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial {{EEG}}},
  volume = {25},
  number = {7},
  journaltitle = {Cerebral Cortex},
  date = {2014},
  pages = {1697--1706},
  author = {O'sullivan, James A. and Power, Alan J. and Mesgarani, Nima and Rajaram, Siddharth and Foxe, John J. and Shinn-Cunningham, Barbara G. and Slaney, Malcolm and Shamma, Shihab A. and Lalor, Edmund C.},
  file = {D\:\\Sauve\\Zotero\\storage\\RNDK2ETS\\457492.html}
}

@article{kayaInvestigatingBottomupAuditory2014,
  langid = {english},
  title = {Investigating Bottom-up Auditory Attention},
  volume = {8},
  issn = {1662-5161},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00327/full},
  doi = {10.3389/fnhum.2014.00327},
  abstract = {Bottom-up attention is a sensory-driven selection mechanism that directs perception towards a subset of the stimulus that is considered salient, or attention-grabbing. Most studies of bottom-up auditory attention have adapted frameworks similar to visual attention models whereby local or global "contrast" is a central concept in defining salient elements in a scene. In the current study, we take a more fundamental approach to modeling auditory attention; providing the first examination of the space of auditory saliency spanning pitch, intensity and timbre; and shedding light on complex interactions among these features. Informed by psychoacoustic results, we develop a computational model of auditory saliency implementing a novel attentional framework, guided by processes hypothesized to take place in the auditory pathway. In particular, the model tests the hypothesis that perception tracks the evolution of sound events in a multidimensional feature space, and flags any deviation from background statistics as salient. Predictions from the model corroborate the relationship between bottom-up auditory attention and statistical inference, and argues for a potential role of predictive coding as mechanism for saliency detection in acoustic scenes.},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  urldate = {2018-11-05},
  date = {2014},
  keywords = {Attention,Psychoacoustics,audition,Bottom-up,saliency},
  author = {Kaya, Emine Merve and Elhilali, Mounya},
  file = {D\:\\Sauve\\Zotero\\storage\\48IR85NL\\Kaya and Elhilali - 2014 - Investigating bottom-up auditory attention.pdf}
}

@inproceedings{kayaTemporalSaliencyMap2012,
  title = {A Temporal Saliency Map for Modeling Auditory Attention},
  booktitle = {Information {{Sciences}} and {{Systems}} ({{CISS}}), 2012 46th {{Annual Conference}} On},
  publisher = {{IEEE}},
  date = {2012},
  pages = {1--6},
  author = {Kaya, Emine Merve and Elhilali, Mounya},
  file = {D\:\\Sauve\\Zotero\\storage\\BJMRJWJ8\\Kaya and Elhilali - 2012 - A temporal saliency map for modeling auditory atte.pdf;D\:\\Sauve\\Zotero\\storage\\FJD8WA3P\\6310945.html}
}

@article{kayaModellingAuditoryAttention2017,
  title = {Modelling Auditory Attention},
  volume = {372},
  url = {http://rstb.royalsocietypublishing.org/content/372/1714/20160101.abstract},
  doi = {10.1098/rstb.2016.0101},
  abstract = {Sounds in everyday life seldom appear in isolation. Both humans and machines are constantly flooded with a cacophony of sounds that need to be sorted through and scoured for relevant information—a phenomenon referred to as the ‘cocktail party problem’. A key component in parsing acoustic scenes is the role of attention, which mediates perception and behaviour by focusing both sensory and cognitive resources on pertinent information in the stimulus space. The current article provides a review of modelling studies of auditory attention. The review highlights how the term attention refers to a multitude of behavioural and cognitive processes that can shape sensory processing. Attention can be modulated by ‘bottom-up’ sensory-driven factors, as well as ‘top-down’ task-specific goals, expectations and learned schemas. Essentially, it acts as a selection process or processes that focus both sensory and cognitive resources on the most relevant events in the soundscape; with relevance being dictated by the stimulus itself (e.g. a loud explosion) or by a task at hand (e.g. listen to announcements in a busy airport). Recent computational models of auditory attention provide key insights into its role in facilitating perception in cluttered auditory scenes.This article is part of the themed issue ‘Auditory and visual scene analysis’.},
  number = {1714},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  date = {2017-02-19},
  author = {Kaya, Emine Merve and Elhilali, Mounya}
}

@book{kondoAuditoryVisualScene2017,
  title = {Auditory and Visual Scene Analysis: An Overview},
  shorttitle = {Auditory and Visual Scene Analysis},
  publisher = {{The Royal Society}},
  date = {2017},
  author = {Kondo, Hirohito M. and van Loon, Anouk M. and Kawahara, Jun-Ichiro and Moore, Brian CJ},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\6G3GWPUP\\20160099.html;D\:\\Sauve\\Zotero\\storage\\KC8N59BN\\20160099.html}
}

@article{rey-mermetInhibitionAgingWhat2018,
  title = {Inhibition in Aging: {{What}} Is Preserved? {{What}} Declines? {{A}} Meta-Analysis},
  volume = {25},
  shorttitle = {Inhibition in Aging},
  number = {5},
  journaltitle = {Psychonomic bulletin \& review},
  date = {2018},
  pages = {1695--1716},
  author = {Rey-Mermet, Alodie and Gade, Miriam},
  file = {D\:\\Sauve\\Zotero\\storage\\8TEJV5V2\\s13423-017-1384-7.html;D\:\\Sauve\\Zotero\\storage\\JZGIWTBT\\s13423-017-1384-7.html}
}

@article{schneiderListeningAgingAdults2002,
  title = {Listening in Aging Adults: From Discourse Comprehension to Psychoacoustics},
  volume = {56},
  journaltitle = {Canadian Journal of Experimental Psychology/Revue canadienne de psychologie expérimentale},
  date = {2002},
  pages = {139-152},
  author = {Schneider, Bruce A. and Daneman, M and Pichora-Fuller, M. Kathleen}
}

@article{humesSpeechUnderstandingElderly1996,
  title = {Speech Understanding in the Elderly},
  volume = {7},
  journaltitle = {Journal of American Academy of Audiology},
  date = {1996},
  pages = {161-167},
  author = {Humes, LE}
}

@article{CommitteeHearingBioacoustics1988,
  title = {Committee on {{Hearing}}, {{Bioacoustics}}, and {{BiomechanicsSpeech}} Understanding and Aging},
  volume = {83},
  journaltitle = {Journal of the Acoustical Society of America},
  date = {1988},
  pages = {859-895}
}

@incollection{altenmullerStrongEmotionsMusic2013,
  title = {Strong Emotions in Music: Are They an Evolutionary Adaptation?},
  shorttitle = {Strong Emotions in Music},
  booktitle = {Sound-{{Perception}}-{{Performance}}},
  publisher = {{Springer}},
  date = {2013},
  pages = {131--156},
  author = {Altenmüller, Eckart and Kopiez, Reinhard and Grewe, Oliver}
}

@article{daikokuSingleNotDual2017,
  title = {Single, but Not Dual, Attention Facilitates Statistical Learning of Two Concurrent Auditory Sequences},
  volume = {7},
  number = {1},
  journaltitle = {Scientific Reports},
  date = {2017},
  pages = {10108},
  author = {Daikoku, Tatsuya and Yumoto, Masato},
  file = {D\:\\Sauve\\Zotero\\storage\\7RPM6EGP\\s41598-017-10476-x.html;D\:\\Sauve\\Zotero\\storage\\HHVW3F4L\\s41598-017-10476-x.html}
}

@article{daikokuNeurophysiologicalMarkersStatistical2018,
  title = {Neurophysiological {{Markers}} of {{Statistical Learning}} in {{Music}} and {{Language}}: {{Hierarchy}}, {{Entropy}} and {{Uncertainty}}},
  volume = {8},
  shorttitle = {Neurophysiological {{Markers}} of {{Statistical Learning}} in {{Music}} and {{Language}}},
  number = {6},
  journaltitle = {Brain sciences},
  date = {2018},
  pages = {114},
  author = {Daikoku, Tatsuya},
  file = {D\:\\Sauve\\Zotero\\storage\\RMJ67QZ5\\Daikoku - 2018 - Neurophysiological Markers of Statistical Learning.pdf;D\:\\Sauve\\Zotero\\storage\\RILTTC4K\\114.html}
}

@article{kondoNormalAgingSlows2018,
  title = {Normal {{Aging Slows Spontaneous Switching}} in {{Auditory}} and {{Visual Bistability}}},
  volume = {389},
  journaltitle = {Neuroscience},
  date = {2018},
  pages = {152--160},
  author = {Kondo, Hirohito M. and Kochiyama, Takanori},
  file = {D\:\\Sauve\\Zotero\\storage\\4KBDPMNX\\S0306452217302981.html;D\:\\Sauve\\Zotero\\storage\\67D3NUXF\\S0306452217302981.html}
}

@article{snyderSequentialAuditoryScene2006,
  title = {Sequential Auditory Scene Analysis Is Preserved in Normal Aging Adults},
  volume = {17},
  number = {3},
  journaltitle = {Cerebral Cortex},
  date = {2006},
  pages = {501--512},
  author = {Snyder, Joel S. and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\8LAZJBD3\\450169.html;D\:\\Sauve\\Zotero\\storage\\8QWLLERQ\\450169.html}
}

@article{stainsbyAuditoryStreamingBased2004,
  title = {Auditory Streaming Based on Temporal Structure in Hearing-Impaired Listeners},
  volume = {192},
  number = {1-2},
  journaltitle = {Hearing Research},
  date = {2004},
  pages = {119--130},
  author = {Stainsby, Thomas H. and Moore, Brian CJ and Glasberg, Brian R.},
  file = {D\:\\Sauve\\Zotero\\storage\\K47AXQLT\\S0378595504000760.html}
}

@article{summersF0ProcessingSeperation1998,
  title = {F0 Processing and the Seperation of Competing Speech Signals by Listeners with Normal Hearing and with Hearing Loss},
  volume = {41},
  number = {6},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  date = {1998},
  pages = {1294--1306},
  author = {Summers, Van and Leek, Marjorie R.},
  file = {D\:\\Sauve\\Zotero\\storage\\3UFP4E7L\\article.html}
}

@article{mooreThresholdsHearingMistuned1986,
  title = {Thresholds for Hearing Mistuned Partials as Separate Tones in Harmonic Complexes},
  volume = {80},
  number = {2},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {1986},
  pages = {479--483},
  author = {Moore, Brian CJ and Glasberg, Brian R. and Peters, Robert W.},
  file = {D\:\\Sauve\\Zotero\\storage\\Q2C2KS3L\\1.html}
}

@article{mooreThresholdsDetectionInharmonicity1985,
  title = {Thresholds for the Detection of Inharmonicity in Complex Tones},
  volume = {77},
  number = {5},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {1985},
  pages = {1861--1867},
  author = {Moore, Brian CJ and Peters, Robert W. and Glasberg, Brian R.},
  file = {D\:\\Sauve\\Zotero\\storage\\NPQWDPPD\\Moore et al. - 1985 - Thresholds for the detection of inharmonicity in c.pdf;D\:\\Sauve\\Zotero\\storage\\M29FFEXW\\1.html}
}

@inproceedings{harrisonDissociatingSensoryCognitive2018,
  location = {{Graz, Austria}},
  title = {Dissociating Sensory and Cognitive Theories of Harmony Perception through Computational Modeling},
  booktitle = {Parncutt, {{R}}., \& {{Sattmann}}, {{S}}. ({{Eds}}.) {{Proceedings}} of {{ICMPC15}}/{{ESCOM10}}},
  publisher = {{Centre for Systematic Musicology}},
  date = {2018},
  pages = {194-199},
  author = {Harrison, Peter MC and Pearce, Marcus T.},
  editor = {Parncutt, Richard and Sattmann, S.}
}

@article{sauveEffectsPitchTiming2018,
  title = {Effects of Pitch and Timing Expectancy on Musical Emotion.},
  volume = {28},
  doi = {http://dx.doi.org/10.1037/pmu0000203},
  number = {1},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  date = {2018},
  pages = {17-39},
  author = {Sauvé, Sarah A. and Sayed, Aminah and Dean, Roger T. and Pearce, Marcus T.},
  file = {D\:\\Sauve\\Zotero\\storage\\6RGF3ZSH\\Sauvé et al. - 2018 - Effects of pitch and timing expectancy on musical .pdf;D\:\\Sauve\\Zotero\\storage\\SI7M3RMN\\2018-22317-001.html}
}

@article{pearceStatisticalLearningProbabilistic2018,
  title = {Statistical Learning and Probabilistic Prediction in Music Cognition: Mechanisms of Stylistic Enculturation},
  volume = {1423},
  doi = {https://doi.org/10.1111/nyas.13654},
  shorttitle = {Statistical Learning and Probabilistic Prediction in Music Cognition},
  number = {1},
  journaltitle = {Annals of the New York Academy of Sciences},
  date = {2018},
  pages = {378-395},
  author = {Pearce, M. T.},
  file = {D\:\\Sauve\\Zotero\\storage\\XZ7798HS\\Pearce - 2018 - Statistical learning and probabilistic prediction .pdf;D\:\\Sauve\\Zotero\\storage\\PWR2JPBD\\nyas.html}
}

@article{omigieElectrophysiologicalCorrelatesMelodic2013,
  title = {Electrophysiological Correlates of Melodic Processing in Congenital Amusia},
  volume = {51},
  doi = {https://doi.org/10.1016/j.neuropsychologia.2013.05.010},
  number = {9},
  journaltitle = {Neuropsychologia},
  date = {2013},
  pages = {1749--1762},
  author = {Omigie, Diana and Pearce, Marcus T. and Williamson, Victoria J. and Stewart, Lauren},
  file = {D\:\\Sauve\\Zotero\\storage\\7Z58Q6TN\\S002839321300153X.html;D\:\\Sauve\\Zotero\\storage\\ZR7K2GM4\\S002839321300153X.html}
}

@article{omigieTrackingPitchProbabilities2012,
  title = {Tracking of Pitch Probabilities in Congenital Amusia},
  volume = {50},
  doi = {https://doi.org/10.1016/j.neuropsychologia.2012.02.034},
  number = {7},
  journaltitle = {Neuropsychologia},
  date = {2012},
  pages = {1483--1493},
  author = {Omigie, Diana and Pearce, Marcus T. and Stewart, Lauren},
  file = {D\:\\Sauve\\Zotero\\storage\\JBSVC87X\\S0028393212001042.html;D\:\\Sauve\\Zotero\\storage\\PUYNS7JF\\S0028393212001042.html}
}

@book{mackayInformationTheoryInference2003,
  location = {{Cambridge, UK}},
  title = {Information Theory, Inference and Learning Algorithms},
  publisher = {{Cambridge University Press}},
  date = {2003},
  author = {MacKay, David JC},
  file = {D\:\\Sauve\\Zotero\\storage\\5WFBC4MY\\books.html}
}

@article{maddenFourTenMilliseconds1992,
  title = {Four to Ten Milliseconds per Year: {{Age}}-Related Slowing of Visual Word Identification},
  volume = {47},
  shorttitle = {Four to Ten Milliseconds per Year},
  number = {2},
  journaltitle = {Journal of Gerontology},
  date = {1992},
  pages = {P59--P68},
  author = {Madden, David J.},
  file = {D\:\\Sauve\\Zotero\\storage\\AVFVYZ9Y\\610190.html}
}

@article{birrenAgingSpeedBehavior1995,
  title = {Aging and Speed of Behavior: {{Possible}} Consequences for Psychological Functioning},
  volume = {46},
  shorttitle = {Aging and Speed of Behavior},
  number = {1},
  journaltitle = {Annual review of psychology},
  date = {1995},
  pages = {329--353},
  author = {Birren, James E. and Fisher, Laurel M.},
  file = {D\:\\Sauve\\Zotero\\storage\\3XQDJQZ8\\Birren and Fisher - 1995 - Aging and speed of behavior Possible consequences.pdf;D\:\\Sauve\\Zotero\\storage\\8Q673Z45\\Birren and Fisher - 1995 - Aging and speed of behavior Possible consequences.pdf}
}

@incollection{ackermanKnowledgeCognitiveAging2011,
  title = {Knowledge and Cognitive Aging},
  booktitle = {The Handbook of Aging and Cognition},
  publisher = {{Psychology Press}},
  date = {2011},
  pages = {452--496},
  author = {Ackerman, Phillip L.},
  file = {D\:\\Sauve\\Zotero\\storage\\AJS8GE3P\\10.html}
}

@incollection{hornTheoryFluidCrystallized1982,
  title = {The Theory of Fluid and Crystallized Intelligence in Relation to Concepts of Cognitive Psychology and Aging in Adulthood},
  booktitle = {Aging and Cognitive Processes},
  publisher = {{Springer}},
  date = {1982},
  pages = {237--278},
  author = {Horn, John L.},
  file = {D\:\\Sauve\\Zotero\\storage\\FM93XAJ6\\978-1-4684-4178-9_14.html}
}

@article{hornAgeDifferencesFluid1967,
  title = {Age Differences in Fluid and Crystallized Intelligence},
  volume = {26},
  journaltitle = {Acta psychologica},
  date = {1967},
  pages = {107--129},
  author = {Horn, John L. and Cattell, Raymond B.},
  file = {D\:\\Sauve\\Zotero\\storage\\XLQ4DJDQ\\000169186790011X.html}
}

@article{wongAgingCorticalMechanisms2009,
  title = {Aging and Cortical Mechanisms of Speech Perception in Noise},
  volume = {47},
  number = {3},
  journaltitle = {Neuropsychologia},
  date = {2009},
  pages = {693--703},
  author = {Wong, Patrick CM and Jin, James Xumin and Gunasekera, Geshri M. and Abel, Rebekah and Lee, Edward R. and Dhar, Sumitrajit},
  file = {D\:\\Sauve\\Zotero\\storage\\S9TGMFNN\\S0028393208004624.html;D\:\\Sauve\\Zotero\\storage\\ZR8MSBGG\\PMC2649004.html}
}

@article{pichora-fullerHowYoungOld1995,
  title = {How Young and Old Adults Listen to and Remember Speech in Noise},
  volume = {97},
  number = {1},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {1995},
  pages = {593--608},
  author = {Pichora-Fuller, M. Kathleen and Schneider, Bruce A. and Daneman, Meredyth},
  file = {D\:\\Sauve\\Zotero\\storage\\G8SU66KZ\\1.html}
}

@article{finkelAgeChangesProcessing2007,
  title = {Age Changes in Processing Speed as a Leading Indicator of Cognitive Aging.},
  volume = {22},
  number = {3},
  journaltitle = {Psychology and aging},
  date = {2007},
  pages = {558},
  author = {Finkel, Deborah and Reynolds, Chandra A. and McArdle, John J. and Pedersen, Nancy L.},
  file = {D\:\\Sauve\\Zotero\\storage\\UEFEBVUR\\2007-13103-012.html}
}

@article{salthouseProcessingspeedTheoryAdult1996,
  title = {The Processing-Speed Theory of Adult Age Differences in Cognition.},
  volume = {103},
  number = {3},
  journaltitle = {Psychological review},
  date = {1996},
  pages = {403},
  author = {Salthouse, Timothy A.},
  file = {D\:\\Sauve\\Zotero\\storage\\MMTVLEMD\\Salthouse - 1996 - The processing-speed theory of adult age differenc.pdf;D\:\\Sauve\\Zotero\\storage\\MWRYM2DZ\\1996-01780-001.html}
}

@article{salthouseWorkingMemoryProcessing1990,
  title = {Working Memory as a Processing Resource in Cognitive Aging},
  volume = {10},
  number = {1},
  journaltitle = {Developmental review},
  date = {1990},
  pages = {101--124},
  author = {Salthouse, Timothy A.},
  file = {D\:\\Sauve\\Zotero\\storage\\QKKXTVBZ\\027322979090006P.html}
}

@article{oberauerRemovingIrrelevantInformation2001,
  title = {Removing Irrelevant Information from Working Memory: A Cognitive Aging Study with the Modified {{Sternberg}} Task.},
  volume = {27},
  shorttitle = {Removing Irrelevant Information from Working Memory},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  date = {2001},
  pages = {948},
  author = {Oberauer, Klaus},
  file = {D\:\\Sauve\\Zotero\\storage\\6YJNW9HJ\\Oberauer - 2001 - Removing irrelevant information from working memor.pdf;D\:\\Sauve\\Zotero\\storage\\C6HK63F2\\2001-01082-006.html}
}

@article{carpenterWorkingMemoryConstraints1994,
  title = {Working Memory Constraints in Comprehension: {{Evidence}} from Individual Differences, Aphasia, and Aging},
  shorttitle = {Working Memory Constraints in Comprehension},
  journaltitle = {Handbook of psycholinguistics},
  date = {1994},
  pages = {1075--1122},
  author = {Carpenter, Patricia A.},
  file = {D\:\\Sauve\\Zotero\\storage\\HWZUCGND\\10010733670.html}
}

@article{bellevilleExaminationWorkingMemory1996,
  title = {Examination of the Working Memory Components in Normal Aging and in Dementia of the {{Alzheimer}} Type},
  volume = {34},
  number = {3},
  journaltitle = {Neuropsychologia},
  date = {1996},
  pages = {195--207},
  author = {Belleville, Sylvie and Peretz, Isabelle and Malenfant, Dominique},
  file = {D\:\\Sauve\\Zotero\\storage\\ZCR758TU\\0028393295000976.html}
}

@article{gazzaleyTopdownSuppressionDeficit2005,
  title = {Top-down Suppression Deficit Underlies Working Memory Impairment in Normal Aging},
  volume = {8},
  number = {10},
  journaltitle = {Nature neuroscience},
  date = {2005},
  pages = {1298},
  author = {Gazzaley, Adam and Cooney, Jeffrey W. and Rissman, Jesse and D'esposito, Mark},
  file = {D\:\\Sauve\\Zotero\\storage\\RRVGKRKM\\nn1543.html}
}

@incollection{hasherWorkingMemoryComprehension1988,
  title = {Working Memory, Comprehension, and Aging: {{A}} Review and a New View},
  volume = {22},
  shorttitle = {Working Memory, Comprehension, and Aging},
  booktitle = {Psychology of Learning and Motivation},
  publisher = {{Elsevier}},
  date = {1988},
  pages = {193--225},
  author = {Hasher, Lynn and Zacks, Rose T.},
  file = {D\:\\Sauve\\Zotero\\storage\\JCD6VWJL\\S0079742108600419.html}
}

@article{salthouseAgingInhibitionWorking1995,
  title = {Aging, Inhibition, Working Memory, and Speed},
  volume = {50},
  number = {6},
  journaltitle = {The Journals of Gerontology Series B: Psychological Sciences and Social Sciences},
  date = {1995},
  pages = {P297--P306},
  author = {Salthouse, Timothy A. and Meinz, Elizabeth J.},
  file = {D\:\\Sauve\\Zotero\\storage\\L2CUERJG\\Salthouse and Meinz - 1995 - Aging, inhibition, working memory, and speed.pdf;D\:\\Sauve\\Zotero\\storage\\H2VJ6G2A\\603877.html}
}

@article{salthouseAgingWorkingMemory1994,
  title = {The Aging of Working Memory.},
  volume = {8},
  number = {4},
  journaltitle = {Neuropsychology},
  date = {1994},
  pages = {535},
  author = {Salthouse, Timothy A.},
  file = {D\:\\Sauve\\Zotero\\storage\\LXCA3JBN\\Salthouse - 1994 - The aging of working memory..pdf;D\:\\Sauve\\Zotero\\storage\\BB4ARMMV\\1995-04938-001.html}
}

@article{reppSensorimotorSynchronizationReview2005,
  title = {Sensorimotor Synchronization: A Review of the Tapping Literature},
  volume = {12},
  shorttitle = {Sensorimotor Synchronization},
  number = {6},
  journaltitle = {Psychonomic bulletin \& review},
  date = {2005},
  pages = {969--992},
  author = {Repp, Bruno H.},
  file = {D\:\\Sauve\\Zotero\\storage\\9FJM6A6D\\BF03206433.html;D\:\\Sauve\\Zotero\\storage\\G953E26U\\BF03206433.html}
}

@book{reganEvokedPotentialsEvoked1989,
  title = {Evoked Potentials and Evoked Magnetic Fields in Science and Medicine. {{Human}} Brain Electrophysiology},
  publisher = {{New York: Elsevier}},
  date = {1989},
  author = {Regan, D.}
}

@article{chakrabartyGestaltInferenceModel2019,
  title = {A {{Gestalt}} Inference Model for Auditory Scene Segregation},
  volume = {15},
  number = {1},
  journaltitle = {PLoS computational biology},
  date = {2019},
  pages = {e1006711},
  author = {Chakrabarty, Debmalya and Elhilali, Mounya},
  file = {D\:\\Sauve\\Zotero\\storage\\PWSBXUF5\\article.html}
}

@book{londonHearingTimePsychological2012,
  location = {{New York, NY}},
  title = {Hearing in Time: {{Psychological}} Aspects of Musical Meter},
  shorttitle = {Hearing in Time},
  publisher = {{Oxford University Press}},
  date = {2012},
  author = {London, Justin},
  file = {D\:\\Sauve\\Zotero\\storage\\KRJ22ZWE\\London - 2012 - Hearing in time Psychological aspects of musical .pdf;D\:\\Sauve\\Zotero\\storage\\MBK7UQIN\\books.html}
}

@article{kropotovEffectAgingERP2016,
  title = {Effect of Aging on {{ERP}} Components of Cognitive Control},
  volume = {8},
  journaltitle = {Frontiers in aging neuroscience},
  date = {2016},
  pages = {69},
  author = {Kropotov, Juri and Ponomarev, Valery and Tereshchenko, Ekaterina P. and Müller, Andreas and Jäncke, Lutz},
  file = {D\:\\Sauve\\Zotero\\storage\\G6WT384N\\full.html}
}

@inproceedings{cambouropoulosIdiomindependentRepresentationChords2014,
  title = {An Idiom-Independent Representation of Chords for Computational Music Analysis and Generation},
  booktitle = {{{ICMC}}},
  date = {2014},
  author = {Cambouropoulos, Emilios and Kaliakatsos-Papakostas, Maximos A. and Tsougras, Costas},
  file = {D\:\\Sauve\\Zotero\\storage\\G5DMU9NS\\Cambouropoulos et al. - 2014 - An idiom-independent representation of chords for .pdf}
}

@article{pophamInharmonicSpeechReveals2018,
  title = {Inharmonic Speech Reveals the Role of Harmonicity in the Cocktail Party Problem},
  volume = {9},
  number = {1},
  journaltitle = {Nature communications},
  date = {2018},
  pages = {2122},
  author = {Popham, Sara and Boebinger, Dana and Ellis, Dan PW and Kawahara, Hideki and McDermott, Josh H.},
  file = {D\:\\Sauve\\Zotero\\storage\\8ZWMEJNQ\\s41467-018-04551-8.html}
}

@article{woodsSchemaLearningCocktail2018,
  title = {Schema Learning for the Cocktail Party Problem},
  volume = {115},
  number = {14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2018},
  pages = {E3313--E3322},
  author = {Woods, Kevin JP and McDermott, Josh H.},
  file = {D\:\\Sauve\\Zotero\\storage\\X2Q5MFZQ\\Woods and McDermott - 2018 - Schema learning for the cocktail party problem.pdf;D\:\\Sauve\\Zotero\\storage\\ZQIH3658\\E3313.html}
}

@article{morse-fortierEffectsMusicalTraining2017,
  title = {The Effects of Musical Training on Speech Detection in the Presence of Informational and Energetic Masking},
  volume = {21},
  journaltitle = {Trends in hearing},
  date = {2017},
  pages = {2331216517739427},
  author = {Morse-Fortier, Charlotte and Parrish, Mary M. and Baran, Jane A. and Freyman, Richard L.},
  file = {D\:\\Sauve\\Zotero\\storage\\XQE8EG57\\Morse-Fortier et al. - 2017 - The effects of musical training on speech detectio.pdf;D\:\\Sauve\\Zotero\\storage\\MAAWF3RM\\2331216517739427.html}
}

@article{coffeyNeuralCorrelatesEarly2017,
  title = {Neural Correlates of Early Sound Encoding and Their Relationship to Speech-in-Noise Perception},
  volume = {11},
  journaltitle = {Frontiers in neuroscience},
  date = {2017},
  pages = {479},
  author = {Coffey, Emily BJ and Chepesiuk, Alexander MP and Herholz, Sibylle C. and Baillet, Sylvain and Zatorre, Robert J.},
  file = {D\:\\Sauve\\Zotero\\storage\\ZWU7WIY4\\full.html}
}

@article{puschmannMusiciansCocktailParty2018,
  title = {Musicians at the {{Cocktail Party}}: {{Neural Substrates}} of {{Musical Training During Selective Listening}} in {{Multispeaker Situations}}},
  shorttitle = {Musicians at the {{Cocktail Party}}},
  journaltitle = {Cerebral Cortex},
  date = {2018},
  author = {Puschmann, Sebastian and Baillet, Sylvain and Zatorre, Robert J.},
  file = {D\:\\Sauve\\Zotero\\storage\\S42NFMYA\\5078215.html}
}

@article{coffeyMusicInNoiseTaskMINT2019,
  title = {The {{Music}}-{{In}}-{{Noise Task}} ({{MINT}}): A Tool for Dissecting Complex Auditory Perception},
  volume = {13},
  shorttitle = {The {{Music}}-{{In}}-{{Noise Task}} ({{MINT}})},
  journaltitle = {Frontiers in Neuroscience},
  date = {2019},
  author = {Coffey, Emily BJ and Arseneau-Bruneau, Isabelle and Zhang, Xiaochen and Zatorre, Robert J.},
  file = {D\:\\Sauve\\Zotero\\storage\\Q2VNZNGM\\PMC6427094.html}
}

@article{meha-bettisonEnhancedSpeechPerception2018,
  title = {Enhanced Speech Perception in Noise and Cortical Auditory Evoked Potentials in Professional Musicians},
  volume = {57},
  number = {1},
  journaltitle = {International journal of audiology},
  date = {2018},
  pages = {40--52},
  author = {Meha-Bettison, Kiriana and Sharma, Mridula and Ibrahim, Ronny K. and Mandikal Vasuki, Pragati Rao},
  file = {D\:\\Sauve\\Zotero\\storage\\QDRP9F2S\\Meha-Bettison et al. - 2018 - Enhanced speech perception in noise and cortical a.pdf;D\:\\Sauve\\Zotero\\storage\\BLUR635V\\14992027.2017.html}
}

@article{mankelInherentAuditorySkills2018,
  title = {Inherent Auditory Skills Rather than Formal Music Training Shape the Neural Encoding of Speech},
  volume = {115},
  number = {51},
  journaltitle = {Proceedings of the National Academy of Sciences},
  date = {2018},
  pages = {13129--13134},
  author = {Mankel, Kelsey and Bidelman, Gavin M.},
  file = {D\:\\Sauve\\Zotero\\storage\\G2BT3CSQ\\Mankel and Bidelman - 2018 - Inherent auditory skills rather than formal music .pdf;D\:\\Sauve\\Zotero\\storage\\6BMS9EEB\\13129.html}
}

@article{bidelmanAfferentefferentConnectivityAuditory2019,
  title = {Afferent-Efferent Connectivity between Auditory Brainstem and Cortex Accounts for Poorer Speech-in-Noise Comprehension in Older Adults},
  journaltitle = {bioRxiv},
  date = {2019},
  pages = {568840},
  author = {Bidelman, Gavin M. and Price, Caitlin N. and Shen, Dawei and Arnott, Stephen and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\Y6L44BYT\\Bidelman et al. - 2019 - Afferent-efferent connectivity between auditory br.pdf;D\:\\Sauve\\Zotero\\storage\\38636BED\\568840v1.html}
}

@article{louiHumansRapidlyLearn2010,
  title = {Humans Rapidly Learn Grammatical Structure in a New Musical Scale},
  volume = {27},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2010},
  pages = {377--388},
  author = {Loui, Psyche and Wessel, David L. and Kam, Carla L. Hudson},
  file = {D\:\\Sauve\\Zotero\\storage\\EMFTXZ36\\Loui et al. - 2010 - Humans rapidly learn grammatical structure in a ne.pdf;D\:\\Sauve\\Zotero\\storage\\3ETIVAT6\\377.html}
}

@article{searsSimulatingMelodicHarmonic2018,
  title = {Simulating Melodic and Harmonic Expectations for Tonal Cadences Using Probabilistic Models},
  volume = {47},
  doi = {https://doi.org/10.1080/09298215.2017.1367010},
  number = {1},
  journaltitle = {Journal of New Music Research},
  date = {2018},
  pages = {29--52},
  author = {Sears, David RW and Pearce, Marcus T. and Caplin, William E. and McAdams, Stephen},
  file = {D\:\\Sauve\\Zotero\\storage\\52TFU3E3\\Sears et al. - 2018 - Simulating melodic and harmonic expectations for t.pdf;D\:\\Sauve\\Zotero\\storage\\LIM57ML7\\09298215.2017.html}
}

@article{margulisBetterUnderstandingPerceived2016,
  title = {Toward {{A Better Understanding}} of {{Perceived Complexity}} in {{Music}}: {{A Commentary}} on {{Eerola}} (2016)},
  volume = {11},
  doi = {http://dx.doi.org/10.18061/emr.v11i1.5275},
  shorttitle = {Toward {{A Better Understanding}} of {{Perceived Complexity}} in {{Music}}},
  number = {1},
  journaltitle = {Empirical Musicology Review},
  date = {2016},
  pages = {18--19},
  author = {Margulis, Elizabeth Hellmuth},
  file = {D\:\\Sauve\\Zotero\\storage\\JX6ZC3WW\\Margulis - 2016 - Toward A Better Understanding of Perceived Complex.pdf;D\:\\Sauve\\Zotero\\storage\\92QGHWWH\\5275.html}
}

@article{eerolaPerceivedComplexityWestern2006,
  title = {Perceived Complexity of Western and {{African}} Folk Melodies by Western and {{African}} Listeners},
  volume = {34},
  doi = {https://doi.org/10.1177/0305735606064842},
  number = {3},
  journaltitle = {Psychology of Music},
  date = {2006},
  pages = {337--371},
  author = {Eerola, Tuomas and Himberg, Tommi and Toiviainen, Petri and Louhivuori, Jukka},
  file = {D\:\\Sauve\\Zotero\\storage\\E9FXXGUF\\Eerola et al. - 2006 - Perceived complexity of western and African folk m.pdf;D\:\\Sauve\\Zotero\\storage\\NAMTDB2V\\0305735606064842.html}
}

@article{buntonSemanticallyMotivatedImprovements1997,
  title = {Semantically Motivated Improvements for {{PPM}} Variants},
  volume = {40},
  doi = {10.1093/comjnl/40.2_and_3.76},
  number = {2-3},
  journaltitle = {The Computer Journal},
  date = {1997},
  pages = {76--93},
  author = {Bunton, Suzanne},
  file = {D\:\\Sauve\\Zotero\\storage\\H83MYIQ6\\8138774.html}
}

@article{begleiterPredictionUsingVariable2004,
  title = {On Prediction Using Variable Order {{Markov}} Models},
  volume = {22},
  journaltitle = {Journal of Artificial Intelligence Research},
  date = {2004},
  pages = {385--421},
  author = {Begleiter, Ron and El-Yaniv, Ran and Yona, Golan},
  file = {D\:\\Sauve\\Zotero\\storage\\ARQ4SFQ2\\Begleiter et al. - 2004 - On prediction using variable order Markov models.pdf;D\:\\Sauve\\Zotero\\storage\\VJX2U6UR\\10394.html}
}

@article{albrechtModelingMusicalComplexity2016,
  title = {Modeling {{Musical Complexity}}: {{Commentary}} on {{Eerola}} (2016)},
  volume = {11},
  shorttitle = {Modeling {{Musical Complexity}}},
  number = {1},
  journaltitle = {Empirical Musicology Review},
  date = {2016},
  pages = {20--26},
  author = {Albrecht, Joshua},
  file = {D\:\\Sauve\\Zotero\\storage\\CTQE2HAA\\Albrecht - 2016 - Modeling Musical Complexity Commentary on Eerola .pdf;D\:\\Sauve\\Zotero\\storage\\PSLDTMVV\\5197.html}
}

@article{trehubMusicalityLifespan2019,
  title = {7 {{Musicality}} across the {{Lifespan}}},
  journaltitle = {Foundations in Music Psychology: Theory and Research},
  date = {2019},
  pages = {265},
  author = {Trehub, Sandra E. and Weiss, Michael W. and Cirelli, Laura K.},
  file = {D\:\\Sauve\\Zotero\\storage\\A5MF8ENU\\books.html}
}

@article{krishnanBeatboxersGuitaristsEngage2018,
  title = {Beatboxers and Guitarists Engage Sensorimotor Regions Selectively When Listening to the Instruments They Can Play},
  volume = {28},
  number = {11},
  journaltitle = {Cerebral Cortex},
  date = {2018},
  pages = {4063--4079},
  author = {Krishnan, Saloni and Lima, César F. and Evans, Samuel and Chen, Sinead and Guldner, Stella and Yeff, Harry and Manly, Tom and Scott, Sophie K.},
  file = {D\:\\Sauve\\Zotero\\storage\\JA4JNGBI\\5087958.html;D\:\\Sauve\\Zotero\\storage\\NHES8HHS\\5087958.html}
}

@article{thomassenAssessingBackgroundDecomposition2018,
  title = {Assessing the Background Decomposition of a Complex Auditory Scene with Event-Related Brain Potentials},
  volume = {370},
  journaltitle = {Hearing research},
  date = {2018},
  pages = {120--129},
  author = {Thomassen, Sabine and Bendixen, Alexandra},
  file = {D\:\\Sauve\\Zotero\\storage\\MCQA5CDD\\S0378595518301485.html}
}

@article{chiappePhrasingInfluencesRecognition1997,
  title = {Phrasing Influences the Recognition of Melodies},
  volume = {4},
  number = {2},
  journaltitle = {Psychonomic Bulletin \& Review},
  date = {1997},
  pages = {254--259},
  author = {Chiappe, Penny and Schmuckler, Mark A.},
  file = {D\:\\Sauve\\Zotero\\storage\\GU2AUQYM\\Chiappe and Schmuckler - 1997 - Phrasing influences the recognition of melodies.pdf;D\:\\Sauve\\Zotero\\storage\\84EERA3P\\BF03209402.html}
}

@article{vuvanTonalHierarchyRepresentations2011,
  title = {Tonal Hierarchy Representations in Auditory Imagery},
  volume = {39},
  number = {3},
  journaltitle = {Memory \& cognition},
  date = {2011},
  pages = {477--490},
  author = {Vuvan, Dominique T. and Schmuckler, Mark A.},
  file = {D\:\\Sauve\\Zotero\\storage\\D8GQA9KZ\\s13421-010-0032-5.html}
}

@article{dalyNotAllMusicians2018,
  title = {Not All Musicians Are Created Equal: {{Statistical}} Concerns Regarding the Categorization of Participants.},
  volume = {28},
  shorttitle = {Not All Musicians Are Created Equal},
  number = {2},
  journaltitle = {Psychomusicology: Music, Mind, and Brain},
  date = {2018},
  pages = {117},
  author = {Daly, Heather R. and Hall, Michael D.},
  file = {D\:\\Sauve\\Zotero\\storage\\BZ22NRFG\\2018-36317-003.html}
}

@thesis{searsClassicalCadenceClosing2017,
  title = {The Classical Cadence as a Closing Schema: {{Learning}}, Memory, and Perception},
  shorttitle = {The Classical Cadence as a Closing Schema},
  institution = {{McGill University Libraries}},
  type = {PhD Thesis},
  date = {2017},
  author = {Sears, David}
}

@article{vuvanMusicalStyleAffects2019,
  langid = {english},
  title = {Musical {{Style Affects}} the {{Strength}} of {{Harmonic Expectancy}}},
  volume = {2},
  issn = {2059-2043},
  url = {https://doi.org/10.1177/2059204318816066},
  doi = {10.1177/2059204318816066},
  abstract = {Research in music perception has typically focused on common-practice music (tonal music from the Western European tradition, ca. 1750–1900) as a model of Western musical structure. However, recent research indicates that different styles within Western tonal music may follow distinct harmonic syntaxes. The current study investigated whether listeners can adapt their harmonic expectations when listening to different musical styles. In two experiments, listeners were presented with short musical excerpts that primed either rock or classical music, followed by a timbre-matched cadence. Results from both experiments indicated that listeners prefer V-I cadences over bVII-I cadences within a classical context, but that this preference is significantly diminished in a rock context. Our findings provide empirical support for the idea that different musical styles do employ different harmonic syntaxes. Furthermore, listeners are not only sensitive to these differences, but are able to adapt their expectations depending on the listening context.},
  journaltitle = {Music \& Science},
  shortjournal = {Music \& Science},
  urldate = {2019-02-12},
  date = {2019-01-01},
  pages = {2059204318816066},
  author = {Vuvan, Dominique T. and Hughes, Bryn},
  file = {D\:\\Sauve\\Zotero\\storage\\DR8UZVT2\\Vuvan and Hughes - 2019 - Musical Style Affects the Strength of Harmonic Exp.pdf}
}

@inproceedings{jordaDigitalInstrumentsPlayers2004,
  title = {Digital {{Instruments}} and {{Players}}: {{Part II}}-{{Diversity}}, {{Freedom}} and {{Control}}.},
  shorttitle = {Digital {{Instruments}} and {{Players}}},
  booktitle = {{{ICMC}}},
  date = {2004},
  author = {Jordà, Sergi},
  file = {D\:\\Sauve\\Zotero\\storage\\SCMMDQKV\\Jordà - 2004 - Digital Instruments and Players Part II-Diversity.pdf}
}

@article{jordaInstrumentsPlayersThoughts2004,
  title = {Instruments and Players: {{Some}} Thoughts on Digital Lutherie},
  volume = {33},
  shorttitle = {Instruments and Players},
  number = {3},
  journaltitle = {Journal of New Music Research},
  date = {2004},
  pages = {321--341},
  author = {Jordà, Sergi},
  file = {D\:\\Sauve\\Zotero\\storage\\VJ2L7KZZ\\Jordà - 2004 - Instruments and players Some thoughts on digital .pdf;D\:\\Sauve\\Zotero\\storage\\HREDM3KZ\\0929821042000317886.html}
}

@inproceedings{jordaDigitalInstrumentsPlayers2004a,
  title = {Digital Instruments and Players: Part {{I}}—Efficiency and Apprenticeship},
  shorttitle = {Digital Instruments and Players},
  booktitle = {Proceedings of the 2004 Conference on {{New}} Interfaces for Musical Expression},
  publisher = {{National University of Singapore}},
  date = {2004},
  pages = {59--63},
  author = {Jordà, Sergi},
  file = {D\:\\Sauve\\Zotero\\storage\\8JTPHMQC\\Jordà - 2004 - Digital instruments and players part I—efficiency.pdf;D\:\\Sauve\\Zotero\\storage\\9YRP6CRN\\citation.html}
}

@article{wanderleyEvaluationInputDevices2002,
  title = {Evaluation of Input Devices for Musical Expression: {{Borrowing}} Tools from Hci},
  volume = {26},
  shorttitle = {Evaluation of Input Devices for Musical Expression},
  number = {3},
  journaltitle = {Computer Music Journal},
  date = {2002},
  pages = {62--76},
  author = {Wanderley, Marcelo Mortensen and Orio, Nicola},
  file = {D\:\\Sauve\\Zotero\\storage\\4WGY4ABF\\Wanderley and Orio - 2002 - Evaluation of input devices for musical expression.pdf}
}

@article{omodhrainFrameworkEvaluationDigital2011,
  title = {A Framework for the Evaluation of Digital Musical Instruments},
  volume = {35},
  number = {1},
  journaltitle = {Computer Music Journal},
  date = {2011},
  pages = {28--42},
  author = {O'modhrain, Sile},
  file = {D\:\\Sauve\\Zotero\\storage\\VH8NSL8F\\O'modhrain - 2011 - A framework for the evaluation of digital musical .pdf;D\:\\Sauve\\Zotero\\storage\\F3SZTNCY\\COMJ_a_00038.html}
}

@article{johnstonDesigningEvaluatingVirtual2008,
  title = {Designing and Evaluating Virtual Musical Instruments: Facilitating Conversational User Interaction},
  volume = {29},
  shorttitle = {Designing and Evaluating Virtual Musical Instruments},
  number = {6},
  journaltitle = {Design Studies},
  date = {2008},
  pages = {556--571},
  author = {Johnston, Andrew and Candy, Linda and Edmonds, Ernest},
  file = {D\:\\Sauve\\Zotero\\storage\\ZRZC7YSR\\S0142694X08000665.html}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical Power Analysis for the Behavioural Sciences},
  publisher = {{Hillsdale, NJ: Erlbaum}},
  date = {1988},
  author = {Cohen, Jacob}
}

@article{mourauxAcrosstrialAveragingEventrelated2008,
  title = {Across-Trial Averaging of Event-Related {{EEG}} Responses and Beyond},
  volume = {26},
  issn = {0730-725X},
  url = {http://www.sciencedirect.com/science/article/pii/S0730725X08000878},
  doi = {10.1016/j.mri.2008.01.011},
  abstract = {Internally and externally triggered sensory, motor and cognitive events elicit a number of transient changes in the ongoing electroencephalogram (EEG): event-related brain potentials (ERPs), event-related synchronization and desynchronization (ERS/ERD), and event-related phase resetting (ERPR). To increase the signal-to-noise ratio of event-related brain responses, most studies rely on across-trial averaging in the time domain, a procedure that is, however, blind to a significant fraction of the elicited cortical activity. Here, we outline the key concepts underlying the limitations of time-domain averaging and consider three alternative methodological approaches that have received increasing interest: time-frequency decomposition of the EEG (using the continuous wavelet transform), blind source separation of the EEG (using Independent Component Analysis) and the analysis of event-related brain responses at the level of single trials. In addition, we provide practical guidelines on the implementation of these methods and on the interpretation of the results they produce.},
  number = {7},
  journaltitle = {Magnetic Resonance Imaging},
  shortjournal = {Magnetic Resonance Imaging},
  series = {Proceedings of the {{International School}} on {{Magnetic Resonance}} and {{Brain Function}}},
  urldate = {2019-02-05},
  date = {2008-09-01},
  pages = {1041-1054},
  keywords = {Time-frequency analysis,Blind source separation (BSS),EEG analysis,Electrophysiology,Event-related desynchronization (ERD),Event-related phase resetting (ERPR),Event-related potentials (ERPs),Event-related synchronization (ERS),Independent component analysis (ICA),Single-trial analysis},
  author = {Mouraux, A. and Iannetti, G. D.},
  file = {D\:\\Sauve\\Zotero\\storage\\ZMXIR5IG\\Mouraux and Iannetti - 2008 - Across-trial averaging of event-related EEG respon.pdf;D\:\\Sauve\\Zotero\\storage\\ARICH76T\\S0730725X08000878.html}
}

@article{knightPrefrontalCortexRegulates1999,
  title = {Prefrontal Cortex Regulates Inhibition and Excitation in Distributed Neural Networks},
  volume = {101},
  number = {2-3},
  journaltitle = {Acta psychologica},
  date = {1999},
  pages = {159--178},
  author = {Knight, Robert T. and Staines, W. Richard and Swick, Diane and Chao, Linda L.},
  file = {D\:\\Sauve\\Zotero\\storage\\XS6YBI6A\\Knight et al. - 1999 - Prefrontal cortex regulates inhibition and excitat.pdf;D\:\\Sauve\\Zotero\\storage\\XMK9FJWV\\S0001691899000049.html}
}

@article{kokVarietiesInhibitionManifestations1999,
  title = {Varieties of Inhibition: Manifestations in Cognition, Event-Related Potentials and Aging},
  volume = {101},
  shorttitle = {Varieties of Inhibition},
  number = {2-3},
  journaltitle = {Acta psychologica},
  date = {1999},
  pages = {129--158},
  author = {Kok, Albert},
  file = {D\:\\Sauve\\Zotero\\storage\\LC46A2ZB\\S0001691899000037.html}
}

@article{boettcherFrequencymodulationFollowingResponse2002,
  title = {The Frequency-Modulation Following Response in Young and Aged Human Subjects},
  volume = {165},
  number = {1-2},
  journaltitle = {Hearing research},
  date = {2002},
  pages = {10--18},
  author = {Boettcher, Flint A. and Madhotra, Deepali and Poth, Elizabeth A. and Mills, John H.},
  file = {D\:\\Sauve\\Zotero\\storage\\PV49DTKK\\S0378595501003987.html}
}

@article{koelschPredictiveProcessesPeculiar2018,
  title = {Predictive {{Processes}} and the {{Peculiar Case}} of {{Music}}},
  journaltitle = {Trends in cognitive sciences},
  date = {2018},
  author = {Koelsch, Stefan and Vuust, Peter and Friston, Karl},
  file = {D\:\\Sauve\\Zotero\\storage\\4BLIM6TS\\S1364661318302547.html}
}

@article{pettigrewSubtitledVideosMismatch2004,
  title = {Subtitled Videos and Mismatch Negativity ({{MMN}}) Investigations of Spoken Word Processing},
  volume = {15},
  number = {7},
  journaltitle = {Journal of the American Academy of Audiology},
  date = {2004},
  pages = {469--485},
  author = {Pettigrew, Catharine M. and Murdoch, Bruce E. and Ponton, Curtis W. and Kei, Joseph and Chenery, Helen J. and Alku, Paavo},
  file = {D\:\\Sauve\\Zotero\\storage\\225CLLFB\\Pettigrew et al. - 2004 - Subtitled videos and mismatch negativity (MMN) inv.pdf;D\:\\Sauve\\Zotero\\storage\\DN7D2K34\\art00002.html}
}

@article{lagroisNeurophysiologicalBehavioralDifferences2018,
  title = {Neurophysiological and {{Behavioral Differences}} between {{Older}} and {{Younger Adults When Processing Violations}} of {{Tonal Structure}} in {{Music}}},
  volume = {12},
  journaltitle = {Frontiers in neuroscience},
  date = {2018},
  pages = {54},
  author = {Lagrois, Marie-Élaine and Peretz, Isabelle and Zendel, Benjamin Rich},
  file = {D\:\\Sauve\\Zotero\\storage\\TBL25FHL\\full.html}
}

@article{ragotTimeMusicAging2002,
  title = {Time, Music, and Aging.},
  volume = {18},
  number = {1-2},
  journaltitle = {Psychomusicology: A Journal of Research in Music Cognition},
  date = {2002},
  pages = {28},
  author = {Ragot, Richard and Ferrandez, Anne-Marie and Pouthas, Viviane},
  file = {D\:\\Sauve\\Zotero\\storage\\VABVXHDS\\2003-09252-004.html}
}

@article{yordanovaEEGThetaFrontal1998,
  title = {{{EEG}} Theta and Frontal Alpha Oscillations during Auditory Processing Change with Aging},
  volume = {108},
  number = {5},
  journaltitle = {Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section},
  date = {1998},
  pages = {497--505},
  author = {Yordanova, Juliana Y. and Kolev, Vasil N. and Başar, Erol},
  file = {D\:\\Sauve\\Zotero\\storage\\8G7VQNPX\\S0168559798000288.html}
}

@article{klimeschEEGAlphaTheta1999,
  title = {{{EEG}} Alpha and Theta Oscillations Reflect Cognitive and Memory Performance: A Review and Analysis},
  volume = {29},
  shorttitle = {{{EEG}} Alpha and Theta Oscillations Reflect Cognitive and Memory Performance},
  number = {2-3},
  journaltitle = {Brain research reviews},
  date = {1999},
  pages = {169--195},
  author = {Klimesch, Wolfgang},
  file = {D\:\\Sauve\\Zotero\\storage\\55ZG836S\\Klimesch - 1999 - EEG alpha and theta oscillations reflect cognitive.pdf;D\:\\Sauve\\Zotero\\storage\\4RLJSX3K\\S0165017398000563.html}
}

@article{henryAgingAffectsBalance2017,
  title = {Aging Affects the Balance of Neural Entrainment and Top-down Neural Modulation in the Listening Brain},
  volume = {8},
  journaltitle = {Nature communications},
  date = {2017},
  pages = {15801},
  author = {Henry, Molly J. and Herrmann, Björn and Kunke, Dunja and Obleser, Jonas},
  file = {D\:\\Sauve\\Zotero\\storage\\7SB7BZE8\\ncomms15801.html}
}

@article{rossNeuralEncodingSound2009,
  title = {Neural Encoding of Sound Duration Persists in Older Adults},
  volume = {47},
  number = {2},
  journaltitle = {Neuroimage},
  date = {2009},
  pages = {678--687},
  author = {Ross, Bernhard and Snyder, Joel S. and Aalto, Meaghan and McDonald, Kelly L. and Dyson, Benjamin J. and Schneider, Bruce and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\U4RVHTF3\\Ross et al. - 2009 - Neural encoding of sound duration persists in olde.pdf;D\:\\Sauve\\Zotero\\storage\\XRXN2SIL\\S1053811909004170.html}
}

@article{alainEffectsAgeBackground2012,
  title = {Effects of Age and Background Noise on Processing a Mistuned Harmonic in an Otherwise Periodic Complex Sound},
  volume = {283},
  number = {1-2},
  journaltitle = {Hearing research},
  date = {2012},
  pages = {126--135},
  author = {Alain, Claude and McDonald, Kelly and Van Roon, Patricia},
  file = {D\:\\Sauve\\Zotero\\storage\\BNVLSNEF\\S0378595511002619.html}
}

@article{pictonHumanAuditoryEvoked1974,
  title = {Human Auditory Evoked Potentials. {{I}}: {{Evaluation}} of Components},
  volume = {36},
  shorttitle = {Human Auditory Evoked Potentials. {{I}}},
  journaltitle = {Electroencephalography and clinical neurophysiology},
  date = {1974},
  pages = {179--190},
  author = {Picton, Terry W. and Hillyard, Steven A. and Krausz, Howard I. and Galambos, Robert},
  file = {D\:\\Sauve\\Zotero\\storage\\ERUTZYMD\\0013469474901552.html}
}

@book{pictonHumanAuditoryEvoked2010,
  langid = {english},
  title = {Human {{Auditory Evoked Potentials}}},
  isbn = {978-1-59756-622-3},
  abstract = {This book reviews how we can record the human brain\&\#39;s response to sounds, and how we can use these recordings to assess hearing. These recordings are used in many different clinical situations--the identification of hearing impairment in newborn infants, the detection of tumors on the auditory nerve, the diagnosis of multiple sclerosis. As well they are used to investigate how the brain is able to hear--how we can attend to particular conversations at a cocktail party and ignore others, how we learn to understand the language we are exposed to, why we have difficulty hearing when we grow old. This book is written by a single author with wide experience in all aspects of these recordings. The content is complete in terms of the essentials. The style is clear; equations are absent and figures are multiple. The intent of the book is to make learning enjoyable and meaningful. Allusions are made to fields beyond the ear, and the clinical importance of the phenomena is always considered.},
  pagetotal = {649},
  publisher = {{Plural Publishing}},
  date = {2010-09-01},
  keywords = {Medical / Audiology & Speech Pathology},
  author = {Picton, Terence W.}
}

@book{haalandWhatDoesWMS2001,
  title = {What Does the {{WMS}}–{{III}} Tell Us about Memory Changes with Normal Aging?},
  abstract = {The standardization sample from the WMS–III (N 5 1250), which varied in age from 16 to 89, was used to determine whether encoding, retrieval, or storage of verbal and spatial information was most affected by normal aging. Immediate and delayed recall and recognition of Logical Memory and Visual Reproduction were examined. Immediate verbal and spatial recall significantly deteriorated with increasing age, and the age-associated deterioration in delayed recall and recognition was largely explained by poorer immediate memory. These findings, in concert with the smaller aging effects for percent retention after a delay, suggest that the aging effect is due to deterioration in encoding more than retrieval or storage of new information. While Visual Reproduction deteriorated more rapidly with age than Logical Memory, the pattern of performance decrements as a function of age were comparable across both tests. Decreases in performance were first seen in the fifth decade with gradual deterioration until the eighth decade when there was another precipitous drop. These results suggest that functions that are more dependent on the frontal lobes are more vulnerable to aging than those that are more dependent on the temporal lobes. (JINS, 2003, 9, 89–96.)},
  date = {2001},
  author = {Haaland, Kathleen Y. and Price, Larry and Larue, Asenath},
  file = {D\:\\Sauve\\Zotero\\storage\\YHDQTPTN\\Haaland et al. - 2001 - What does the WMS–III tell us about memory changes.pdf;D\:\\Sauve\\Zotero\\storage\\A6AIJ4JJ\\summary.html}
}

@article{nilssonMemoryFunctionNormal2003,
  langid = {english},
  title = {Memory Function in Normal Aging},
  volume = {107},
  issn = {1600-0404},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1034/j.1600-0404.107.s179.5.x},
  doi = {10.1034/j.1600-0404.107.s179.5.x},
  abstract = {Basic findings obtained on memory functions in normal aging are presented and discussed with respect to five separate but interacting memory systems. These systems are: episodic memory, semantic memory, short-term memory, perceptual representation system and procedural memory. All available evidence from cross-sectional research shows that there is a linear, decreasing memory performance as a function of age for episodic memory. Longitudinal studies suggest, however, that this age deficit may be an overestimation, by showing a relatively stable performance level up to middle age, followed by a sharp decline. Studies on semantic memory, short-term memory, perceptual representation system, and procedural memory show a relatively constant performance level across the adult life span, although some tasks used to assess short-term memory and procedural memory have revealed an age deficit. Disregarding the mixed results for these latter two memory systems, it can be concluded that episodic memory is unique in showing an age deficit. Episodic memory is also unique in the sense that it is the only memory system showing gender differences in performance throughout the adult life span with a significantly higher performance for women.},
  number = {s179},
  journaltitle = {Acta Neurologica Scandinavica},
  urldate = {2019-01-09},
  date = {2003},
  pages = {7-13},
  keywords = {age deficit,episodic memory,perceptual representation system,procedural memory,semantic memory,short-term memory},
  author = {Nilsson, Lars-Göran},
  file = {D\:\\Sauve\\Zotero\\storage\\BZQJUFJH\\j.1600-0404.107.s179.5.html}
}

@article{henryMetaanalyticReviewProspective2004,
  title = {A Meta-Analytic Review of Prospective Memory and Aging.},
  volume = {19},
  number = {1},
  journaltitle = {Psychology and aging},
  date = {2004},
  pages = {27},
  author = {Henry, Julie D. and MacLeod, Mairi S. and Phillips, Louise H. and Crawford, John R.},
  file = {D\:\\Sauve\\Zotero\\storage\\CEHVUBYW\\Henry et al. - 2004 - A meta-analytic review of prospective memory and a.pdf;D\:\\Sauve\\Zotero\\storage\\R3QHXSU2\\2004-11614-004.html}
}

@article{einsteinNormalAgingProspective1990,
  title = {Normal Aging and Prospective Memory.},
  volume = {16},
  number = {4},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  date = {1990},
  pages = {717},
  author = {Einstein, Gilles O. and McDaniel, Mark A.},
  file = {D\:\\Sauve\\Zotero\\storage\\ZUWUWME4\\1990-27910-001.html}
}

@article{hasherAutomaticEffortfulProcesses1979,
  title = {Automatic and Effortful Processes in Memory.},
  volume = {108},
  number = {3},
  journaltitle = {Journal of experimental psychology: General},
  date = {1979},
  pages = {356},
  author = {Hasher, Lynn and Zacks, Rose T.},
  file = {D\:\\Sauve\\Zotero\\storage\\6TGF6PVJ\\Hasher and Zacks - 1979 - Automatic and effortful processes in memory..pdf;D\:\\Sauve\\Zotero\\storage\\2ED7G9PA\\1981-00461-001.html}
}

@incollection{hasherWorkingMemoryComprehension1988a,
  title = {Working Memory, Comprehension, and Aging: {{A}} Review and a New View},
  volume = {22},
  shorttitle = {Working Memory, Comprehension, and Aging},
  booktitle = {Psychology of Learning and Motivation},
  publisher = {{Elsevier}},
  date = {1988},
  pages = {193--225},
  author = {Hasher, Lynn and Zacks, Rose T.},
  file = {D\:\\Sauve\\Zotero\\storage\\TR44HUJT\\Hasher and Zacks - 1988 - Working memory, comprehension, and aging A review.pdf;D\:\\Sauve\\Zotero\\storage\\BQU6GXGW\\S0079742108600419.html}
}

@article{craikMemoryChangesNormal1994,
  title = {Memory Changes in Normal Aging},
  volume = {3},
  number = {5},
  journaltitle = {Current directions in psychological science},
  date = {1994},
  pages = {155--158},
  author = {Craik, Fergus IM},
  file = {D\:\\Sauve\\Zotero\\storage\\R655DI23\\Craik - 1994 - Memory changes in normal aging.pdf}
}

@article{lumacaRandomRegularNeural2018,
  title = {From Random to Regular: Neural Constraints on the Emergence of Isochronous Rhythm during Cultural Transmission},
  volume = {13},
  shorttitle = {From Random to Regular},
  number = {8},
  journaltitle = {Social cognitive and affective neuroscience},
  date = {2018},
  pages = {877--888},
  author = {Lumaca, Massimo and Haumann, Niels Trusbak and Vuust, Peter and Brattico, Elvira and Baggio, Giosuè},
  file = {D\:\\Sauve\\Zotero\\storage\\3HJ4TXL2\\5054956.html;D\:\\Sauve\\Zotero\\storage\\6K6JXBWJ\\5054956.html}
}

@article{lumacaWeightingNeuralPrediction2018,
  title = {Weighting of Neural Prediction Error by Rhythmic Complexity: A Predictive Coding Account Using {{Mismatch Negativity}}},
  shorttitle = {Weighting of Neural Prediction Error by Rhythmic Complexity},
  journaltitle = {European Journal of Neuroscience},
  date = {2018},
  author = {Lumaca, Massimo and Trusbak Haumann, Niels and Brattico, Elvira and Grube, Manon and Vuust, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\JWPBC5MH\\ejn.html}
}

@article{sankaranDecodingDynamicRepresentation2018,
  title = {Decoding the Dynamic Representation of Musical Pitch from Human Brain Activity},
  volume = {8},
  number = {1},
  journaltitle = {Scientific reports},
  date = {2018},
  pages = {839},
  author = {Sankaran, N. and Thompson, W. F. and Carlile, S. and Carlson, T. A.},
  file = {D\:\\Sauve\\Zotero\\storage\\VZ8JIGB5\\s41598-018-19222-3.html}
}

@article{southwellEnhancedDeviantResponses2018,
  title = {Enhanced Deviant Responses in Patterned Relative to Random Sound Sequences},
  volume = {109},
  journaltitle = {Cortex},
  date = {2018},
  pages = {92--103},
  author = {Southwell, Rosy and Chait, Maria},
  file = {D\:\\Sauve\\Zotero\\storage\\CBNS25TS\\S0010945218302843.html}
}

@article{wisniewskiThetaandAlphapowerEnhancements2017,
  title = {Theta-and Alpha-Power Enhancements in the Electroencephalogram as an Auditory Delayed Match-to-Sample Task Becomes Impossibly Difficult},
  volume = {54},
  number = {12},
  journaltitle = {Psychophysiology},
  date = {2017},
  pages = {1916--1928},
  author = {Wisniewski, Matthew G. and Thompson, Eric R. and Iyer, Nandini},
  file = {D\:\\Sauve\\Zotero\\storage\\ZPDSKNBE\\psyp.html}
}

@article{scanlonEcologicalCocktailParty2018,
  title = {The Ecological Cocktail Party: {{Measuring}} Brain Activity during an Auditory Oddball Task with Background Noise},
  shorttitle = {The Ecological Cocktail Party},
  journaltitle = {bioRxiv},
  date = {2018},
  pages = {371435},
  author = {Scanlon, Joanna EM and Cormier, Danielle L. and Townsend, Kimberley A. and Kuziek, Jon WP and Mathewson, Kyle E.},
  file = {D\:\\Sauve\\Zotero\\storage\\RCYRJ5UD\\Scanlon et al. - 2018 - The ecological cocktail party Measuring brain act.pdf;D\:\\Sauve\\Zotero\\storage\\DXHR72D7\\371435.html}
}

@article{giffordNeuronalPhaseConsistency2018,
  title = {Neuronal Phase Consistency Tracks Dynamic Changes in Acoustic Spectral Regularity},
  journaltitle = {European Journal of Neuroscience},
  date = {2018},
  author = {Gifford, Adam M. and Sperling, Michael R. and Sharan, Ashwini and Gorniak, Richard J. and Williams, Ryan B. and Davis, Kathryn and Kahana, Michael J. and Cohen, Yale E.},
  file = {D\:\\Sauve\\Zotero\\storage\\DQ9VVJ4G\\ejn.html}
}

@article{skerritt-davisDetectingChangeStochastic2018,
  title = {Detecting Change in Stochastic Sound Sequences},
  volume = {14},
  number = {5},
  journaltitle = {PLoS computational biology},
  date = {2018},
  pages = {e1006162},
  author = {Skerritt-Davis, Benjamin and Elhilali, Mounya},
  file = {D\:\\Sauve\\Zotero\\storage\\PAWWSD88\\article.html}
}

@book{weschlerWAISIIIWeschlerAdult1997,
  title = {{{WAIS}}-{{III}}: {{Weschler Adult Intelligence Scale}}},
  shorttitle = {{{WAIS}}-{{III}}},
  publisher = {{San Antonio, TX: The Psychological Corporation}},
  date = {1997},
  author = {Weschler, D.}
}

@article{lappeCorticalPlasticityInduced2008,
  title = {Cortical Plasticity Induced by Short-Term Unimodal and Multimodal Musical Training},
  volume = {28},
  number = {39},
  journaltitle = {Journal of Neuroscience},
  date = {2008},
  pages = {9632--9639},
  author = {Lappe, Claudia and Herholz, Sibylle C. and Trainor, Laurel J. and Pantev, Christo},
  file = {D\:\\Sauve\\Zotero\\storage\\EZW3K4SF\\Lappe et al. - 2008 - Cortical plasticity induced by short-term unimodal.pdf;D\:\\Sauve\\Zotero\\storage\\BCYJR9DC\\9632.html}
}

@article{chobertTwelveMonthsActive2012,
  title = {Twelve Months of Active Musical Training in 8-to 10-Year-Old Children Enhances the Preattentive Processing of Syllabic Duration and Voice Onset Time},
  volume = {24},
  number = {4},
  journaltitle = {Cerebral Cortex},
  date = {2012},
  pages = {956--967},
  author = {Chobert, Julie and François, Clément and Velay, Jean-Luc and Besson, Mireille},
  file = {D\:\\Sauve\\Zotero\\storage\\EI42YK6A\\323388.html;D\:\\Sauve\\Zotero\\storage\\PFY8PI5U\\323388.html}
}

@article{hydeMusicalTrainingShapes2009,
  title = {Musical Training Shapes Structural Brain Development},
  volume = {29},
  number = {10},
  journaltitle = {Journal of Neuroscience},
  date = {2009},
  pages = {3019--3025},
  author = {Hyde, Krista L. and Lerch, Jason and Norton, Andrea and Forgeard, Marie and Winner, Ellen and Evans, Alan C. and Schlaug, Gottfried},
  file = {D\:\\Sauve\\Zotero\\storage\\L9YE9B62\\Hyde et al. - 2009 - Musical training shapes structural brain developme.pdf;D\:\\Sauve\\Zotero\\storage\\YEVTNUXE\\3019.html}
}

@article{sprengReliableDifferencesBrain2010,
  title = {Reliable Differences in Brain Activity between Young and Old Adults: A Quantitative Meta-Analysis across Multiple Cognitive Domains},
  volume = {34},
  shorttitle = {Reliable Differences in Brain Activity between Young and Old Adults},
  number = {8},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  date = {2010},
  pages = {1178--1194},
  author = {Spreng, R. Nathan and Wojtowicz, Magdalena and Grady, Cheryl L.},
  file = {D\:\\Sauve\\Zotero\\storage\\D6K4MI9G\\S0149763410000102.html}
}

@article{parkAdaptiveBrainAging2009,
  title = {The Adaptive Brain: Aging and Neurocognitive Scaffolding},
  volume = {60},
  shorttitle = {The Adaptive Brain},
  journaltitle = {Annual review of psychology},
  date = {2009},
  pages = {173--196},
  author = {Park, Denise C. and Reuter-Lorenz, Patricia},
  file = {D\:\\Sauve\\Zotero\\storage\\CR26MVFS\\annurev.psych.59.103006.html;D\:\\Sauve\\Zotero\\storage\\R2DQZP2Z\\annurev.psych.59.103006.html}
}

@article{rosePerceptualGroupingTone1997,
  title = {Perceptual Grouping of Tone Sequences by Normally Hearing and Hearing-Impaired Listeners},
  volume = {102},
  number = {3},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {1997},
  pages = {1768--1778},
  author = {Rose, Marina M. and Moore, Brian CJ},
  file = {D\:\\Sauve\\Zotero\\storage\\EA5Z8D9M\\Rose and Moore - 1997 - Perceptual grouping of tone sequences by normally .pdf;D\:\\Sauve\\Zotero\\storage\\LU5XGPYY\\1.html}
}

@article{krumhanslAcquisitionHierarchyTonal1982,
  title = {Acquisition of the Hierarchy of Tonal Functions in Music},
  volume = {10},
  number = {3},
  journaltitle = {Memory \& Cognition},
  date = {1982},
  pages = {243--251},
  author = {Krumhansl, Carol L. and Keil, Frank C.},
  file = {D\:\\Sauve\\Zotero\\storage\\7C5JXKRG\\Krumhansl and Keil - 1982 - Acquisition of the hierarchy of tonal functions in.pdf;D\:\\Sauve\\Zotero\\storage\\2ZNC44RF\\BF03197636.html}
}

@article{zentnerEmotionsEvokedSound2008,
  title = {Emotions Evoked by the Sound of Music: Characterization, Classification, and Measurement.},
  volume = {8},
  shorttitle = {Emotions Evoked by the Sound of Music},
  number = {4},
  journaltitle = {Emotion},
  date = {2008},
  pages = {494},
  author = {Zentner, Marcel and Grandjean, Didier and Scherer, Klaus R.},
  file = {D\:\\Sauve\\Zotero\\storage\\9WASBVSM\\2008-09984-007.html}
}

@article{chenEventrelatedPotentialCorrelates2011,
  title = {Event-Related Potential Correlates of the Expectancy Violation Effect during Emotional Prosody Processing},
  volume = {86},
  number = {3},
  journaltitle = {Biological Psychology},
  date = {2011},
  pages = {158--167},
  author = {Chen, Xuhai and Zhao, Lun and Jiang, Aishi and Yang, Yufang},
  file = {D\:\\Sauve\\Zotero\\storage\\BZI7UD8X\\S0301051110003017.html}
}

@article{reuter-lorenzHowDoesIt2014,
  title = {How Does It {{STAC}} up? {{Revisiting}} the Scaffolding Theory of Aging and Cognition},
  volume = {24},
  shorttitle = {How Does It {{STAC}} Up?},
  number = {3},
  journaltitle = {Neuropsychology review},
  date = {2014},
  pages = {355--370},
  author = {Reuter-Lorenz, Patricia A. and Park, Denise C.},
  file = {D\:\\Sauve\\Zotero\\storage\\8DLMN92Y\\s11065-014-9270-9.html}
}

@article{lynchEffectsAgingProcessing1994,
  title = {Effects of Aging on Processing of Novel Musical Structure},
  volume = {49},
  number = {4},
  journaltitle = {Journal of gerontology},
  date = {1994},
  pages = {P165--P172},
  author = {Lynch, Michael P. and Steffens, Michele L.},
  file = {D\:\\Sauve\\Zotero\\storage\\REQB8X6R\\565950.html}
}

@article{roweInterferenceAgingVisuospatial2010,
  title = {Interference, Aging, and Visuospatial Working Memory: {{The}} Role of Similarity.},
  volume = {24},
  shorttitle = {Interference, Aging, and Visuospatial Working Memory},
  number = {6},
  journaltitle = {Neuropsychology},
  date = {2010},
  pages = {804},
  author = {Rowe, Gillian and Hasher, Lynn and Turcotte, Josée},
  file = {D\:\\Sauve\\Zotero\\storage\\W5YMZCFA\\Rowe et al. - 2010 - Interference, aging, and visuospatial working memo.pdf;D\:\\Sauve\\Zotero\\storage\\5BQPHPTW\\2010-22445-004.html}
}

@article{connellyAgeReadingImpact1991,
  title = {Age and Reading: The Impact of Distraction.},
  volume = {6},
  shorttitle = {Age and Reading},
  number = {4},
  journaltitle = {Psychology and aging},
  date = {1991},
  pages = {533},
  author = {Connelly, S. Lisa and Hasher, Lynn and Zacks, Rose T.},
  file = {D\:\\Sauve\\Zotero\\storage\\AG5KMWF7\\Connelly et al. - 1991 - Age and reading the impact of distraction..pdf;D\:\\Sauve\\Zotero\\storage\\5IG3JC9G\\1992-18667-001.html}
}

@article{hasherInhibitoryControlCircadian1999,
  title = {Inhibitory Control, Circadian Arousal, and Age.},
  date = {1999},
  author = {Hasher, Lynn and Zacks, Rose T. and May, Cynthia P.},
  file = {D\:\\Sauve\\Zotero\\storage\\EVHJZPPJ\\1999-02468-022.html}
}

@article{simonAuditorySRCompatibility1967,
  title = {Auditory {{SR}} Compatibility: The Effect of an Irrelevant Cue on Information Processing.},
  volume = {51},
  shorttitle = {Auditory {{SR}} Compatibility},
  number = {3},
  journaltitle = {Journal of applied psychology},
  date = {1967},
  pages = {300},
  author = {Simon, J. Richard and Rudell, Alan P.},
  file = {D\:\\Sauve\\Zotero\\storage\\EU7DQGIT\\2005-10288-001.html}
}

@article{bialystokMusicalExpertiseBilingualism2009,
  title = {Musical Expertise, Bilingualism, and Executive Functioning.},
  volume = {35},
  number = {2},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {2009},
  pages = {565},
  author = {Bialystok, Ellen and DePape, Anne-Marie},
  file = {D\:\\Sauve\\Zotero\\storage\\YYR3L3KQ\\Bialystok and DePape - 2009 - Musical expertise, bilingualism, and executive fun.pdf;D\:\\Sauve\\Zotero\\storage\\8I3KRSUW\\2009-03843-019.html}
}

@article{hsuDistinctiveRepresentationMispredicted2015,
  langid = {english},
  title = {Distinctive {{Representation}} of {{Mispredicted}} and {{Unpredicted Prediction Errors}} in {{Human Electroencephalography}}},
  volume = {35},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.2204-15.2015},
  abstract = {The predictive coding model of perception proposes that neuronal responses are modulated by the amount of sensory input that the internal prediction cannot account for (i.e., prediction error). However, there is little consensus on what constitutes nonpredicted stimuli. Conceptually, whereas mispredicted stimuli may induce both prediction error generated by prediction that is not perceived and prediction error generated by sensory input that is not anticipated, unpredicted stimuli involves no top-down, only bottom-up, propagation of information in the system. Here, we examined the possibility that the processing of mispredicted and unpredicted stimuli are dissociable at the neurophysiological level using human electroencephalography. We presented participants with sets of five tones in which the frequency of the fifth tones was predicted, mispredicted, or unpredicted. Participants were required to press a key when they detected a softer fifth tone to maintain their attention. We found that mispredicted and unpredicted stimuli are associated with different amount of cortical activity, probably reflecting differences in prediction error. Moreover, relative to predicted stimuli, the mispredicted prediction error manifested as neuronal enhancement and the unpredicted prediction error manifested as neuronal attenuation on the N1 event-related potential component. These results highlight the importance of differentiating between the two nonpredicted stimuli in theoretical work on predictive coding.},
  number = {43},
  journaltitle = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  shortjournal = {J. Neurosci.},
  date = {2015-10-28},
  pages = {14653-14660},
  keywords = {Attention,Evoked Potentials,Acoustic Stimulation,Adult,Female,Male,Neurons,Perception,Young Adult,Cerebral Cortex,Psychomotor Performance,Electroencephalography,Humans,Pitch Perception},
  author = {Hsu, Yi-Fang and Le Bars, Solene and Hämäläinen, Jarmo A. and Waszak, Florian},
  file = {D\:\\Sauve\\Zotero\\storage\\LAJM8MX6\\Hsu et al. - 2015 - Distinctive Representation of Mispredicted and Unp.pdf},
  eprinttype = {pmid},
  eprint = {26511253}
}

@article{skoeAuditoryBrainstemResponse2010,
  title = {Auditory Brainstem Response to Complex Sounds: A Tutorial},
  volume = {31},
  shorttitle = {Auditory Brainstem Response to Complex Sounds},
  number = {3},
  journaltitle = {Ear and hearing},
  date = {2010},
  pages = {302},
  author = {Skoe, Erika and Kraus, Nina},
  file = {D\:\\Sauve\\Zotero\\storage\\R8SYCTHH\\PMC2868335.html}
}

@article{disbergenAssessingTopdownBottomup2018,
  title = {Assessing Top-down and Bottom-up Contributions to Auditory Stream Segregation and Integration with Polyphonic Music},
  volume = {12},
  journaltitle = {Frontiers in neuroscience},
  date = {2018},
  pages = {121},
  author = {Disbergen, Niels R. and Valente, Giancarlo and Formisano, Elia and Zatorre, Robert J.},
  file = {D\:\\Sauve\\Zotero\\storage\\QYGNQCTY\\full.html}
}

@article{duMusicalTrainingSharpens2017,
  langid = {english},
  title = {Musical Training Sharpens and Bonds Ears and Tongue to Hear Speech Better},
  volume = {114},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/114/51/13579},
  doi = {10.1073/pnas.1712223114},
  abstract = {The idea that musical training improves speech perception in challenging listening environments is appealing and of clinical importance, yet the mechanisms of any such musician advantage are not well specified. Here, using functional magnetic resonance imaging (fMRI), we found that musicians outperformed nonmusicians in identifying syllables at varying signal-to-noise ratios (SNRs), which was associated with stronger activation of the left inferior frontal and right auditory regions in musicians compared with nonmusicians. Moreover, musicians showed greater specificity of phoneme representations in bilateral auditory and speech motor regions (e.g., premotor cortex) at higher SNRs and in the left speech motor regions at lower SNRs, as determined by multivoxel pattern analysis. Musical training also enhanced the intrahemispheric and interhemispheric functional connectivity between auditory and speech motor regions. Our findings suggest that improved speech in noise perception in musicians relies on stronger recruitment of, finer phonological representations in, and stronger functional connectivity between auditory and frontal speech motor cortices in both hemispheres, regions involved in bottom-up spectrotemporal analyses and top-down articulatory prediction and sensorimotor integration, respectively.},
  number = {51},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  urldate = {2018-11-26},
  date = {2017-12-19},
  pages = {13579-13584},
  keywords = {musical training,auditory–motor integration,functional connectivity,multivoxel pattern classification,speech in noise perception},
  author = {Du, Yi and Zatorre, Robert J.},
  file = {D\:\\Sauve\\Zotero\\storage\\ASDFSILQ\\Du and Zatorre - 2017 - Musical training sharpens and bonds ears and tongu.pdf;D\:\\Sauve\\Zotero\\storage\\UK4933YT\\13579.html},
  eprinttype = {pmid},
  eprint = {29203648}
}

@article{cacciagliaAuditoryPredictionsShape2019,
  title = {Auditory Predictions Shape the Neural Responses to Stimulus Repetition and Sensory Change},
  volume = {186},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S1053811918320779},
  doi = {10.1016/j.neuroimage.2018.11.007},
  abstract = {Perception is a highly active process relying on the continuous formulation of predictive inferences using short-term sensory memory templates, which are recursively adjusted based on new input. According to this idea, earlier studies have shown that novel stimuli preceded by a higher number of repetitions yield greater novelty responses, indexed by larger mismatch negativity (MMN). However, it is not clear whether this MMN memory trace effect is driven by more adapted responses to prior stimulation or rather by a heightened processing of the unexpected deviant, and only few studies have so far attempted to characterize the functional neuroanatomy of these effects. Here we implemented a modified version of the auditory frequency oddball paradigm that enables modeling the responses to both repeated standard and deviant stimuli. Fifteen subjects underwent functional magnetic resonance imaging (fMRI) while their attention was diverted from auditory stimulation. We found that deviants with longer stimulus history of standard repetitions yielded a more robust and widespread activation in the bilateral auditory cortex. Standard tones repetition yielded a pattern of response entangling both suppression and enhancement effects depending on the predictability of upcoming stimuli. We also observed that regularity encoding and deviance detection mapped onto spatially segregated cortical subfields. Our data provide a better understanding of the neural representations underlying auditory repetition and deviance detection effects, and further support that perception operates through the principles of Bayesian predictive coding.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2018-11-20},
  date = {2019-02-01},
  pages = {200-210},
  keywords = {Repetition suppression,Auditory deviance detection,Event-related fMRI,Predictive coding,Repetition enhancement},
  author = {Cacciaglia, Raffaele and Costa-Faidella, Jordi and Zarnowiec, Katarzyna and Grimm, Sabine and Escera, Carles},
  file = {D\:\\Sauve\\Zotero\\storage\\N3VV5UD4\\Cacciaglia et al. - 2019 - Auditory predictions shape the neural responses to.pdf;D\:\\Sauve\\Zotero\\storage\\K5K33MCX\\S1053811918320779.html}
}

@article{guoImprovedDigitSpan2018,
  title = {Improved {{Digit Span}} in {{Children}} after a 6-{{Week Intervention}} of {{Playing}} a {{Musical Instrument}}: {{An Exploratory Randomized Controlled Trial}}},
  volume = {8},
  shorttitle = {Improved {{Digit Span}} in {{Children}} after a 6-{{Week Intervention}} of {{Playing}} a {{Musical Instrument}}},
  journaltitle = {Frontiers in psychology},
  date = {2018},
  pages = {2303},
  author = {Guo, Xia and Ohsawa, Chie and Suzuki, Akiko and Sekiyama, Kaoru},
  file = {D\:\\Sauve\\Zotero\\storage\\LMG3H3CK\\full.html}
}

@article{gordonRecruitmentMotorSystem2018,
  title = {Recruitment of the Motor System during Music Listening: {{An ALE}} Meta-Analysis of {{fMRI}} Data\mbox.},
  doi = {10.1371/journal.pone.0207213},
  journaltitle = {PLOS ONE},
  date = {2018},
  author = {Gordon, C.L. and Cobb, P. and Balasubramaniam, Ramesh}
}

@article{rodenDoesMusicTraining2014,
  title = {Does Music Training Enhance Working Memory Performance? {{Findings}} from a Quasi-Experimental Longitudinal Study},
  volume = {42},
  shorttitle = {Does Music Training Enhance Working Memory Performance?},
  number = {2},
  journaltitle = {Psychology of Music},
  date = {2014},
  pages = {284--298},
  author = {Roden, Ingo and Grube, Dietmar and Bongard, Stephan and Kreutz, Gunter},
  file = {D\:\\Sauve\\Zotero\\storage\\78R43N6G\\Roden et al. - 2014 - Does music training enhance working memory perform.pdf;D\:\\Sauve\\Zotero\\storage\\Y37YTIGN\\0305735612471239.html}
}

@article{kocsisPromotingPerceptionTwo2016,
  title = {Promoting the Perception of Two and Three Concurrent Sound Objects: {{An}} Event-Related Potential Study},
  volume = {107},
  shorttitle = {Promoting the Perception of Two and Three Concurrent Sound Objects},
  journaltitle = {International Journal of Psychophysiology},
  date = {2016},
  pages = {16--28},
  author = {Kocsis, Zsuzsanna and Winkler, István and Bendixen, Alexandra and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\3V66JTSW\\S0167876016301192.html;D\:\\Sauve\\Zotero\\storage\\LYEJF4WT\\S0167876016301192.html}
}

@article{farkasFunctionalBrainNetworks2018,
  title = {Functional Brain Networks Underlying Idiosyncratic Switching Patterns in Multi-Stable Auditory Perception},
  volume = {108},
  journaltitle = {Neuropsychologia},
  date = {2018},
  pages = {82--91},
  author = {Farkas, Dávid and Denham, Susan L. and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\MBGFPIEF\\S0028393217304621.html;D\:\\Sauve\\Zotero\\storage\\W7EWWQ58\\S0028393217304621.html}
}

@article{widmerDiscoveringSimpleRules2003,
  title = {Discovering Simple Rules in Complex Data: {{A}} Meta-Learning Algorithm and Some Surprising Musical Discoveries},
  volume = {146},
  shorttitle = {Discovering Simple Rules in Complex Data},
  number = {2},
  journaltitle = {Artificial Intelligence},
  date = {2003},
  pages = {129--148},
  author = {Widmer, Gerhard},
  file = {D\:\\Sauve\\Zotero\\storage\\5CS2J6DR\\Widmer - 2003 - Discovering simple rules in complex data A meta-l.pdf;D\:\\Sauve\\Zotero\\storage\\DEKK4WZA\\S000437020300016X.html}
}

@article{szalardyLargescaleFunctionalBrain2018,
  title = {Large-Scale Functional Brain Network Correlates of Speech Predictability Effects on Speaker Separation},
  volume = {131},
  journaltitle = {International Journal of Psychophysiology},
  date = {2018},
  pages = {S27},
  author = {Szalárdy, O. and German, B. and Tóth, B. and Orosz, G. and Farkas, D. and Hajdu, B. and Honbolygó, F. and Winkler, I.}
}

@article{szalardyNeuronalCorrelatesInformational2019,
  title = {Neuronal Correlates of Informational and Energetic Masking in the Human Brain in a Multi-Talker Situation},
  volume = {10},
  journaltitle = {Frontiers in psychology},
  date = {2019},
  author = {Szalárdy, Orsolya and Tóth, Brigitta and Farkas, Dávid and György, Erika and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\5RHGY4BT\\PMC6465330.html}
}

@article{szalardyEffectsAttentionTaskrelevance2018,
  title = {The Effects of Attention and Task-Relevance on the Processing of Syntactic Violations during Listening to Two Concurrent Speech Streams},
  volume = {18},
  number = {5},
  journaltitle = {Cognitive, Affective, \& Behavioral Neuroscience},
  date = {2018},
  pages = {932--948},
  author = {Szalárdy, Orsolya and Tóth, Brigitta and Farkas, Dávid and Kovács, Annamária and Urbán, Gábor and Orosz, Gábor and Szabó, Beáta Tünde and Hunyadi, László and Hajdu, Botond and Winkler, István},
  file = {D\:\\Sauve\\Zotero\\storage\\4BIIQQDG\\s13415-018-0614-4.html}
}

@online{KernScores,
  title = {{{KernScores}}},
  url = {http://kern.humdrum.org/}
}

@software{MuseScore,
  title = {{{MuseScore}}},
  url = {https://musescore.org/en/download}
}

@online{MASQDataset2019,
  title = {{{MASQ Dataset}}},
  url = {https://github.com/sarahsauve},
  year = {Last updated April 23, 2019}
}

@online{IMSLP,
  title = {{{IMSLP}}},
  url = {https://imslp.org/wiki/}
}

@article{louiEffectsAttentionNeural2005,
  title = {Effects of Attention on the Neural Processing of Harmonic Syntax in {{Western}} Music},
  volume = {25},
  number = {3},
  journaltitle = {Cognitive Brain Research},
  date = {2005},
  pages = {678--687},
  author = {Loui, Psyche and Grent, Tineke and Torpey, Dana and Woldorff, Marty},
  file = {D\:\\Sauve\\Zotero\\storage\\BMVRGCQT\\S0926641005002612.html}
}

@article{leinoRepresentationHarmonyRules2007,
  title = {Representation of Harmony Rules in the Human Brain: {{Further}} Evidence from Event-Related Potentials},
  volume = {1142},
  shorttitle = {Representation of Harmony Rules in the Human Brain},
  journaltitle = {Brain research},
  date = {2007},
  pages = {169--177},
  author = {Leino, Sakari and Brattico, Elvira and Tervaniemi, Mari and Vuust, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\IWRIEJHV\\S0006899307000789.html}
}

@article{koelschChildrenProcessingMusic2003,
  title = {Children Processing Music: Electric Brain Responses Reveal Musical Competence and Gender Differences},
  volume = {15},
  shorttitle = {Children Processing Music},
  number = {5},
  journaltitle = {Journal of Cognitive Neuroscience},
  date = {2003},
  pages = {683--693},
  author = {Koelsch, Stefan and Grossmann, Tobias and Gunter, Thomas C. and Hahne, Anja and Schröger, Erich and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\B36D5549\\Koelsch et al. - 2003 - Children processing music electric brain response.pdf;D\:\\Sauve\\Zotero\\storage\\5PBRDV5J\\jocn.2003.15.5.html}
}

@article{koelschElectricBrainResponses2003,
  title = {Electric Brain Responses Reveal Gender Differences in Music Processing},
  volume = {14},
  number = {5},
  journaltitle = {NeuroReport},
  date = {2003},
  pages = {709--713},
  author = {Koelsch, Stefan and Maess, Burkhard and Grossmann, Tobias and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\5YLFSZC8\\Electric_brain_responses_reveal_gender_differences.10.html;D\:\\Sauve\\Zotero\\storage\\D8VGMJT2\\Differentiating_ERAN_and_MMN__An_ERP_study.10.html}
}

@article{koelschProcessingTonalModulations2003,
  title = {Processing Tonal Modulations: An {{ERP}} Study},
  volume = {15},
  shorttitle = {Processing Tonal Modulations},
  number = {8},
  journaltitle = {Journal of Cognitive Neuroscience},
  date = {2003},
  pages = {1149--1159},
  author = {Koelsch, Stefan and Gunter, Thomas and Schröger, Erich and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\HU63WKFT\\Koelsch et al. - 2003 - Processing tonal modulations an ERP study.pdf;D\:\\Sauve\\Zotero\\storage\\G7JHN8BY\\089892903322598111.html}
}

@article{koelschEffectsMusicalExpertise2002,
  title = {Effects of Musical Expertise on the Early Right Anterior Negativity: An Event-Related Brain Potential Study},
  volume = {39},
  shorttitle = {Effects of Musical Expertise on the Early Right Anterior Negativity},
  number = {5},
  journaltitle = {Psychophysiology},
  date = {2002},
  pages = {657--663},
  author = {Koelsch, Stefan and Schmidt, Björn-Helmer and Kansok, Julia},
  file = {D\:\\Sauve\\Zotero\\storage\\5FTDUW6E\\Koelsch et al. - 2002 - Effects of musical expertise on the early right an.pdf;D\:\\Sauve\\Zotero\\storage\\YHUEBS6Z\\64CAF3E463CD7244AE3F508065A47F36.html}
}

@article{koelschUntanglingSyntacticSensory2007,
  title = {Untangling Syntactic and Sensory Processing: {{An ERP}} Study of Music Perception},
  volume = {44},
  shorttitle = {Untangling Syntactic and Sensory Processing},
  number = {3},
  journaltitle = {Psychophysiology},
  date = {2007},
  pages = {476--490},
  author = {Koelsch, Stefan and Jentschke, Sebastian and Sammler, Daniela and Mietchen, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\WTY49BDL\\Koelsch et al. - 2007 - Untangling syntactic and sensory processing An ER.pdf;D\:\\Sauve\\Zotero\\storage\\CJNWUC7H\\j.1469-8986.2007.00517.html}
}

@article{maessMusicalSyntaxProcessed2001,
  title = {Musical Syntax Is Processed in {{Broca}}'s Area: An {{MEG}} Study},
  volume = {4},
  shorttitle = {Musical Syntax Is Processed in {{Broca}}'s Area},
  number = {5},
  journaltitle = {Nature neuroscience},
  date = {2001},
  pages = {540},
  author = {Maess, Burkhard and Koelsch, Stefan and Gunter, Thomas C. and Friederici, Angela D.},
  file = {D\:\\Sauve\\Zotero\\storage\\YSI46WWK\\Maess et al. - 2001 - Musical syntax is processed in Broca's area an ME.pdf;D\:\\Sauve\\Zotero\\storage\\JJY68Q3R\\nn0501_540.html}
}

@article{mirandaDoubleDissociationRules2007a,
  title = {Double Dissociation between Rules and Memory in Music: {{An}} Event-Related Potential Study},
  volume = {38},
  shorttitle = {Double Dissociation between Rules and Memory in Music},
  number = {2},
  journaltitle = {Neuroimage},
  date = {2007},
  pages = {331--345},
  author = {Miranda, Robbin A. and Ullman, Michael T.},
  file = {D\:\\Sauve\\Zotero\\storage\\UT7SLHYN\\S1053811907006635.html}
}

@article{bharuchaRepresentationHarmonicStructure1983,
  title = {The Representation of Harmonic Structure in Music: {{Hierarchies}} of Stability as a Function of Context},
  volume = {13},
  shorttitle = {The Representation of Harmonic Structure in Music},
  number = {1},
  journaltitle = {Cognition},
  date = {1983},
  pages = {63--102},
  author = {Bharucha, Jamshed and Krumhansl, Carol L.},
  file = {D\:\\Sauve\\Zotero\\storage\\V2V4C889\\Bharucha and Krumhansl - 1983 - The representation of harmonic structure in music.pdf;D\:\\Sauve\\Zotero\\storage\\LVJ59SG8\\0010027783900033.html}
}

@article{sauveImpactAgingNeurophysiological2019,
  langid = {american},
  title = {The Impact of Aging on Neurophysiological Entrainment to a Metronome},
  volume = {30},
  issn = {0959-4965},
  url = {https://journals.lww.com/neuroreport/Abstract/publishahead/The_impact_of_aging_on_neurophysiological.98134.aspx},
  doi = {10.1097/WNR.0000000000001267},
  abstract = {In music, entrainment to the beat allows listeners to make predictions about upcoming events. Previous work has shown that neural oscillations will entrain to the beat of the music or rhythmic stimuli. Despite the fact that aging is known to impact both auditory and cognitive processing, little is known about how aging affects neural entrainment to rhythmic stimuli. In this study, younger and older participants listened to isochronous sequences at a slower and faster rate while EEG data was recorded. Steady-state evoked potentials had amplitude peaks at the stimulus rate and its harmonics. Steady-state evoked potentials at the stimulus rate and the first harmonic was attenuated in older adults compared to younger adults. Additionally, no amplitude difference was found for the second and third harmonics in older adults, while there was a decrease in amplitude in younger adults. This age-related decline in the entrainment specificity of the brain responses to the stimulus rate, suggests that aging may decrease the ability to entrain to stimuli in the environment, and further suggests that older adults may be less able to inhibit neural entrainment that is not directly related to the incoming stimulus.},
  number = {10},
  journaltitle = {NeuroReport},
  urldate = {2019-05-21},
  date = {2019},
  pages = {730-734},
  author = {Sauvé, Sarah A. and Bolt, Emily L. W. and Fleming, David and Zendel, Benjamin Rich},
  file = {D\:\\Sauve\\Zotero\\storage\\PURQAWPJ\\The_impact_of_aging_on_neurophysiological.98134.html}
}

@incollection{koelschNeuralSubstratesProcessing2009,
  title = {Neural Substrates of Processing Syntax and Semantics in Music},
  booktitle = {Music That Works},
  publisher = {{Springer}},
  date = {2009},
  pages = {143--153},
  author = {Koelsch, Stefan},
  file = {D\:\\Sauve\\Zotero\\storage\\LDEZNLGA\\Koelsch - 2009 - Neural substrates of processing syntax and semanti.pdf;D\:\\Sauve\\Zotero\\storage\\RHHU53AB\\978-3-211-75121-3_9.html}
}

@article{koelschShorttermEffectsProcessing2008,
  title = {Short-Term Effects of Processing Musical Syntax: {{An ERP}} Study},
  volume = {1212},
  shorttitle = {Short-Term Effects of Processing Musical Syntax},
  abstract = {!!ERAN reduces in amplitude after 2h of exposure of syntactic irregularities!!},
  journaltitle = {Brain research},
  date = {2008},
  pages = {55--62},
  author = {Koelsch, Stefan and Jentschke, Sebastian},
  file = {D\:\\Sauve\\Zotero\\storage\\476UUYUH\\S0006899307026108.html}
}

@article{koelschElectricBrainResponses2002,
  title = {Electric Brain Responses to Inappropriate Harmonies during Listening to Expressive Music},
  volume = {113},
  number = {6},
  journaltitle = {Clinical Neurophysiology},
  date = {2002},
  pages = {862--869},
  author = {Koelsch, Stefan and Mulder, Juul},
  file = {D\:\\Sauve\\Zotero\\storage\\SVGXRUIL\\S1388245702000500.html}
}

@article{koelschInteractionSyntaxProcessing2005,
  title = {Interaction between Syntax Processing in Language and in Music: An {{ERP}} Study},
  volume = {17},
  shorttitle = {Interaction between Syntax Processing in Language and in Music},
  number = {10},
  journaltitle = {Journal of cognitive neuroscience},
  date = {2005},
  pages = {1565--1577},
  author = {Koelsch, Stefan and Gunter, Thomas C. and Wittfoth, Matthias and Sammler, Daniela},
  file = {D\:\\Sauve\\Zotero\\storage\\DX28AG33\\Koelsch et al. - 2005 - Interaction between syntax processing in language .pdf;D\:\\Sauve\\Zotero\\storage\\9HWU6ZSB\\089892905774597290.html}
}

@article{koelschMusicMattersPreattentive2002,
  title = {Music Matters: {{Preattentive}} Musicality of the Human Brain},
  volume = {39},
  shorttitle = {Music Matters},
  number = {1},
  journaltitle = {Psychophysiology},
  date = {2002},
  pages = {38--48},
  author = {Koelsch, Stefan and Schroger, Erich and Gunter, Thomas C.},
  file = {D\:\\Sauve\\Zotero\\storage\\M2QR8QLI\\Koelsch et al. - 2002 - Music matters Preattentive musicality of the huma.pdf;D\:\\Sauve\\Zotero\\storage\\ERZJVV6M\\6FBF3BC9E95A3CECBAB165A73E52CABC.html}
}

@article{nematiLostMusicNeural2019,
  title = {Lost in Music: {{Neural}} Signature of Pleasure and Its Role in Modulating Attentional Resources},
  volume = {1711},
  shorttitle = {Lost in Music},
  journaltitle = {Brain research},
  date = {2019},
  pages = {7--15},
  author = {Nemati, Samaneh and Akrami, Haleh and Salehi, Sina and Esteky, Hossein and Moghimi, Sahar},
  file = {D\:\\Sauve\\Zotero\\storage\\WFUBN7HI\\S0006899319300174.html}
}

@article{cratonItOnlyRock2019,
  title = {It’s Only Rock ‘n Roll (but {{I}} like It): {{Chord}} Perception and Rock’s Liberal Harmonic Palette},
  shorttitle = {It’s Only Rock ‘n Roll (but {{I}} like It)},
  journaltitle = {Musicae Scientiae},
  date = {2019},
  pages = {1029864919845023},
  author = {Craton, Lincoln G. and Lee, Jane Hyo Jin and Krahe, Peter M.},
  file = {D\:\\Sauve\\Zotero\\storage\\N6H86I5W\\1029864919845023.html}
}

@article{zendelMusicTrainingImproves2019,
  title = {Music Training Improves the Ability to Understand Speech-in-Noise in Older Adults},
  journaltitle = {Neurobiology of Aging},
  date = {2019},
  author = {Zendel, Benjamin Rich and West, Greg and Belleville, Sylvie and Peretz, Isabelle},
  file = {D\:\\Sauve\\Zotero\\storage\\KEPHJP9L\\S019745801930171X.html}
}

@article{morillonOrganizationalPrinciplesMultidimensional2018,
  title = {Organizational Principles of Multidimensional Predictions in Human Auditory Attention},
  volume = {8},
  number = {1},
  journaltitle = {Scientific reports},
  date = {2018},
  pages = {13466},
  author = {Morillon, Benjamin},
  file = {D\:\\Sauve\\Zotero\\storage\\74JWM3M8\\s41598-018-31878-5.html}
}

@article{omigieIntracranialRecordingsComputational2019,
  title = {Intracranial {{Recordings}} and {{Computational Modeling}} of {{Music Reveal}} the {{Time Course}} of {{Prediction Error Signaling}} in {{Frontal}} and {{Temporal Cortices}}},
  journaltitle = {Journal of cognitive neuroscience},
  date = {2019},
  pages = {1--19},
  author = {Omigie, Diana and Pearce, Marcus and Lehongre, Katia and Hasboun, Dominique and Navarro, Vincent and Adam, Claude and Samson, Severine},
  file = {D\:\\Sauve\\Zotero\\storage\\K7T54GW6\\Omigie et al. - 2019 - Intracranial Recordings and Computational Modeling.pdf;D\:\\Sauve\\Zotero\\storage\\6IA74LMT\\jocn_a_01388.html}
}

@article{coathModelCorticalResponses2009,
  title = {Model Cortical Responses for the Detection of Perceptual Onsets and Beat Tracking in Singing},
  volume = {21},
  number = {2-3},
  journaltitle = {Connection Science},
  date = {2009},
  pages = {193--205},
  author = {Coath, Martin and Denham, Susan L. and Smith, Leigh M. and Honing, Henkjan and Hazan, Amaury and Holonowicz, Piotr and Purwins, Hendrik},
  file = {D\:\\Sauve\\Zotero\\storage\\DM2EUEB9\\Coath et al. - 2009 - Model cortical responses for the detection of perc.pdf;D\:\\Sauve\\Zotero\\storage\\PZHNH5H4\\09540090902733905.html}
}

@article{cuddyMemoryMelodiesLyrics2012,
  title = {Memory for Melodies and Lyrics in {{Alzheimer}}'s Disease},
  volume = {29},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2012},
  pages = {479--491},
  author = {Cuddy, Lola L. and Duffin, Jacalyn M. and Gill, Sudeep S. and Brown, Cassandra L. and Sikka, Ritu and Vanstone, Ashley D.},
  file = {D\:\\Sauve\\Zotero\\storage\\4TFEBSSR\\Cuddy et al. - 2012 - Memory for melodies and lyrics in Alzheimer's dise.pdf;D\:\\Sauve\\Zotero\\storage\\MEK5MC73\\479.html}
}

@article{vanstoneEpisodicSemanticMemory2012,
  title = {Episodic and Semantic Memory for Melodies in {{Alzheimer}}'s Disease},
  volume = {29},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2012},
  pages = {501--507},
  author = {Vanstone, Ashley D. and Sikka, Ritu and Tangness, Leila and Sham, Rosalind and Garcia, Angeles and Cuddy, Lola L.},
  file = {D\:\\Sauve\\Zotero\\storage\\C39QZH35\\Vanstone et al. - 2012 - Episodic and semantic memory for melodies in Alzhe.pdf}
}

@online{CognitiveRealityHierarchic,
  title = {The {{Cognitive Reality}} of {{Hierarchic Structure}} in... - {{Google Scholar}}},
  url = {https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=+The+Cognitive+Reality+of+Hierarchic+Structure+in+Tonal+and++Atonal+Music&btnG=},
  urldate = {2019-06-12},
  file = {D\:\\Sauve\\Zotero\\storage\\EGDFU67K\\scholar.html}
}

@article{dibbenCognitiveRealityHierarchic1994,
  title = {The {{Cognitive Reality}} of {{Hierarchic Structure}} in {{Tonal}} and {{Atonal Music}}},
  volume = {12},
  doi = {10.2307/40285753},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {1994},
  pages = {1-25},
  author = {Dibben, Nicola}
}

@book{francesPercpetionMusique1958,
  location = {{Paris}},
  title = {La Percpetion de La Musique},
  publisher = {{Vrin}},
  year = {1958, rééd 1972},
  author = {Francès, R.}
}

@article{imbertyHowWePerceive1993,
  title = {How Do We Perceive Atonal Music? {{Suggestions}} for a Theoretical Approach},
  volume = {9},
  doi = {10.1080/07494469300640541},
  number = {1-2},
  journaltitle = {Contemporary Music Review},
  date = {1993},
  pages = {325-337},
  author = {Imberty, Michel}
}

@article{deanAlgorithmicallygeneratedCorporaThat2016a,
  title = {Algorithmically-Generated {{Corpora}} That Use {{Serial Compositional Principles Can Contribute}} to the {{Modeling}} of {{Sequential Pitch Structure}} in {{Non}}-Tonal {{Music}}},
  volume = {11},
  doi = {10.18061/emr.v11i1.4900},
  number = {1},
  journaltitle = {Empirical Musicology Review},
  date = {2016},
  pages = {27-46},
  author = {Dean, Roger T. and Pearce, Marcus T.}
}

@article{krumhanslPerceptionToneHierarchies1987,
  title = {The {{Perception}} of {{Tone Hierarchies}} and {{Mirror Forms}} in {{Twelve}}-{{Tone Serial Music}}},
  volume = {5},
  doi = {10.2307/40285385},
  number = {1},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {1987},
  pages = {31-78},
  author = {Krumhansl, Carol L. and Sandell, Gregory J. and Sergeant, Desmond C.}
}

@article{daikokuPitchclassDistributionModulates2016,
  title = {Pitch-Class Distribution Modulates the Statistical Learning of Atonal Chord Sequences},
  volume = {108},
  doi = {https://doi.org/10.1016/j.bandc.2016.06.008},
  journaltitle = {Brain and Cognition},
  date = {2016},
  pages = {1-10},
  author = {Daikoku, Tatsuya and Yatomi, Yutaka and Yumoto, Masato}
}

@article{soliAssessmentCommunicationHandicap1994,
  title = {Assessment of Communication Handicap with the {{HINT}}},
  volume = {45},
  journaltitle = {Hearing Instruments},
  date = {1994},
  pages = {12--12},
  author = {Soli, S. D. and Nilsson, M.}
}

@article{nilssonDevelopmentHearingNoise1994,
  title = {Development of the {{Hearing}} in {{Noise Test}} for the Measurement of Speech Reception Thresholds in Quiet and in Noise},
  volume = {95},
  number = {2},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {1994},
  pages = {1085--1099},
  author = {Nilsson, Michael and Soli, Sigfrid D. and Sullivan, Jean A.},
  file = {D\:\\Sauve\\Zotero\\storage\\5ULGG6KP\\Nilsson et al. - 1994 - Development of the Hearing in Noise Test for the m.pdf;D\:\\Sauve\\Zotero\\storage\\TVW7H5YW\\1.html}
}

@article{killionDevelopmentQuickSpeechinnoise2004,
  title = {Development of a Quick Speech-in-Noise Test for Measuring Signal-to-Noise Ratio Loss in Normal-Hearing and Hearing-Impaired Listeners},
  volume = {116},
  number = {4},
  journaltitle = {The Journal of the Acoustical Society of America},
  date = {2004},
  pages = {2395--2405},
  author = {Killion, Mead C. and Niquette, Patricia A. and Gudmundsen, Gail I. and Revit, Lawrence J. and Banerjee, Shilpi},
  file = {D\:\\Sauve\\Zotero\\storage\\FYLSCA59\\Killion et al. - 2004 - Development of a quick speech-in-noise test for me.pdf;D\:\\Sauve\\Zotero\\storage\\RVVVWZMG\\1.html}
}

@article{wilsonDevelopmentUseAuditory1993,
  title = {Development and Use of Auditory Compact Discs in Auditory Evaluation},
  volume = {30},
  journaltitle = {Journal of Rehabilitation Research and Development},
  date = {1993},
  pages = {342--342},
  author = {Wilson, Richard H.},
  file = {D\:\\Sauve\\Zotero\\storage\\G92C4ZMW\\Wilson - 1993 - Development and use of auditory compact discs in a.pdf}
}

@book{woodcockWoodcockJohnsonIIITests2001,
  title = {Woodcock-{{Johnson III}} Tests of Cognitive Abilities},
  publisher = {{Riverside Publishing Company Itasca, IL}},
  date = {2001},
  author = {Woodcock, Richard W. and Mather, Nancy and McGrew, Kevin S. and Wendling, Barbara J.}
}

@article{sorqvistWorkingMemoryCapacity2012,
  langid = {english},
  title = {Working Memory Capacity and Visual-Verbal Cognitive Load Modulate Auditory-Sensory Gating in the Brainstem: Toward a Unified View of Attention},
  volume = {24},
  issn = {1530-8898},
  doi = {10.1162/jocn_a_00275},
  shorttitle = {Working Memory Capacity and Visual-Verbal Cognitive Load Modulate Auditory-Sensory Gating in the Brainstem},
  abstract = {Two fundamental research questions have driven attention research in the past: One concerns whether selection of relevant information among competing, irrelevant, information takes place at an early or at a late processing stage; the other concerns whether the capacity of attention is limited by a central, domain-general pool of resources or by independent, modality-specific pools. In this article, we contribute to these debates by showing that the auditory-evoked brainstem response (an early stage of auditory processing) to task-irrelevant sound decreases as a function of central working memory load (manipulated with a visual-verbal version of the n-back task). Furthermore, individual differences in central/domain-general working memory capacity modulated the magnitude of the auditory-evoked brainstem response, but only in the high working memory load condition. The results support a unified view of attention whereby the capacity of a late/central mechanism (working memory) modulates early precortical sensory processing.},
  number = {11},
  journaltitle = {Journal of Cognitive Neuroscience},
  shortjournal = {J Cogn Neurosci},
  date = {2012-11},
  pages = {2147-2154},
  keywords = {Attention,Auditory Perception,Acoustic Stimulation,Memory; Short-Term,Brain Stem,Photic Stimulation,Psychomotor Performance,Humans,Cognition,Sensory Gating,Visual Perception},
  author = {Sörqvist, Patrik and Stenfelt, Stefan and Rönnberg, Jerker},
  file = {D\:\\Sauve\\Zotero\\storage\\6GCHUQ7X\\Sörqvist et al. - 2012 - Working memory capacity and visual-verbal cognitiv.pdf},
  eprinttype = {pmid},
  eprint = {22849400}
}

@book{burnhamHearingEyeII2013,
  langid = {english},
  title = {Hearing {{Eye II}}: {{The Psychology Of Speechreading And Auditory}}-{{Visual Speech}}},
  isbn = {978-1-135-47195-8},
  shorttitle = {Hearing {{Eye II}}},
  abstract = {This volume outlines some of the developments in practical and theoretical research into speechreading lipreading that have taken place since the publication of the original "Hearing by Eye". It comprises 15 chapters by international researchers in psychology, psycholinguistics, experimental and clinical speech science, and computer engineering. It answers theoretical questions what are the mechanisms by which heard and seen speech combine? and practical ones what makes a good speechreader? Can machines be programmed to recognize seen and seen-and-heard speech?. The book is written in a non-technical way and starts to articulate a behaviourally-based but cross-disciplinary programme of research in understanding how natural language can be delivered by different modalities.},
  pagetotal = {338},
  publisher = {{Psychology Press}},
  date = {2013-10-28},
  keywords = {Psychology / Cognitive Psychology & Cognition},
  author = {Burnham, Douglas and Campbell *G.Away*, Ruth and Dodd, B. J.},
  eprinttype = {googlebooks}
}

@article{campbellHowBrainsSee2013,
  title = {How Brains See Speech: {{The}} Cortical Localisation of Speechreading in Hearing People},
  shorttitle = {How Brains See Speech},
  journaltitle = {Hearing Eye II: The Psychology Of Speechreading And Auditory-Visual Speech},
  date = {2013},
  pages = {177},
  author = {Campbell, Ruth},
  file = {D\:\\Sauve\\Zotero\\storage\\8MDUWD7A\\books.html}
}

@article{mooreNatureAuditoryProcessing2010,
  title = {Nature of Auditory Processing Disorder in Children},
  volume = {126},
  number = {2},
  journaltitle = {Pediatrics},
  date = {2010},
  pages = {e382},
  author = {Moore, David R. and Ferguson, Melanie A. and Edmondson-Jones, A. Mark and Ratib, Sonia and Riley, Alison},
  file = {D\:\\Sauve\\Zotero\\storage\\AP5LDE62\\Moore et al. - 2010 - Nature of auditory processing disorder in children.pdf}
}

@incollection{hasherInhibitoryDeficitHypothesis2015,
  langid = {english},
  title = {Inhibitory {{Deficit Hypothesis}}},
  isbn = {978-1-118-52137-3},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118521373.wbeaa259},
  abstract = {The inhibitory deficit hypothesis, or inhibitory theory, is an attention-based, general model of cognition that accounts for age differences in patterns of spared and impaired functioning in healthy older adults, patterns also seen in other populations such as those with attention deficits, those with depression, and individuals who are out of phase with their circadian rhythms. Efficient inhibitory processes enable goals to regulate thought and action by suppressing otherwise automatic activation of goal-irrelevant information. Deficits in inhibitory regulation result in heightened distractibility, greater knowledge of irrelevant information, sustained access to no longer relevant information, greater reliance on environmental cues, reduced working memory capacity, greater proactive interference, poorer retrieval of details, greater production of highly probable responses, and the implicit use of the recent past in new situations. Differences among individuals and among groups result from the efficiency of inhibition operating in the service of goals.},
  booktitle = {The {{Encyclopedia}} of {{Adulthood}} and {{Aging}}},
  publisher = {{American Cancer Society}},
  urldate = {2019-06-14},
  date = {2015},
  pages = {1-5},
  keywords = {attention,deletion,distraction,encoding,inhibition,restraint,retrieval},
  author = {Hasher, Lynn},
  file = {D\:\\Sauve\\Zotero\\storage\\RCDDWT2T\\9781118521373.html},
  doi = {10.1002/9781118521373.wbeaa259}
}

@article{russoTuneTuneOut2008,
  langid = {english},
  title = {Tune in or Tune out: Age-Related Differences in Listening to Speech in Music},
  volume = {29},
  issn = {1538-4667},
  doi = {10.1097/AUD.0b013e31817bdd1f},
  shorttitle = {Tune in or Tune Out},
  abstract = {OBJECTIVES: To examine age-related differences in listening to speech in music.
DESIGN: In the first experiment, the effect of music familiarity on word identification was compared with a standard measure of word identification in multitalker babble. The average level of the backgrounds was matched and two speech-to-background ratios were tested. In the second experiment, recognition recall was measured for background music heard during a word identification task.
RESULTS: For older adults, word identification did not depend on the type of background, but for younger adults word identification was better when the background was familiar music than when it was unfamiliar music or babble. Younger listeners remembered background music better than older listeners, with the pattern of false alarms suggesting that younger listeners consciously processed the background music more than older listeners. In other words, younger listeners attempted to "tune in" the music background, but older listeners attempted to "tune out" the background.
CONCLUSIONS: These findings reveal age-related differences in listening to speech in music. When older listeners are confronted with a music background they tend to focus attention on the speech foreground. In contrast, younger listeners attend to both the speech foreground and music background. When music is familiar, this strategy adopted by younger listeners seems to be beneficial to word identification.},
  number = {5},
  journaltitle = {Ear and Hearing},
  shortjournal = {Ear Hear},
  date = {2008-10},
  pages = {746-760},
  keywords = {Attention,Music,Acoustic Stimulation,Adult,Speech Perception,Recognition (Psychology),Young Adult,Adolescent,Aged,Noise,Perceptual Masking,Mental Recall,Humans,Aging},
  author = {Russo, Frank A. and Pichora-Fuller, M. Kathleen},
  eprinttype = {pmid},
  eprint = {18596643}
}

@article{yatesSensitivityMelodyRhythm2019,
  title = {Sensitivity to {{Melody}}, {{Rhythm}}, and {{Beat}} in {{Supporting Speech}}-in-{{Noise Perception}} in {{Young Adults}}},
  volume = {40},
  doi = {10.1097/AUD.0000000000000621},
  number = {2},
  journaltitle = {Ear and Hearing},
  date = {2019},
  pages = {358-367},
  author = {Yates, Kathryn M. and Moore, David R. and Amitay, Sygal and Barry, Johanna G.}
}

@article{vandenboschdernederlandenChangeDetectionComplex2018,
  title = {Change Detection in Complex Auditory Scenes Is Predicted by Auditory Memory, Pitch Perception, and Years of Musical Training},
  doi = {https://doi.org/10.1007/s00426-018-1072-x},
  journaltitle = {Psychological Research},
  date = {2018},
  pages = {1-17},
  author = {Vanden Bosch der Nederlanden, Christina M. and Zaragoza, Che'Renee and Rubio-Garcia, Angie and Clarkson, Evan and Snyder, Joel S.}
}

@article{cahana-amitayHowOlderAdults2015,
  title = {How Older Adults Use Cognition in Sentence-Final Word Recognition},
  volume = {23},
  doi = {10.1080/13825585.2015.1111291},
  abstract = {This study examined the effects of executive control and working memory on older adults' sentence-final word recognition. The question we addressed was the importance of executive functions to this process and how it is modulated by the predictability of the speech material. To this end, we tested 173 neurologically intact adult native English speakers aged 55-84 years. Participants were given a sentence-final word recognition test in which sentential context was manipulated and sentences were presented in different levels of babble, and multiple tests of executive functioning assessing inhibition, shifting, and efficient access to long-term memory, as well as working memory. Using a generalized linear mixed model, we found that better inhibition was associated with higher accuracy in word recognition, while increased age and greater hearing loss were associated with poorer performance. Findings are discussed in the framework of semantic control and are interpreted as supporting a theoretical view of executive control which emphasizes functional diversity among executive components.},
  number = {4},
  journaltitle = {Aging Neuropsychology and Cognition},
  date = {2015},
  pages = {1-27},
  author = {Cahana-Amitay, Dalia and Spiro, Avron and Sayers, Jesse T. and Oveis, Abigail C. and Higby, Eve and Ojo, Emmanuel and Duncan, Susan and Goral, Mira and Hyun, Jungmoon and Albert, Martin and Obler, Loraine K.}
}

@article{shanySurpriseRelatedActivationNucleus2019,
  title = {Surprise-{{Related Activation}} in the {{Nucleus Accumbens Interacts}} with {{Music}}-{{Induced Pleasantness}}},
  journaltitle = {Social cognitive and affective neuroscience},
  date = {2019},
  author = {Shany, Ofir and Singer, Neomi and Gold, Benjamin Paul and Jacoby, Nori and Tarrasch, Ricardo and Hendler, Talma and Granot, Roni},
  file = {D\:\\Sauve\\Zotero\\storage\\3AA4FAAP\\Shany et al. - 2019 - Surprise-Related Activation in the Nucleus Accumbe.pdf;D\:\\Sauve\\Zotero\\storage\\TJUMPDYZ\\5400657.html}
}

@article{krumhanslTracingDynamicChanges1982,
  title = {Tracing the Dynamic Changes in Perceived Tonal Organization in a Spatial Representation of Musical Keys.},
  volume = {89},
  number = {4},
  journaltitle = {Psychological review},
  date = {1982},
  pages = {334},
  author = {Krumhansl, Carol L. and Kessler, Edward J.},
  file = {D\:\\Sauve\\Zotero\\storage\\3PALIW72\\1982-27246-001.html}
}

@article{cohenDevelopmentTonalityInduction2000,
  title = {Development of Tonality Induction: {{Plasticity}}, Exposure, and Training},
  volume = {17},
  shorttitle = {Development of Tonality Induction},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2000},
  pages = {437--459},
  author = {Cohen, Annabel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\CIKQQA8M\\437.html}
}

@article{wilsonEvaluationBKBSINHINT2007,
  title = {An Evaluation of the {{BKB}}-{{SIN}}, {{HINT}}, {{QuickSIN}}, and {{WIN}} Materials on Listeners with Normal Hearing and Listeners with Hearing Loss},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  date = {2007},
  author = {Wilson, Richard H. and McArdle, Rachel A. and Smith, Sherri L.},
  file = {D\:\\Sauve\\Zotero\\storage\\EE3EQ3WT\\059).html}
}

@article{sauveInformationtheoreticModellingPerceivedinpress,
  title = {Information-Theoretic Modelling of Perceived Musical Complexity},
  abstract = {What makes a piece of music appear complex to a listener?  This research extends previous work by Eerola (2016), examining information content generated by a computational model of auditory expectation (IDyOM) based on statistical learning and probabilistic prediction as an empirical definition of perceived musical complexity. We systematically manipulated the melody, rhythm and harmony of the short polyphonic musical excerpts using the model to ensure that these manipulations systematically varied information content in the intended direction. Complexity ratings collected from 28 participants are found to positively correlate most strongly with melodic and harmonic information content, which correspond to descriptive musical features such as the proportion of out-of-key notes and tonal ambiguity. When individual differences are considered, these explain more variance than the manipulated predictors. Musical background is not a significant predictor of complexity ratings. The results support information content, as implemented by IDyOM, as an information-theoretic measure of complexity as well as extending IDyOM’s range of applications to perceived complexity.},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  year = {in press},
  author = {Sauvé, Sarah A. and Pearce, Marcus T.}
}

@article{quiroga-martinezReducedPredictionError2019,
  title = {Reduced Prediction Error Responses in High-as Compared to Low-Uncertainty Musical Contexts},
  journaltitle = {bioRxiv},
  date = {2019},
  pages = {422949},
  author = {Quiroga-Martinez, D. R. and Hansen, N. C. and Højlund, A. and Pearce, M. and Brattico, E. and Vuust, P.},
  file = {D\:\\Sauve\\Zotero\\storage\\9GMDT4XU\\422949v2.html}
}

@article{ballSchoenbergSerialismCognition2011,
  title = {Schoenberg, Serialism and Cognition: {{Whose}} Fault If {{No}} One Listens?},
  volume = {36},
  shorttitle = {Schoenberg, Serialism and Cognition},
  number = {1},
  journaltitle = {Interdisciplinary Science Reviews},
  date = {2011},
  pages = {24--41},
  author = {Ball, Philip},
  file = {D\:\\Sauve\\Zotero\\storage\\URQ9TGEL\\Ball - 2011 - Schoenberg, serialism and cognition Whose fault i.pdf;D\:\\Sauve\\Zotero\\storage\\UIHLPKSG\\030801811X12941390545645.html}
}

@article{schoenbergCompositionTwelveTones1975,
  title = {Composition with Twelve Tones},
  volume = {223},
  journaltitle = {Style and idea},
  date = {1975},
  author = {Schoenberg, Arnold}
}

@article{krumhanslQuantificationHierarchyTonal1979,
  title = {Quantification of the Hierarchy of Tonal Functions within a Diatonic Context.},
  volume = {5},
  number = {4},
  journaltitle = {Journal of experimental psychology: Human Perception and Performance},
  date = {1979},
  pages = {579},
  author = {Krumhansl, Carol L. and Shepard, Roger N.},
  file = {D\:\\Sauve\\Zotero\\storage\\N8CDUNXK\\Krumhansl and Shepard - 1979 - Quantification of the hierarchy of tonal functions.pdf;D\:\\Sauve\\Zotero\\storage\\N5U2UXID\\1981-00394-001.html}
}

@article{biancoPupilResponsesPitch2019,
  title = {Pupil Responses to Pitch Deviants Reflect Predictability of Melodic Sequences.},
  date = {2019},
  author = {Bianco, R. and Ptasczynski, L. E. and Omigie, D.},
  file = {D\:\\Sauve\\Zotero\\storage\\FZ2ZLVDC\\ppr84611.html}
}

@article{lagroisPoorSynchronizationMusical2019,
  title = {Poor {{Synchronization}} to {{Musical Beat Generalizes}} to {{Speech}}},
  volume = {9},
  number = {7},
  journaltitle = {Brain Sciences},
  date = {2019},
  pages = {157},
  author = {Lagrois, Marie-Élaine and Palmer, Caroline and Peretz, Isabelle},
  file = {D\:\\Sauve\\Zotero\\storage\\6ULTGYZM\\Lagrois et al. - 2019 - Poor Synchronization to Musical Beat Generalizes t.pdf;D\:\\Sauve\\Zotero\\storage\\YLF7YFCT\\157.html}
}

@article{zuanazziAdditiveInteractiveEffects2018,
  title = {Additive and Interactive Effects of Spatial Attention and Expectation on Perceptual Decisions},
  volume = {8},
  number = {1},
  journaltitle = {Scientific reports},
  date = {2018},
  pages = {6732},
  author = {Zuanazzi, Arianna and Noppeney, Uta},
  file = {D\:\\Sauve\\Zotero\\storage\\3BV9SXG7\\s41598-018-24703-6.html}
}

@article{hurleyMappingDynamicAllocation2018,
  title = {Mapping the Dynamic Allocation of Temporal Attention in Musical Patterns.},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {2018},
  author = {Hurley, Brian K. and Fink, Lauren K. and Janata, Petr},
  file = {D\:\\Sauve\\Zotero\\storage\\Q6L8H9BH\\Hurley et al. - 2018 - Mapping the dynamic allocation of temporal attenti.pdf;D\:\\Sauve\\Zotero\\storage\\DJM6UUME\\doiLanding.html}
}

@article{wolfCombiningPhaseAdvancement2019,
  title = {Combining {{Phase Advancement}} and {{Period Correction Explains Rushing}} during {{Joint Rhythmic Activities}}},
  volume = {9},
  number = {1},
  journaltitle = {Scientific Reports},
  date = {2019},
  pages = {9350},
  author = {Wolf, Thomas and Vesper, Cordula and Sebanz, Natalie and Keller, Peter E. and Knoblich, Günther},
  file = {D\:\\Sauve\\Zotero\\storage\\8HZQNSM5\\s41598-019-45601-5.html}
}

@article{agresNovelMusicbasedGame2019,
  title = {A Novel Music-Based Game with Motion Capture to Support Cognitive and Motor Function in the Elderly},
  journaltitle = {arXiv preprint arXiv:1906.10428},
  date = {2019},
  author = {Agres, Kat and Lui, Simon and Herremans, Dorien},
  file = {D\:\\Sauve\\Zotero\\storage\\QUMD8ZC4\\Agres et al. - 2019 - A novel music-based game with motion capture to su.pdf;D\:\\Sauve\\Zotero\\storage\\7LW43FYZ\\1906.html}
}

@article{cameronNeuralEntrainmentAssociated2019,
  title = {Neural Entrainment Is Associated with Subjective Groove and Complexity for Performed but Not Mechanical Musical Rhythms},
  journaltitle = {Experimental brain research},
  date = {2019},
  pages = {1--11},
  author = {Cameron, Daniel J. and Zioga, Ioanna and Lindsen, Job P. and Pearce, Marcus T. and Wiggins, Geraint A. and Potter, Keith and Bhattacharya, Joydeep},
  file = {D\:\\Sauve\\Zotero\\storage\\W4SJH4XN\\s00221-019-05557-4.html}
}

@article{bizleySimultaneousAssessmentSpeech2015,
  title = {Simultaneous {{Assessment}} of {{Speech Identification}} and {{Spatial Discrimination}}: {{A Potential Testing Approach}} for {{Bilateral Cochlear Implant Users}}?},
  volume = {19},
  shorttitle = {Simultaneous {{Assessment}} of {{Speech Identification}} and {{Spatial Discrimination}}},
  journaltitle = {Trends in hearing},
  date = {2015},
  pages = {2331216515619573},
  author = {Bizley, Jennifer K. and Elliott, Naomi and Wood, Katherine C. and Vickers, Deborah A.},
  file = {D\:\\Sauve\\Zotero\\storage\\AA2T2L3S\\Bizley et al. - 2015 - Simultaneous Assessment of Speech Identification a.pdf;D\:\\Sauve\\Zotero\\storage\\5AWI3BW8\\2331216515619573.html}
}

@article{kliuchkoFractionatingAuditoryPriors2019,
  title = {Fractionating Auditory Priors: {{A}} Neural Dissociation between Active and Passive Experience of Musical Sounds},
  volume = {14},
  shorttitle = {Fractionating Auditory Priors},
  number = {5},
  journaltitle = {PloS one},
  date = {2019},
  pages = {e0216499},
  author = {Kliuchko, Marina and Brattico, Elvira and Gold, Benjamin P. and Tervaniemi, Mari and Bogert, Brigitte and Toiviainen, Petri and Vuust, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\TVK9UL8R\\article.html}
}

@article{corrigallElectrophysiologicalCorrelatesKey2019,
  title = {Electrophysiological {{Correlates}} of {{Key}} and {{Harmony Processing}} in 3-Year-Old {{Children}}},
  volume = {36},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2019},
  pages = {435--447},
  author = {Corrigall, Kathleen A. and Trainor, Laurel J.},
  file = {D\:\\Sauve\\Zotero\\storage\\KWIUP57L\\Corrigall and Trainor - 2019 - Electrophysiological Correlates of Key and Harmony.pdf;D\:\\Sauve\\Zotero\\storage\\9Q8KAW2G\\435.html}
}

@article{zhangSingleItemMeasure2019,
  title = {A {{Single Item Measure}} for {{Identifying Musician}} and {{Nonmusician Categories Based}} on {{Measures}} of {{Musical Sophistication}}},
  volume = {36},
  number = {5},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2019},
  pages = {457--467},
  author = {Zhang, J. Diana and Schubert, Emery},
  file = {D\:\\Sauve\\Zotero\\storage\\C6J6KGAG\\Zhang and Schubert - 2019 - A Single Item Measure for Identifying Musician and.pdf;D\:\\Sauve\\Zotero\\storage\\TV8VWYXV\\457.html}
}

@article{guLateralInhibitionMechanism2019,
  title = {A {{Lateral Inhibition Mechanism Explains}} the {{Dissociation}} between {{Mismatch Negativity}} and {{Behavioral Pitch Discrimination}}},
  journaltitle = {Brain Research},
  date = {2019},
  pages = {146308},
  author = {Gu, Feng and Wong, Lena and Hu, Axu and Zhang, Xiaochu and Tong, Xiuli},
  file = {D\:\\Sauve\\Zotero\\storage\\7ZXE2TD2\\S0006899319303543.html}
}

@article{batesFittingLinearMixedEffects2015,
  langid = {english},
  title = {Fitting {{Linear Mixed}}-{{Effects Models Using}} {\textbf{Lme4}}},
  volume = {67},
  issn = {1548-7660},
  url = {http://www.jstatsoft.org/v67/i01/},
  doi = {10.18637/jss.v067.i01},
  number = {1},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Soft.},
  urldate = {2019-07-17},
  date = {2015},
  author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
  file = {D\:\\Sauve\\Zotero\\storage\\PUVMZMN6\\Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using blme4.pdf}
}

@article{lakensEquivalenceTestsPractical2017,
  langid = {english},
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for {\emph{t}} {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  volume = {8},
  issn = {1948-5506, 1948-5514},
  url = {http://journals.sagepub.com/doi/10.1177/1948550617697177},
  doi = {10.1177/1948550617697177},
  shorttitle = {Equivalence {{Tests}}},
  number = {4},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  urldate = {2019-07-17},
  date = {2017-05},
  pages = {355-362},
  author = {Lakens, Daniël},
  file = {D\:\\Sauve\\Zotero\\storage\\I9SRGWQX\\Lakens - 2017 - Equivalence Tests A Practical Primer for iti.pdf}
}

@article{glasbergModelLoudnessApplicable2002,
  title = {A Model of Loudness Applicable to Time-Varying Sounds},
  volume = {50},
  number = {5},
  journaltitle = {Journal of the Audio Engineering Society},
  date = {2002},
  pages = {331--342},
  author = {Glasberg, Brian R. and Moore, Brian CJ},
  file = {D\:\\Sauve\\Zotero\\storage\\JM5IXP46\\Glasberg and Moore - 2002 - A model of loudness applicable to time-varying sou.pdf;D\:\\Sauve\\Zotero\\storage\\F9KIZYE3\\browse.html}
}

@article{muggeoSegmentedPackageFit2008,
  title = {Segmented: An {{R Package}} to {{Fit Regression Models}} with {{Broken}}-{{Line Relationships}}},
  volume = {8},
  url = {https://cran.r-project.org/doc/Rnews/},
  number = {1},
  journaltitle = {R News},
  date = {2008},
  pages = {20-25},
  author = {Muggeo, Vito M. R.}
}

@article{koelschDifferencesElectricBrain2010,
  title = {Differences in Electric Brain Responses to Melodies and Chords},
  volume = {22},
  number = {10},
  journaltitle = {Journal of Cognitive Neuroscience},
  date = {2010},
  pages = {2251--2262},
  author = {Koelsch, Stefan and Jentschke, Sebastian},
  file = {D\:\\Sauve\\Zotero\\storage\\R739L29D\\jocn.2009.html;D\:\\Sauve\\Zotero\\storage\\ZUGC2GHY\\jocn.2009.html}
}

@article{orrRelationshipComplexityLiking2005a,
  title = {Relationship between Complexity and Liking as a Function of Expertise},
  volume = {22},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2005},
  pages = {583--611},
  author = {Orr, Mark G. and Ohlsson, Stellan},
  file = {D\:\\Sauve\\Zotero\\storage\\ZIF9SGH6\\Orr and Ohlsson - 2005 - Relationship between complexity and liking as a fu.pdf;D\:\\Sauve\\Zotero\\storage\\XVU2J9BA\\583.html}
}

@article{clementeSet200Musical,
  title = {A {{Set}} of 200 {{Musical Stimuli Varying}} in {{Balance}}, {{Contour}}, {{Symmetry}}, and {{Complexity}}: {{Behavioral}} and {{Computational Assessments}}.},
  journaltitle = {Under review},
  author = {Clemente, A and Vila-Vidal, M and Pearce, M. T. and Aquiló, G and Corradi, G and Nadal, M}
}

@article{goldPredictabilityUncertaintyPleasure,
  title = {Predictability and Uncertainty in the Pleasure of Music: A Reward for Learning?},
  journaltitle = {Under review},
  author = {Gold, B. P. and Pearce, M. T. and Mas-Herrero, E and Dagher, A and Zatorre, Robert J.}
}

@article{cheungUncertaintySurpriseJointly,
  title = {Uncertainty and {{Surprise Jointly Shape Musical Pleasure}} and {{Modulate Amygdala Activity}}},
  journaltitle = {Under review},
  author = {Cheung, V. K. M and Harrison, P. M. C. and Meyer, Leonard and Pearce, M. T. and Haynes, J-D and Koelsch, Stefan}
}

@book{craikHandbookAgingCognition2011,
  title = {The Handbook of Aging and Cognition},
  publisher = {{Psychology press}},
  date = {2011},
  author = {Craik, Fergus IM and Salthouse, Timothy A.},
  file = {D\:\\Sauve\\Zotero\\storage\\ETYCGNT6\\Craik and Salthouse - 2011 - The handbook of aging and cognition.pdf;D\:\\Sauve\\Zotero\\storage\\K5YQWWGI\\books.html}
}

@article{ruizEEGOscillatoryPatterns2011,
  title = {{{EEG}} Oscillatory Patterns Are Associated with Error Prediction during Music Performance and Are Altered in Musician's Dystonia},
  volume = {55},
  number = {4},
  journaltitle = {Neuroimage},
  date = {2011},
  pages = {1791--1803},
  author = {Ruiz, María Herrojo and Strübing, Felix and Jabusch, Hans-Christian and Altenmüller, Eckart},
  file = {D\:\\Sauve\\Zotero\\storage\\U6G278NS\\S1053811910016502.html}
}

@article{tsogliWhenStatisticalMMN2019,
  title = {When the Statistical {{MMN}} Meets the Physical {{MMN}}},
  volume = {9},
  number = {1},
  journaltitle = {Scientific reports},
  date = {2019},
  pages = {5563},
  author = {Tsogli, Vera and Jentschke, Sebastian and Daikoku, Tatsuya and Koelsch, Stefan},
  file = {D\:\\Sauve\\Zotero\\storage\\HQ5PQW2N\\s41598-019-42066-4.html}
}

@article{bouwerBeatbasedMemorybasedTemporal2019,
  title = {Beat-Based and Memory-Based Temporal Expectations in Rhythm: Similar Perceptual Effects, Different Underlying Mechanisms},
  shorttitle = {Beat-Based and Memory-Based Temporal Expectations in Rhythm},
  journaltitle = {bioRxiv},
  date = {2019},
  pages = {613398},
  author = {Bouwer, Fleur L. and Honing, Henkjan and Slagter, Heleen A.},
  file = {D\:\\Sauve\\Zotero\\storage\\NYPVGGKU\\Bouwer et al. - 2019 - Beat-based and memory-based temporal expectations .pdf;D\:\\Sauve\\Zotero\\storage\\7WRCM42D\\613398v1.html}
}

@article{daikokuMusicalCreativityDepth2018,
  title = {Musical Creativity and Depth of Implicit Knowledge: Spectral and Temporal Individualities in Improvisation},
  volume = {12},
  shorttitle = {Musical Creativity and Depth of Implicit Knowledge},
  journaltitle = {Frontiers in computational neuroscience},
  date = {2018},
  pages = {89},
  author = {Daikoku, Tatsuya},
  file = {D\:\\Sauve\\Zotero\\storage\\KSEZSZS2\\full.html}
}

@article{quiroga-martinezMusicalPredictionError2019,
  title = {Musical Prediction Error Responses Similarly Reduced by Predictive Uncertainty in Musicians and Non-Musicians},
  journaltitle = {bioRxiv},
  date = {2019},
  pages = {754333},
  author = {Quiroga-Martinez, David Ricardo and Hansen, Niels Chr and Hoejlund, Andreas and Pearce, Marcus Thomas and Brattico, Elvira and Vuust, Peter},
  file = {D\:\\Sauve\\Zotero\\storage\\2Q4YZLJY\\Quiroga-Martinez et al. - 2019 - Musical prediction error responses similarly reduc.pdf;D\:\\Sauve\\Zotero\\storage\\YB374636\\754333v1.html}
}

@article{bidelmanAfferentefferentConnectivityAuditory2019a,
  title = {Afferent-Efferent Connectivity between Auditory Brainstem and Cortex Accounts for Poorer Speech-in-Noise Comprehension in Older Adults},
  journaltitle = {Hearing research},
  date = {2019},
  pages = {107795},
  author = {Bidelman, Gavin M. and Price, Caitlin N. and Shen, Dawei and Arnott, Stephen R. and Alain, Claude},
  file = {D\:\\Sauve\\Zotero\\storage\\H45B98MZ\\S037859551930200X.html}
}

@article{macgregorMusicalEmotionDiscrimination2019,
  title = {The {{Musical Emotion Discrimination Task}}: {{A New Measure}} for {{Assessing}} the {{Ability}} to {{Discriminate Emotions}} in {{Music}}},
  volume = {10},
  shorttitle = {The {{Musical Emotion Discrimination Task}}},
  journaltitle = {Frontiers in Psychology},
  date = {2019},
  pages = {1955},
  author = {MacGregor, Chloe Lara and Mullensiefen, Daniel},
  file = {D\:\\Sauve\\Zotero\\storage\\EU2KVG7N\\full.html;D\:\\Sauve\\Zotero\\storage\\XX45GRKG\\full.html}
}

@article{largeNeurodynamicAccountMusical2016a,
  title = {A Neurodynamic Account of Musical Tonality},
  volume = {33},
  number = {3},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2016},
  pages = {319--331},
  author = {Large, Edward W. and Kim, Ji Chul and Flaig, Nicole Kristine and Bharucha, Jamshed J. and Krumhansl, Carol Lynne},
  file = {D\:\\Sauve\\Zotero\\storage\\SI6Z9ZLB\\Large et al. - 2016 - A neurodynamic account of musical tonality.pdf;D\:\\Sauve\\Zotero\\storage\\BJW25EL2\\319.html}
}

@article{dilibertoCorticalEncodingMelodic2019,
  title = {Cortical Encoding of Melodic Expectations in Human Temporal Cortex},
  journaltitle = {bioRxiv},
  date = {2019},
  pages = {714634},
  author = {Di Liberto, Giovanni M. and Pelofi, Claire and Bianco, Roberta and Patel, Prachi and Mehta, Ashesh and Herrero, Jose and de Cheveigné, Alain and Shamma, Shihab and Mesgarani, Nima},
  options = {useprefix=true},
  file = {D\:\\Sauve\\Zotero\\storage\\RMDAV38F\\Di Liberto et al. - 2019 - Cortical encoding of melodic expectations in human.pdf;D\:\\Sauve\\Zotero\\storage\\2KS9USTX\\714634v2.html}
}

@article{milneSpectralPitchClass2015,
  title = {A Spectral Pitch Class Model of the Probe Tone Data and Scalic Tonality},
  volume = {32},
  number = {4},
  journaltitle = {Music Perception: An Interdisciplinary Journal},
  date = {2015},
  pages = {364--393},
  author = {Milne, Andrew J. and Laney, Robin and Sharp, David B.},
  file = {D\:\\Sauve\\Zotero\\storage\\DEQAQQYX\\Milne et al. - 2015 - A spectral pitch class model of the probe tone dat.pdf;D\:\\Sauve\\Zotero\\storage\\JN2YZINS\\364.html}
}

@article{korkaActionIntentionbasedStimulus2019,
  title = {Action {{Intention}}-Based and {{Stimulus Regularity}}-Based {{Predictions}}: {{Same}} or {{Different}}?},
  shorttitle = {Action {{Intention}}-Based and {{Stimulus Regularity}}-Based {{Predictions}}},
  journaltitle = {Journal of Cognitive Neuroscience},
  date = {2019},
  pages = {1--16},
  author = {Korka, Betina and Schröger, Erich and Widmann, Andreas},
  file = {D\:\\Sauve\\Zotero\\storage\\PTABRKV5\\Korka et al. - 2019 - Action Intention-based and Stimulus Regularity-bas.pdf;D\:\\Sauve\\Zotero\\storage\\66GXIPU5\\jocn_a_01456.html}
}

@article{collinsCombinedModelSensory2014,
  title = {A Combined Model of Sensory and Cognitive Representations Underlying Tonal Expectations in Music: {{From}} Audio Signals to Behavior.},
  volume = {121},
  shorttitle = {A Combined Model of Sensory and Cognitive Representations Underlying Tonal Expectations in Music},
  number = {1},
  journaltitle = {Psychological review},
  date = {2014},
  pages = {33},
  author = {Collins, Tom and Tillmann, Barbara and Barrett, Frederick S. and Delbé, Charles and Janata, Petr},
  file = {D\:\\Sauve\\Zotero\\storage\\FL3VL435\\Collins et al. - 2014 - A combined model of sensory and cognitive represen.pdf;D\:\\Sauve\\Zotero\\storage\\GC3WYBLR\\2014-03591-001.html}
}

@article{criscuoloAssociationMusicalTraining2019,
  title = {On the Association between Musical Training, Intelligence and Executive Functions in Adulthood},
  volume = {10},
  journaltitle = {Frontiers in psychology},
  date = {2019},
  pages = {1704},
  author = {Criscuolo, Antonio and Bonetti, Leonardo and Särkämö, Teppo and Kliuchko, Marina and Brattico, Elvira},
  file = {D\:\\Sauve\\Zotero\\storage\\9QSUF75V\\full.html;D\:\\Sauve\\Zotero\\storage\\XTL7YTTQ\\full.html}
}

@article{guoEffectsVeridicalExpectations2016,
  title = {Effects of Veridical Expectations on Syntax Processing in Music: {{Event}}-Related Potential Evidence},
  volume = {6},
  shorttitle = {Effects of Veridical Expectations on Syntax Processing in Music},
  journaltitle = {Scientific reports},
  date = {2016},
  pages = {19064},
  author = {Guo, Shuang and Koelsch, Stefan},
  file = {D\:\\Sauve\\Zotero\\storage\\U7V87CJ8\\srep19064.html}
}

@article{bessonEventrelatedPotentialERP1995,
  title = {An Event-Related Potential ({{ERP}}) Study of Musical Expectancy: {{Comparison}} of Musicians with Nonmusicians.},
  volume = {21},
  shorttitle = {An Event-Related Potential ({{ERP}}) Study of Musical Expectancy},
  number = {6},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  date = {1995},
  pages = {1278},
  author = {Besson, Mireille and Faïta, Frédérique},
  file = {D\:\\Sauve\\Zotero\\storage\\JN66WFDI\\Besson and Faïta - 1995 - An event-related potential (ERP) study of musical .pdf;D\:\\Sauve\\Zotero\\storage\\Y9Y32RZE\\doiLanding.html}
}

@book{barthReportingPapajaReproducible,
  title = {4 {{Reporting}} | Papaja: {{Reproducible APA}} Manuscripts with {{R Markdown}}},
  url = {https://crsh.github.io/papaja_man/reporting.html#numerical-values},
  shorttitle = {4 {{Reporting}} | Papaja},
  abstract = {""},
  urldate = {2019-09-27},
  author = {Barth, Frederik Aust \& Marius},
  file = {D\:\\Sauve\\Zotero\\storage\\44VVZ9JT\\reporting.html}
}


